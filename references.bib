% Sort all references alphabetically, so you can easily search and organize
% Please keep all references complete and include the doi where available (arxiv do not have it)
% 04.03.2022

%%% AAA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{Abbeel:2004:InverseRL,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004},
  abstract={We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.}
}

@article{Abel:2017:AgentAgnosticHumanInTheLoopRL,
  title={Agent-agnostic human-in-the-loop reinforcement learning},
  author={Abel, David and Salvatier, John and Stuhlm{\"u}ller, Andreas and Evans, Owain},
  journal={arXiv preprint arXiv:1701.04079},
  year={2017},
  abstract={Providing Reinforcement Learning agents with expert advice can dramatically improve various aspects of learning. Prior work has developed teaching protocols that enable agents to learn efficiently in complex environments; many of these methods tailor the teacher's guidance to agents with a particular representation or underlying learning scheme, offering effective but specialized teaching procedures. In this work, we explore protocol programs, an agent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is to incorporate the beneficial properties of a human teacher into Reinforcement Learning without making strong assumptions about the inner workings of the agent. We show how to represent existing approaches such as action pruning, reward shaping, and training in simulation as special cases of our schema and conduct preliminary experiments on simple domains.}
}

@article{akalin:21,
  title={Reinforcement learning approaches in social robotics},
  author={Akalin, Neziha and Loutfi, Amy},
  journal={Sensors},
  volume={21},
  number={4},
  pages={1292},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute},
  abstract={This survey analyzes RL applied to social robotics. A relevant outcome is the identification of challenges in interactive learning observed in this specific domain. Human-centered explainability can learn from social robotics and help reduce some of those challenges. For instance, human teachers tend to reduce the feedback frequency as the training progresses, resulting in a diminished cumulative reward. Reducing the feedback frequency could be caused by fatigue, boredom, or the belief that the robot "remembers" and "understands" all previous feedback, requiring less advice later. Transparency issues may also arise during the training of a physical robot via human reward, causing the teacher to give incorrect feedback. Moreover, ambiguous robot behavior might affect the willingness of a human to interact again. Inexperienced users usually take more time training. Transparency or explainability could reduce confusion and help guide human trainers. Further, human trainers tend to give more positive feedback, and the learning agent should be aware of this bias. Making the agent's assumptions transparent to the trainer can improve the process.}
}

@article{Alber:2019:Innvestigate,
  title={iNNvestigate neural networks!},
  author={Alber, Maximilian and Lapuschkin, Sebastian and Seegerer, Philipp and H{\"a}gele, Miriam and Sch{\"u}tt, Kristof T and Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert and D{\"a}hne, Sven and Kindermans, Pieter-Jan},
  journal={J. Mach. Learn. Res.},
  volume={20},
  number={93},
  pages={1--8},
  year={2019},
  abstract={In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and predictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this shortcoming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-the-box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.}
}

@article{alharin2020reinforcement,
  title={Reinforcement learning interpretation methods: A survey},
  author={Alharin, Alnour and Doan, Thanh-Nam and Sartipi, Mina},
  journal={IEEE Access},
  volume={8},
  pages={171058--171077},
  year={2020},
  publisher={IEEE}
}

@inproceedings{AlonsoEtAl:2018:xAINLBeerClassifier,
  title={Explainable AI Beer Style Classifier.},
  author={Alonso, Jos{\'e} Maria and Ramos-Soto, Alejandro and Castiello, Ciro and Mencar, Corrado},
  url={https://ceur-ws.org/Vol-2151/Paper_S1.pdf},
  year={2018},
  booktitle={SICSA ReaLX},
  abstract={This paper describes how to build an eXplainable Artificial Intelligence (XAI) classifier for a real use case related to beer style classification. It combines an opaque machine learning algorithm (Random Forest) with an interpretable machine learning algorithm (Decision Tree). The result is a XAI classifier which provides users with a good interpretability-accuracy trade-off but also with explanation capabilities. First, the opaque algorithm acts as an “oracle” which finds out the most plausible output. Then, we generate a textual explanation of the given output which emerges as an automatic interpretation of the inference process carried out by the related decision tree, if the outputs from both classifiers coincide. We apply a Natural Language Generation Approach to generate the textual explanations}
}

@InProceedings{amir2016interactive,
author = {Amir, Ofra and Kamar, Ece and Kolobov, Andrey and Grosz, Barbara},
title = {Interactive Teaching Strategies for Agent Training},
booktitle = {In Proceedings of IJCAI 2016},
year = {2016},
month = {May},
}

@article{Andrychowicz:2017:HERHindsightExperienceReplay,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1707.01495},
  year={2017},
  abstract={Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.
We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.}
}

@article{araiza2019safe,
  title={Safe and trustworthy human-robot interaction},
  author={Araiza-Illan, D and Eder, K},
  journal={Humanoid Robotics: A Reference, eds A. Goswami and P. Vadakkepat (Dordrecht: Springer Netherlands)},
  pages={2397--2419},
  year={2019}
}

@article{arakawa:18,
  title={Dqn-tamer: Human-in-the-loop reinforcement learning with intractable feedback},
  author={Arakawa, Riku and Kobayashi, Sosuke and Unno, Yuya and Tsuboi, Yuta and Maeda, Shin-ichi},
  journal={arXiv preprint arXiv:1810.11748},
  year={2018},
  abstract={Exploration has been one of the greatest chal- lenges in reinforcement learning (RL), which is a large obstacle in the application of RL to robotics. Even with state-of-the-art RL algorithms, building a well-learned agent often requires too many trials, mainly due to the difficulty of matching its actions with rewards in the distant future. A remedy for this is to train an agent with real-time feedback from a human observer who immediately gives rewards for some actions. This study tackles a series of challenges for introducing such a human- in-the-loop RL scheme. The first contribution of this work is our experiments with a precisely modeled human observer: BINARY, DELAY, STOCHASTICITY, UNSUSTAINABILITY, and NATURAL REACTION. We also propose an RL method called DQN-TAMER, which efficiently uses both human feedback and distant rewards. We find that DQN-TAMER agents outperform their baselines in Maze and Taxi simulated environments. Furthermore, we demonstrate a real-world human-in-the-loop RL application where a camera automatically recognizes a user’s facial expressions as feedback to the agent while the agent explores a maze.}
}

@article{Arras:2017:ExplainingRNNsPerturbationAnalysis,
  title={Explaining recurrent neural network predictions in sentiment analysis},
  author={Arras, Leila and Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={arXiv preprint arXiv:1706.07206},
  year={2017},
  abstract={Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.}
}

@inproceedings{Arzate:2020:SurveyInteractiveRL,
  title={A survey on interactive reinforcement learning: Design principles and open challenges},
  author={Arzate Cruz, Christian and Igarashi, Takeo},
  booktitle={Proceedings of the 2020 ACM Designing Interactive Systems Conference},
  pages={1195--1209},
  year={2020},
  abstract={Interactive reinforcement learning (RL) has been successfully used in various applications in different fields, which has also motivated HCI researchers to contribute in this area. In this paper, we survey interactive RL to empower human-computer interaction (HCI) researchers with the technical background in RL needed to design new interaction techniques and propose new applications. We elucidate the roles played by HCI researchers in interactive RL, identifying ideas and promising research directions. Furthermore, we propose generic design principles that will provide researchers with a guide to effectively implement interactive RL applications.}
}

@article{AudibertMunosSzepesv:2009:ExplorationExploitation,
  title={Exploration--exploitation tradeoff using variance estimates in multi-armed bandits},
  author={Audibert, Jean-Yves and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Theoretical Computer Science},
  volume={410},
  number={19},
  pages={1876--1902},
  year={2009},
  publisher={Elsevier},
  doi={doi.org/10.1016/j.tcs.2009.01.016},
  abstract={Algorithms based on upper confidence bounds for balancing exploration and exploitation are gaining popularity since they are easy to implement, efficient and effective. This paper considers a variant of the basic algorithm for the stochastic, multi-armed bandit problem that takes into account the empirical variance of the different arms. In earlier experimental works, such algorithms were found to outperform the competing algorithms. We provide the first analysis of the expected regret for such algorithms. As expected, our results show that the algorithm that uses the variance estimates has a major advantage over its alternatives that do not use such estimates provided that the variances of the payoffs of the suboptimal arms are low. We also prove that the regret concentrates only at a polynomial rate. This holds for all the upper confidence bound based algorithms and for all bandit problems except those special ones where with probability one the payoff obtained by pulling the optimal arm is larger than the expected payoff for the second best arm. Hence, although upper confidence bound bandit algorithms achieve logarithmic expected regret rates, they might not be suitable for a risk-averse decision maker. We illustrate some of the results by computer simulations.}
}


@article{AzarLazaricBrunskill:2013:LifelongLearning,
  title={Sequential transfer in multi-armed bandit with finite set of models},
  author={Azar, Mohammad Gheshlaghi and Lazaric, Alessandro and Brunskill, Emma},
  journal={arXiv:1307.6887},
  year={2013},
  abstract={Learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents. Although results in supervised and reinforcement learning show that transfer may significantly improve the learning performance, most of the literature on transfer is focused on batch learning tasks. In this paper we study the problem of sequential transfer in online learning, notably in the multi-armed bandit framework, where the objective is to minimize the total regret over a sequence of tasks by transferring knowledge from prior tasks. We introduce a novel bandit algorithm based on a method-of-moments approach for estimating the possible tasks and derive regret bounds for it.}
}

%%% BBB %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Bach:2015:LayerWiseRelevancePropagation,
  title={On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
  author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={PloS one},
  volume={10},
  number={7},
  pages={e0130140},
  year={2015},
  publisher={Public Library of Science San Francisco, CA USA},
  abstract={Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.}
}

@inproceedings{Bapst:2019:AgentsPhysicalGNN,
  title={Structured agents for physical construction},
  author={Bapst, Victor and Sanchez-Gonzalez, Alvaro and Doersch, Carl and Stachenfeld, Kimberly and Kohli, Pushmeet and Battaglia, Peter and Hamrick, Jessica},
  booktitle={International Conference on Machine Learning},
  pages={464--474},
  year={2019},
  organization={PMLR},
  abstract={Physical construction—the ability to compose objects, subject to physical dynamics, to serve some function—is fundamental to human intelligence. We introduce a suite of challenging physical construction tasks inspired by how children play with blocks, such as matching a target configuration, stacking blocks to connect objects together, and creating shelter-like structures over target objects. We examine how a range of deep reinforcement learning agents fare on these challenges, and introduce several new approaches which provide superior performance. Our results show that agents which use structured representations (e.g., objects and scene graphs) and structured policies (e.g., object-centric actions) outperform those which use less structured representations, and generalize better beyond their training when asked to reason about larger scenes. Model-based agents which use Monte-Carlo Tree Search also outperform strictly model-free agents in our most challenging construction problems. We conclude that approaches which combine structured representations and reasoning with powerful learning are a key path toward agents that possess rich intuitive physics, scene understanding, and planning.}
}

@article{bellemare2020autonomous,
  title={Autonomous navigation of stratospheric balloons using reinforcement learning},
  author={Bellemare, Marc G and Candido, Salvatore and Castro, Pablo Samuel and Gong, Jun and Machado, Marlos C and Moitra, Subhodeep and Ponda, Sameera S and Wang, Ziyu},
  journal={Nature},
  volume={588},
  number={7836},
  pages={77--82},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{Ben-YounesEtAl:2022:DrivingBehaviorEx,
  title={Driving behavior explanation with multi-level fusion},
  author={Ben-Younes, H{\'e}di and Zablocki, {\'E}loi and P{\'e}rez, Patrick and Cord, Matthieu},
  journal={Pattern Recognition},
  volume={123},
  pages={108421},
  year={2022},
  publisher={Elsevier},
  abstract = {In this era of active development of autonomous vehicles, it becomes crucial to provide driving systems with the capacity to explain their decisions. In this work, we focus on generating high-level driving explanations as the vehicle drives. We present BEEF, for BEhavior Explanation with Fusion, a deep architecture which explains the behavior of a trajectory prediction model. Supervised by annotations of human driving decisions justifications, BEEF learns to fuse features from multiple levels. Leveraging recent advances in the multi-modal fusion literature, BEEF is carefully designed to model the correlations between high-level decisions features and mid-level perceptual features. The flexibility and efficiency of our approach are validated with extensive experiments on the HDD and BDD-X datasets.}
}


@article{biyik:21,
  title={Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences},
  author={B{\i}y{\i}k, Erdem and Losey, Dylan P and Palan, Malayandi and Landolfi, Nicholas C and Shevchuk, Gleb and Sadigh, Dorsa},
  journal={The International Journal of Robotics Research},
  pages={02783649211041652},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England},
  abstract={The paper proposes an algorithm that uses demonstrations (passive information) to initialize a belief of the reward function and actively queries the user's preferences to learn the actual reward function efficiently. The algorithm decides when to use each type of information and when to stop querying the teacher. More importantly, it accounts for the human's skill to provide data by maximizing the utility of questions while minimizing the human's uncertainty over their answer. Using gain information is more data-efficient than the previous approach for generating queries.}
}

%%% CCC %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{Caltagarione:2017:DrivingPathGeneration,
  title={LIDAR-based driving path generation using fully convolutional neural networks},
  author={Caltagirone, Luca and Bellone, Mauro and Svensson, Lennart and Wahde, Mattias},
  booktitle={2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)},
  pages={1--6},
  year={2017},
  organization={IEEE},
  abstract={In this work, a novel learning-based approach has been developed to generate driving paths by integrating LIDAR point clouds, GPS-IMU information, and Google driving directions. The system is based on a fully convolutional neural network that jointly learns to carry out perception and path generation from real-world driving sequences and that is trained using automatically generated training examples. Several combinations of input data were tested in order to assess the performance gain provided by specific information modalities. The fully convolutional neural network trained using all the available sensors together with driving directions achieved the best MaxF score of 88.13\% when considering a region of interest of 60 × 60 meters. By considering a smaller region of interest, the agreement between predicted paths and ground-truth increased to 92.60\%. The positive results obtained in this work indicate that the proposed system may help fill the gap between low-level scene parsing and behavior-reflex approaches by generating outputs that are close to vehicle control and at the same time human-interpretable.}
}


@article{ChakrabortyEtAl:2021:SurveyAdversarialAttacks,
author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
title = {A survey on adversarial attacks and defences},
journal = {CAAI Transactions on Intelligence Technology},
volume = {6},
number = {1},
pages = {25-45},
doi = {https://doi.org/10.1049/cit2.12028},
url = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/cit2.12028},
eprint = {https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/cit2.12028},
abstract = {Abstract Deep learning has evolved as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. The advancement of deep learning has been so radical that today it can surpass human-level performance. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, efficient deep learning systems can be jeopardised by using crafted adversarial samples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. Herein, the authors attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate on the efficiency and challenges of recent countermeasures against them.},
year = {2021}
}


@inproceedings{cederborg2015policy,
  title={Policy shaping with human teachers},
  author={Cederborg, Thomas and Grover, Ishaan and Isbell, Charles L and Thomaz, Andrea L},
  booktitle={Twenty-Fourth International Joint Conference on Artificial Intelligence},
  year={2015},
  abstract={In this work we evaluate the performance of a policy shaping algorithm using 26 human teachers. We examine if the algorithm is suitable for human generated data on two different boards in a pac-man domain, comparing performance to an oracle that provides critique based on one known winning policy. Perhaps surprisingly, we show that the data generated by our 26 participants yields even better performance for the agent than data generated by the oracle. This might be because humans do not discourage exploring multiple winning policies. Additionally, we evaluate the impact of different verbal instructions, and different interpretations of silence, finding that the usefulness of data is affected both by what instructions is given to teachers, and how the data is interpreted.}
}


@article{Christiano:2017:DeepRLHumanPreferences,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={arXiv preprint arXiv:1706.03741},
  year={2017},
  abstract={For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.}
}

@article{Collins:2019:Cognition,
  title = {Reinforcement learning: bringing together computation and cognition},
  author = {Anne Gabrielle Eva Collins},
  year = {2019},
  journal = {Current Opinion in Behavioral Sciences},
  volume = {29},
  pages = {63--68},
  abstract = {A key aspect of human intelligence is our ability to learn very quickly. This ability is still lacking in artificial intelligence. This article will highlight recent research showing how bringing together the fields of artificial intelligence and cognitive science may benefit both. Ideas from artificial intelligence have provided helpful formal theories to account for aspects of human learning. In return, ideas from cognitive science and neuroscience can also inform artificial intelligence research with directions to make algorithms more human-like. For example, recent work shows that human learning can only be understood in the context of multiple separate, interacting memory systems, rather than as a single, complex learner. This insight is starting to show promise in improving artificial agents’ learning efficiency.},
  doi = {10.1016/j.cobeha.2019.04.011}
}

%%% DDD %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{da2020uncertainty,
  title={Uncertainty-aware action advising for deep reinforcement learning agents},
  author={Da Silva, Felipe Leno and Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={5792--5799},
  year={2020}
}

@article{dazeley2021explainable,
  title={Explainable reinforcement learning for Broad-XAI: a conceptual framework and survey},
  author={Dazeley, Richard and Vamplew, Peter and Cruz, Francisco},
  journal={arXiv preprint arXiv:2108.09003},
  year={2021}
}
@inproceedings{de:17,
  title={How people explain action (and autonomous intelligent systems should too)},
  author={De Graaf, Maartje MA and Malle, Bertram F},
  booktitle={2017 AAAI Fall Symposium Series},
  year={2017},
  abstract ={To make Autonomous Intelligent Systems (AIS), such as virtual agents and embodied robots, “explainable” we need to understand how people respond to such systems and what expectations they have of them. Our thesis is that people will regard most AIS as intentional agents and apply the
conceptual framework and psychological mechanisms of human behavior explanation to them. We present a well supported theory of how people explain human behavior and sketch what it would take to implement the underlying framework of explanation in AIS. The benefits will be considerable: When an AIS is able to explain its behavior in ways that people find comprehensible, people are more likely to form correct mental models of such a system and calibrate their trust in the system.}
}
@article{DeSaintsEtAl:2008:phri,
  title = {An atlas of physical human–robot interaction},
  author = {{De Santis}, Agostino and Siciliano, Bruno and {De Luca}, Alessandro and Bicchi, Antonio},
  year = {2008},
  journal = {Mechanism and Machine Theory},
  volume = {43},
  number = {3},
  pages = {253--270},
  abstract = {A broad spectrum of issues have to be addressed in order to tackle the problem of a safe and dependable physical Human–Robot Interaction (pHRI). In the immediate future, metrics related to safety and dependability have to be found in order to successfully introduce robots in everyday enviornments. While there are certainly also “cognitive” issues involved, due to the human perception of the robot (and vice versa), and other objective metrics related to fault detection and isolation, our discussion focuses on the peculiar aspects of “physical” interaction with robots. In particular, safety and dependability are the underlying evaluation criteria for mechanical design, actuation, and control architectures. Mechanical and control issues are discussed with emphasis on techniques that provide safety in an intrinsic way or by means of control components. Attention is devoted to dependability, mainly related to sensors, control architectures, and fault handling and tolerance. Suggestions are provided to draft metrics for evaluating safety and dependability in pHRI, and references to the works of the scientific groups involved in the pHRI research complete the study. The present atlas is a result of the EURON perspective research project “Physical Human–Robot Interaction in anthropic DOMains (PHRIDOM)”, aimed at charting the new territory of pHRI, and constitutes the scientific basis for the ongoing STReP project “Physical Human–Robot Interaction: depENDability and Safety (PHRIENDS)”, aimed at developing key components for the next generation of robots, designed to share their environment with people.},
  doi = {10.1016/j.mechmachtheory.2007.03.003}
}

@article{DoroudiThomasBrunskill:2017:ImportanceSampling,
  title={Importance Sampling for Fair Policy Selection},
  author={Doroudi, Shayan and Thomas, Philip S and Brunskill, Emma},
  journal={Grantee Submission},
  year={2017},
  publisher={ERIC},
  abstract={We consider the problem of off-policy policy selection in reinforcement learning: using historical data generated from running one policy to compare two or more policies. We show that approaches based on importance sampling can be "unfair"--they can select the worse of two policies more often than not. We give two examples where the unfairness of importance sampling could be practically concerning. We then present sufficient conditions to theoretically guarantee fairness and a related notion of safety. Finally, we provide a practical importance sampling-based estimator to help mitigate one of the systematic sources of unfairness resulting from using importance sampling for policy selection},
  doi={10.24963/ijcai.2018/729}
}

@inproceedings{dovsilovic2018explainable,
  title={Explainable artificial intelligence: A survey},
  author={Do{\v{s}}ilovi{\'c}, Filip Karlo and Br{\v{c}}i{\'c}, Mario and Hlupi{\'c}, Nikica},
  booktitle={2018 41st International convention on information and communication technology, electronics and microelectronics (MIPRO)},
  pages={0210--0215},
  year={2018},
  organization={IEEE}
}

%%% EEE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{EderHarperLeonards:2014:HITLRoboticsSafetyAssurance,
  title={Towards the safety of human-in-the-loop robotics: Challenges and opportunities for safety assurance of robotic co-workers'},
  author={Eder, Kerstin and Harper, Chris and Leonards, Ute},
  booktitle={The 23rd IEEE International Symposium on Robot and Human Interactive Communication},
  pages={660--665},
  year={2014},
  organization={IEEE},
  abstract={The success of the human-robot co-worker team in a flexible manufacturing environment where robots learn from demonstration heavily relies on the correct and safe operation of the robot. How this can be achieved is a challenge that requires addressing both technical as well as human-centric research questions. In this paper we discuss the state of the art in safety assurance, existing as well as emerging standards in this area, and the need for new approaches to safety assurance in the context of learning machines. We then focus on robotic learning from demonstration, the challenges these techniques pose to safety assurance and outline opportunities to integrate safety considerations into algorithms “by design”. Finally, from a human-centric perspective, we stipulate that, to achieve high levels of safety and ultimately trust, the robotic co-worker must meet the innate expectations of the humans it works with. It is our aim to stimulate a discussion focused on the safety aspects of human-in-the-loop robotics, and to foster multidisciplinary collaboration to address the research challenges identified.},
  doi={10.1109/ROMAN.2014.6926328}
}

@misc{EC:LegalFramework,
  key={Regulatory framework proposal on artificial intelligence},
  title = {Regulatory framework proposal on artificial intelligence},
  year={2021},
  howpublished = {\url{https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai}},
  note = {(02.02.2022)}
}

@misc{EUResolution:2020:ethicalAI,
  key={Resolution on a framework of ethical artificial intelligence, robotics and related technologies},
  title = {European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020/2012(INL)},
  year={2020},
  howpublished = {\url{https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52020IP0275}},
  note = {(02.02.2022)}
}

@misc{EUDirective:2006:Machinery,
  key = {Machinery Directive 2006/42/EC},
  year = {2006},
  title = {Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on machinery, and amending Directive 95/16/EC},
  howpublished = {\url{https://eur-lex.europa.eu/eli/dir/2006/42/2019-07-26}},
  note = {(02.02.2022)}
}

@article{EvansEtAl:2021:ExplainabilityParadox,
   year = {2022},
   author = {Evans, Theodore and Retzlaff, Carl Orge and Geißler, Christian and Kargl, Michaela and Plass, Markus and Müller, Heimo and Kiehl, Tim-Rasmus and Zerbe, Norman and Holzinger, Andreas},
   title = {The explainability paradox: Challenges for xAI in digital pathology},
   journal = {Future Generation Computer Systems},
   volume = {133},
   number = {8},
   pages = {281--296},
   doi = {10.1016/j.future.2022.03.009}
}

%%% FFF %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% GGG %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{gao2018reinforcement,
  title={Reinforcement learning from imperfect demonstrations},
  author={Gao, Yang and Xu, Huazhe and Lin, Ji and Yu, Fisher and Levine, Sergey and Darrell, Trevor},
  journal={arXiv preprint arXiv:1802.05313},
  year={2018},
  abstract={proposes a normalized actor-critic algorithm that learns from demonstrations, no supervised loss function is used in the framework. When demonstrations are biased sample from environment, other learning from demonstration methods will not penalize the bad actions explicitly, thus making the undesirable actions from demonstrator favorable. In the normalized actor-critic framework, the Q-function can be normalized over all the actions thus scaling down the Q-values of unfavourable actions. However, the effectiveness is demonstrated on toy Minecraft and Torcs (racing game). This approach can deal with imperfect demonstrations.}
}

@article{GeirhosEtAl:2020:ShortcutLearningDNN,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
  publisher={Nature Publishing Group},
  doi={10.1038/s42256-020-00257-z},
  abstract={Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today’s machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this Perspective we seek to distil how many of deep learning’s failures can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in comparative psychology, education and linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.}
}



@article{GlanoisEtAl:2021:SurveyInterpretableRL,
  title={A Survey on Interpretable Reinforcement Learning},
  author={Glanois, Claire and Weng, Paul and Zimmer, Matthieu and Li, Dong and Yang, Tianpei and Hao, Jianye and Liu, Wulong},
  journal={arXiv preprint arXiv:2112.13112},
  year={2021},
  abstract={Although deep reinforcement learning has become a promising machine learning approach for sequential decision-making problems, it is still not mature enough for high-stake domains such as autonomous driving or medical applications. In such contexts, a learned policy needs for instance to be interpretable, so that it can be inspected before any deployment (e.g., for safety and verifiability reasons). This survey provides an overview of various approaches to achieve higher interpretability in reinforcement learning (RL). To that aim, we distinguish interpretability (as a property of a model) and explainability (as a post-hoc operation, with the intervention of a proxy) and discuss them in the context of RL with an emphasis on the former notion. In particular, we argue that interpretable RL may embrace different facets: interpretable inputs, interpretable (transition/reward) models, and interpretable decision-making. Based on this scheme, we summarize and analyze recent work related to interpretable RL with an emphasis on papers published in the past 10 years. We also discuss briefly some related research areas and point to some potential promising research directions.}
}

@article{GoodfellowShlensSzegedy:2014:AdversarialExamples,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014},
  abstract={Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.}
}

@article{griffith2013policy,
  title={Policy shaping: Integrating human feedback with reinforcement learning},
  author={Griffith, Shane and Subramanian, Kaushik and Scholz, Jonathan and Isbell, Charles L and Thomaz, Andrea L},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{guan2020explanation,
  title={Explanation augmented feedback in human-in-the-loop reinforcement learning},
  author={Guan, Lin and Verma, Mudit and Guo, Sihang and Zhang, Ruohan and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2006.14804},
  year={2020},
  abstract = {Human-in-the-loop Reinforcement Learning (HRL) aims to integrate human guid- ance with Reinforcement Learning (RL) algorithms to improve sample efficiency and performance. A common type of human guidance in HRL is binary evaluative “good" or “bad" feedback for queried states and actions. However, this type of learning scheme suffers from the problems of weak supervision and poor efficiency in leveraging human feedback. To address this, we present EXPAND (EXPlanation AugmeNted feeDback) which provides a visual explanation in the form of saliency maps from humans in addition to the binary feedback. EXPAND employs a state perturbation approach based on salient information in the state to augment the binary feedback. We choose five tasks, namely Pixel-Taxi and four Atari games, to evaluate this approach. We demonstrate the effectiveness of our method using two metrics: environment sample efficiency and human feedback sample efficiency. We show that our method significantly outperforms previous methods. We also analyze the results qualitatively by visualizing the agent’s attention. Finally, we present an ablation study to confirm our hypothesis that augmenting binary feedback with state salient information results in a boost in performance.}
}

@article{guo2021edge,
  title={EDGE: Explaining Deep Reinforcement Learning Policies},
  author={Guo, Wenbo and Wu, Xian and Khan, Usmann and Xing, Xinyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{Guo:2022:RLSurveyHumanPriorKnowledge,
  title={Survey of Reinforcement Learning based on Human Prior Knowledge},
  author={Guo, Zijing and Yao, Chendie and Feng, Yanghe and Xu, Yue},
  journal={Journal of Uncertain Systems},
  volume={15},
  number={01},
  pages={2230001},
  year={2022},
  publisher={World Scientific},
  abstract={At present, task planning is mainly solved by rules or operational research methods. The time complexity and space complexity of these methods increase exponentially with the growth of problem size. Thus, it is challenging to solve large-scale problems. Moreover, it is helpless to solve dynamic task planning problems. Reinforcement learning (RL) is often used to solve the dynamic planning problem of continuous decision making. RL agent constantly interacts with the environment to achieve the expected goal. However, the RL method meets challenges when handling the problem with ample state space. More sampling and exploration are needed to update the strategy gradient, and the convergence speed is slow. Humans ensure a quick start of learning by using prior knowledge, which reduces the exploration time of problems. Thus, we review the methods which combine human prior knowledge with RL through temporal node classification of human prior knowledge combined with RL. In this way, agents effectively reduce the sampling and exploration of the environment and eventually get the optimal strategy faster. Finally, we propose the development direction.}
}


@article{GunningAha:2019:DARPA,
   year = {2019},
   author = {Gunning, David and Aha, David W.},
   title = {DARPA's Explainable Artificial Intelligence Program},
   journal = {AI Magazine},
   volume = {40},
   number = {2},
   pages = {44--58},
   abstract = {Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA’s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychological requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychological theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems’ explanations improve user understanding, user trust, and user task performance.},
   doi = {10.1609/aimag.v40i2.2850 }
}

@article{GuptaEtAl:2021:MARLCoordinationHAMMER,
  title={HAMMER: Multi-Level Coordination of Reinforcement Learning Agents via Learned Messaging},
  author={Gupta, Nikunj and Srinivasaraghavan, G and Mohalik, Swarup Kumar and Taylor, Matthew E},
  journal={arXiv preprint arXiv:2102.00824},
  year={2021},
  abstract={Cooperative multi-agent reinforcement learning (MARL) has achieved significant results, most notably by leveraging the representation learning abilities of deep neural networks. However, large centralized approaches quickly become infeasible as the number of agents scale, and fully decentralized approaches can miss important opportunities for information sharing and coordination. Furthermore, not all agents are equal - in some cases, individual agents may not even have the ability to send communication to other agents or explicitly model other agents. This paper considers the case where there is a single, powerful, central agent that can observe the entire observation space, and there are multiple, low powered, local agents that can only receive local observations and cannot communicate with each other. The job of the central agent is to learn what message to send to different local agents, based on the global observations, not by centrally solving the entire problem and sending action commands, but by determining what additional information an individual agent should receive so that it can make a better decision. After explaining our MARL algorithm, hammer, and where it would be most applicable, we implement it in the cooperative navigation and multi-agent walker domains. Empirical results show that 1) learned communication does indeed improve system performance, 2) results generalize to multiple numbers of agents, and 3) results generalize to different reward structures.}
}

%%% HHH %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{HaSchmidhuber:2018:CoreKnowledgeWorldModels,
  title={World models},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1803.10122},
  year={2018},
  abstract={We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. }
}


@article{habibian:21,
  title={Here's What I've Learned: Asking Questions that Reveal Reward Learning},
  author={Habibian, Soheil and Jonnavittula, Ananth and Losey, Dylan P},
  journal={arXiv preprint arXiv:2107.01995},
  year={2021},
  abstract={The state-of-the-art preference learning focuses on informative questions; however, most people assume that the robot's questions reveal the robot's knowledge. This paper studies how robot questions influence humans' perception of a robot. The robot chooses informative questions that at the same time reveal its learning. A user study shows that people perceived that the robot learned similarly with purely informative queries or with the combined approach. However, users prefer revealing+informative questions since they find these questions easier to answer, more revealing about the robot's learning, and better focused on uncertain aspects of the task. A follow-up study determines that accounting what the human learns from the robot's learning helps decide which features are still unclear to the robot and whether the robot is ready to be deployed}
}

@article{hadfield:17,
  title={Inverse reward design},
  author={Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart J and Dragan, Anca},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  abstract={Autonomous agents optimize the reward function we give them. What they don’tknow is how hard it is for us to design a reward function that actually captureswhat we want.   When designing the reward,  we might think of some specifictraining scenarios, and make sure that the reward will lead to the right behaviorinthosescenarios. Inevitably, agents encounternewscenarios (e.g., new types ofterrain) where optimizing that same reward may lead to undesired behavior. Ourinsight is that reward functions are merelyobservationsabout what the designeractuallywants, and that they should be interpreted in the context in which they weredesigned. We introduceinverse reward design(IRD) as the problem of inferring thetrue objective based on the designed reward and the training MDP. We introduceapproximate methods for solving IRD problems, and use their solution to planrisk-averse behavior in test MDPs. Empirical results suggest that this approach canhelp alleviate negative side effects of misspecified reward functions and mitigatereward hacking.}
}

@article{Harmon:1997:ReinforcementLearningATutorial,
  title={Reinforcement Learning: A Tutorial.},
  author={Harmon, Mance E and Harmon, Stephanie S},
  journal = {Technical Report Defense Technical Information Center},
  year={1997},
  publisher={WRIGHT LAB WRIGHT-PATTERSON AFB OH},
  abstract={The purpose of this tutorial is to provide an introduction to reinforcement learning RL at a level easily understood by students and researchers in a wide range of disciplines. The intent is not to present a rigorous mathematical discussion that requires a great deal of effort on the part of the reader, but rather to present a conceptual framework that might serve as an introduction to a more rigorous study of RL. The fundamental principles and techniques used to solve RL problems are presented. The most popular RL algorithms are presented. Section 1 presents an overview of RL and provides a simple example to develop intuition of the underlying dynamic programming mechanism. In Section 2 the parts of a reinforcement learning problem are discussed. These include the environment, reinforcement function, and value function. Section 3 gives a description of the most widely used reinforcement learning algorithms. These include TDlambda and both the residual and direct forms of value iteration, Q-learning, and advantage learning. In Section 4 some of the ancillary issues of RL are briefly discussed, such as choosing an exploration strategy and a discount factor. The conclusion is given in Section 5. Finally, Section 6 is a glossary of commonly used terms followed by references and bibliography.}
}

@inproceedings{HayesShah:2017:AutonomousPolicyExplanation,
  title={Improving robot controller transparency through autonomous policy explanation},
  author={Hayes, Bradley and Shah, Julie A},
  booktitle={2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI},
  pages={303--312},
  year={2017},
  organization={IEEE},
  doi = {10.1145/2909824.3020233},
  abstract={Shared expectations and mutual understanding are critical facets of teamwork. Achieving these in human-robot collaborative contexts can be especially challenging, as humans and robots are unlikely to share a common language to convey intentions, plans, or justifications. Even in cases where human co-workers can inspect a robot's control code, and particularly when statistical methods are used to encode control policies, there is no guarantee that meaningful insights into a robot's behavior can be derived or that a human will be able to efficiently isolate the behaviors relevant to the interaction. We present a series of algorithms and an accompanying system that enables robots to autonomously synthesize policy descriptions and respond to both general and targeted queries by human collaborators. We demonstrate applicability to a variety of robot controller types including those that utilize conditional logic, tabular reinforcement learning, and deep reinforcement learning, synthesizing informative policy descriptions for collaborators and facilitating fault diagnosis by non-experts.}
}

@book{Hermann:2011:Sonification,
  title={The sonification handbook},
  author={Hermann, Thomas and Hunt, Andy and Neuhoff, John G},
  year={2011},
  publisher={Logos Verlag Berlin}
}

@inproceedings{hester2018deep,
  title={Deep q-learning from demonstrations},
  author={Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Osband, Ian and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  year={2018},
  abstract={Using demonstrations in Deep RL using a combination of RL loss and supervised large margin classification loss to imitate the expert actions. However, experiments were done with human demonstrators on Atari games, no robotics domain considered}
}

@article{heuillet2021explainability,
  title={Explainability in deep reinforcement learning},
  author={Heuillet, Alexandre and Couthouis, Fabien and D{\'\i}az-Rodr{\'\i}guez, Natalia},
  journal={Knowledge-Based Systems},
  volume={214},
  pages={106685},
  year={2021},
  publisher={Elsevier}
}

@article{Hochreiter:1997:Lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press},
  abstract={Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O . 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

@article{Hoffman:2018:MetricsXAI,
  title={Metrics for explainable AI: Challenges and prospects},
  author={Hoffman, Robert R and Mueller, Shane T and Klein, Gary and Litman, Jordan},
  journal={arXiv preprint arXiv:1812.04608},
  year={2018},
  abstract={The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.}
}

@article{HolzingerEtAl:2019:Wiley-Paper,
   year = {2019},
   author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and Müller, Heimo},
   title = {Causability and Explainability of Artificial Intelligence in Medicine},
   journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
   volume = {9},
   number = {4},
   pages = {1--13},
   abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black‐box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use‐case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system.},
   doi = {10.1002/widm.1312}
}

@article{HolzingerMueller:2021:HumanAI,
   year = {2021},
   author = {Holzinger, Andreas and Mueller, Heimo},
   title = {Toward Human-AI Interfaces to Support Explainability and Causability in Medical AI},
   journal = {IEEE COMPUTER},
   volume = {54},
   number = {10},
   pages = {78--86},
   abstract = {Our concept of causability is a measure of whether and to what extent humans can understand a given machine explanation. We motivate causability with a clinical case from cancer research. We argue for using causability in medical artificial intelligence (AI) to develop and evaluate future human-AI interfaces.},
   doi = {10.1109/MC.2021.3092610}
}

@article{HolzingerEtAl:2021:MultiModalCausabilityGNN,
   year = {2021},
   author = {Holzinger, Andreas and Malle, Bernd and Saranti, Anna and Pfeifer, Bastian},
   title = {Towards Multi-Modal Causability with Graph Neural Networks enabling Information Fusion for explainable AI},
   journal = {Information Fusion},
   volume = {71},
   number = {7},
   pages = {28--37},
   abstract = {AI is remarkably successful and outperforms human experts in certain tasks, even in complex domains such as medicine. Humans on the other hand are experts at multi-modal thinking and can embed new inputs almost instantly into a conceptual knowledge space shaped by experience. In many fields the aim is to build systems capable of explaining themselves, engaging in interactive what-if questions. Such questions, called counterfactuals, are becoming important in the rising field of explainable AI (xAI). Our central hypothesis is that using conceptual knowledge as a guiding model of reality will help to train more explainable, more robust and less biased machine learning models, ideally able to learn from fewer data. One important aspect in the medical domain is that various modalities contribute to one single result. Our main question is “How can we construct a multi-modal feature representation space (spanning images, text, genomics data) using knowledge bases as an initial connector for the development of novel explanation interface techniques?”. In this paper we argue for using Graph Neural Networks as a method-of-choice, enabling information fusion for multi-modal causability (causability – not to confuse with causality – is the measurable extent to which an explanation to a human expert achieves a specified level of causal understanding). The aim of this paper is to motivate the international xAI community to further work into the fields of multi-modal embeddings and interactive explainability, to lay the foundations for effective future human–AI interfaces. We emphasize that Graph Neural Networks play a major role for multi-modal causability, since causal links between features can be defined directly using graph structures.},
   doi = {10.1016/j.inffus.2021.01.008}
}

@article{Holzinger:2016:iML,
   year = {2016},
   author = {Holzinger, Andreas},
   title = {Interactive Machine Learning for Health Informatics: When do we need the human-in-the-loop?},
   journal = {Brain Informatics},
   volume = {3},
   number = {2},
   pages = {119--131},
   abstract = {Machine learning (ML) is the fastest growing field in computer science, and health informatics is amongst the greatest challenges. The goal of ML is to develop algorithms which can learn and improve over time and can be used for predictions. Most ML researchers concentrate on automatic Machine Learning (aML), where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from big data with many training sets. However, in the health domain, sometimes we are confronted with a small number of data sets or rare events, where aML-approaches suffer of insufficient training samples. Here interactive Machine Learning (iML) may be of help, having its roots in Reinforcement Learning (RL), Preference Learning (PL) and Active Learning (AL). The term iML is not yet well used, so we define it as algorithms that can interact with agents and can optimize their learning behaviour through these interactions, where the agents can also be human. This human-in-the-loop can be beneficial in solving computationally hard problems, e.g., subspace clustering, protein folding, or k-anonymization of health data, where human expertise can help to reduce an exponential search space through heuristic selection of samples. Therefore, what would otherwise be an NP-hard problem, reduces greatly in complexity through the input and the assistance of a human agent involved in the learning phase.},
   keywords = {interactive Machine learning, health informatics},
   doi = {10.1007/s40708-016-0042-6}
}

@incollection{HolzingerEtAl:2016:iMLExperiment,
   year = {2016},
   author = {Holzinger, Andreas and Plass, Markus and Holzinger, Katharina and Crisan, Gloria Cerasela and Pintea, Camelia-M. and Palade, Vasile},
   title = {Towards interactive Machine Learning (iML): Applying Ant Colony Algorithms to solve the Traveling Salesman Problem with the Human-in-the-Loop approach},
   booktitle = {Springer Lecture Notes in Computer Science LNCS 9817},
   publisher = {Springer},
   address = {Heidelberg, Berlin, New York},
   pages = {81--95},
   abstract = {Most Machine Learning (ML) researchers focus on automatic Machine Learning (aML) where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from the availability of "big data". However, sometimes, for example in health informatics, we are confronted not a small number of data sets or rare events, and with complex problems where aML-approaches fail or deliver unsatisfactory results. Here, interactive Machine Learning (iML) may be of help and the "human-in-the-loop" approach may be beneficial in solving computationally hard problems, where human expertise can help to reduce an exponential search space through heuristics. In this paper, experiments are discussed which help to evaluate the effectiveness of the iML-"human-in-the-loop" approach, particularly in opening the "black box", thereby enabling a human to directly and indirectly manipulating and interacting with an algorithm. For this purpose, we selected the Ant Colony Optimization (ACO) framework, and use it on the Traveling Salesman Problem (TSP) which is of high importance in solving many practical problems in health informatics, e.g. in the study of proteins.},
   keywords = {interactive Machine Learning, Human-in-the-loop, Traveling Salesman Problem, Ant Colony Optimization},
   doi = {10.1007/978-3-319-45507-56}
}

@article{Holzinger:2019:HumanLoopAPIN,
   year = {2019},
   author = {Holzinger, Andreas and Plass, Markus and Kickmeier-Rust, Michael and Holzinger, Katharina and Crişan, Gloria Cerasela and Pintea, Camelia-M. and Palade, Vasile},
   title = {Interactive machine learning: experimental evidence for the human in the algorithmic loop},
   journal = {Applied Intelligence},
   volume = {49},
   number = {7},
   pages = {2401--2414},
   doi = {10.1007/s10489-018-1361-5}
}



@incollection{HolzWoj:2022:XAIOverview,
   year = {2022},
   author = {Holzinger, Andreas and Saranti, Anna and Molnar, Christoph and Biececk, Prezemyslaw and Samek, Wojciech},
   title = {Explainable AI Methods - A Brief Overview},
   booktitle = {XXAI - Lecture Notes in Artificial Intelligence LNAI 13200},
   publisher = {Springer},
   doi = {10.1007/978-3-031-04083-2_2}
}

@article{HolzingerEtAl:2020:QualityOfExplanations,
   year = {2020},
   author = {Holzinger, Andreas and Carrington, Andre and Mueller, Heimo},
   title = {Measuring the Quality of Explanations: The System Causability Scale (SCS). Comparing Human and Machine Explanations},
   journal = {KI - Künstliche Intelligenz (German Journal of Artificial intelligence), Special Issue on Interactive Machine Learning, Edited by Kristian Kersting, TU Darmstadt},
   volume = {34},
   number = {2},
   pages = {193--198},
   doi = {10.1007/s13218-020-00636-z}
}

@article{HolzingerMueller:2022:PersonasAI,
   year = {2022},
   author = {Holzinger, Andreas and Kargl, Michaela and Kipperer, Bettina and Regitnig, Peter and Plass, Markus and Müller, Heimo},
   title = {Personas for Artificial Intelligence (AI) An Open Source Toolbox},
   journal = {IEEE Access},
   volume = {10},
   pages = {23732--23747},
   doi = {10.1109/ACCESS.2022.3154776}
}


@article{HudecEtAl-2021-Interpretable,
   year = {2021},
   author = {Hudec, Miroslav and Minarikova, Erika and Mesiar, Radko and Saranti, Anna and Holzinger, Andreas},
   title = {Classification by ordinal sums of conjunctive and disjunctive functions for explainable AI and interpretable machine learning solutions},
   journal = {Knowledge Based Systems},
   volume = {220},
   pages = {106916},
   abstract = {The main goal of classification is dividing entities into several classes. The classification considering uncertainty of belonging to the classes separates entities into the classes yes, no, maybe, where it is desirable to indicate the inclination towards belonging to yes or no. Neural networks have proven their high performance in sharp classification, but the solution is not traceable and therefore difficult or impossible for a human expert to interpret and to understand. Rule-based systems are explainable in principle, however, are based on formal inference structures and also have problems with interpretability due to their high complexity. We must stress that even human experts sometimes cannot explain, but rather construct mental models of the problem and consult these models to select the best possible solution. In our work, we propose classification by aggregation functions of the mixed behaviour by the variability in ordinal sums of conjunctive and disjunctive functions. In this way, domain experts should only assign the key observations regarding considered attributes. Consequently, the variability of functions provides room for machine learning to learn the best possible option from data. Such a solution is re-traceable, reproducible, and explainable to domain experts. In this paper we discuss the proposed approach on examples and outline the research steps in interactive machine learning with a human-in-the-loop via aggregation functions.},
   doi = {10.1016/j.knosys.2021.106916}
}

@article{HusseinEtAl:2017:ImitationLearning,
   author = {Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
   title = {Imitation Learning: A Survey of Learning Methods},
   year = {2017},
   issue_date = {March 2018},
   publisher = {Association for Computing Machinery},
   address = {New York, NY, USA},
   journal = {ACM Computing Surveys},
   month = {apr},
   articleno = {21},
   numpages = {35},
   volume = {50},
   number = {2},
   issn = {0360-0300},
   url = {https://doi.org/10.1145/3054912},
   abstract = {Imitation learning techniques aim to mimic human behavior in a given task. An agent (a learning machine) is trained to perform a task from demonstrations by learning a mapping between observations and actions. The idea of teaching by imitation has been around for many years; however, the field is gaining attention recently due to advances in computing and sensing as well as rising demand for intelligent applications. The paradigm of learning by imitation is gaining popularity because it facilitates teaching complex tasks with minimal expert knowledge of the tasks. Generic imitation learning methods could potentially reduce the problem of teaching a task to that of providing demonstrations, without the need for explicit programming or designing reward functions specific to the task. Modern sensors are able to collect and transmit high volumes of data rapidly, and processors with high computational power allow fast processing that maps the sensory data to actions in a timely manner. This opens the door for many potential AI applications that require real-time perception and reaction such as humanoid robots, self-driving vehicles, human computer interaction, and computer games, to name a few. However, specialized algorithms are needed to effectively and robustly learn models as learning by imitation poses its own set of challenges. In this article, we survey imitation learning methods and present design options in different steps of the learning process. We introduce a background and motivation for the field as well as highlight challenges specific to the imitation problem. Methods for designing and evaluating imitation learning tasks are categorized and reviewed. Special attention is given to learning methods in robotics and games as these domains are the most popular in the literature and provide a wide array of problems and methodologies. We extensively discuss combining imitation learning approaches using different sources and methods, as well as incorporating other motion learning methods to enhance imitation. We also discuss the potential impact on industry, present major applications, and highlight current and future research directions.},
   doi = {10.1145/3054912}
}

%%% III %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{ibarz2021train,
  title={How to train your robot with deep reinforcement learning: lessons we have learned},
  author={Ibarz, Julian and Tan, Jie and Finn, Chelsea and Kalakrishnan, Mrinal and Pastor, Peter and Levine, Sergey},
  journal={The International Journal of Robotics Research},
  volume={40},
  number={4-5},
  pages={698--721},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{IsbellEtAl:2006:AdaptiveSocialAgent,
   author = {Isbell, Charles Lee and Kearns, Michael and Singh, Satinder and Shelton, Christian R. and Stone, Peter and Kormann, Dave},
   year = {2006},
   title = {Cobot in LambdaMOO: An Adaptive Social Statistics Agent},
   journal = {Autonomous Agents and Multi-Agent Systems},
   volume = {13},
   pages = {327--354},
   abstract = {We describe our development of Cobot, a novel software agent who lives in LambdaMOO, a popular virtual world frequented by hundreds of users. Cobot’s goal was to become an actual part of that community. Here, we present a detailed discussion of the functionality that made him one of the objects most frequently interacted with in LambdaMOO, human or artificial. Cobot’s fundamental power is that he has the ability to collect social statistics summarizing the quantity and quality of interpersonal interactions. Initially, Cobot acted as little more than a reporter of this information; however, as he collected more and more data, he was able to use these statistics as models that allowed him to modify his own behavior. In particular, cobot is able to use this data to “self-program,” learning the proper way to respond to the actions of individual users, by observing how others interact with one another. Further, Cobot uses reinforcement learning to proactively take action in this complex social environment, and adapts his behavior based on multiple sources of human reward. Cobot represents a unique experiment in building adaptive agents who must live in and navigate social spaces.},
   doi = {10.1007/s10458-006-0005-z}
}

@misc{ISO:13482:2014,
   key={EN ISO 13482:2014},
   title ={{EN ISO 13482:2014}},
   year={2014},
   howpublished = {\url{https://www.iso.org/standard/53820.html}},
   note = {(02.02.2022)}
}

%%% JJJ %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{JainEtAl:2021:EpistemicUncertaintyPrediction,
  author    = {Moksh Jain and
               Salem Lahlou and
               Hadi Nekoei and
               Victor Butoi and
               Paul Bertin and
               Jarrid Rector{-}Brooks and
               Maksym Korablyov and
               Yoshua Bengio},
  title     = {{DEUP:} Direct Epistemic Uncertainty Prediction},
  journal   = {CoRR},
  volume    = {abs/2102.08501},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.08501},
  eprinttype = {arXiv},
  eprint    = {2102.08501},
  timestamp = {Fri, 19 Feb 2021 11:02:14 +0100},
  abstract  = {Epistemic uncertainty is the part of out-of-sample prediction error due to the lack of knowledge of the learner. Whereas previous work was focusing on model variance, we propose a principled approach for directly estimating epistemic uncertainty by learning to predict generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator of epistemic uncertainty includes the effect of model bias and can be applied in non-stationary learning environments arising in active learning or reinforcement learning. In addition to demonstrating these properties of Direct Epistemic Uncertainty Prediction (DEUP), we illustrate its advantage against existing methods for uncertainty estimation on downstream tasks including sequential model optimization and reinforcement learning. We also evaluate the quality of uncertainty estimates from DEUP for probabilistic classification of images and for estimating uncertainty about synergistic drug combinations.}
}

@article{jiang:21,
  title={Temporal-logic-based reward shaping for continuing reinforcement learning tasks},
  author={Jiang, Yuqian and Bharadwaj, Suda and Wu, Bo and Shah, Rishi and Topcu, Ufuk and Stone, Peter},
  journal={Good Systems-Published Research},
  year={2021},
  publisher={University of Texas at Austin},
  abstract={The paper proposes a framework for non-experienced users to interact with average-reward RL algorithms. More precisely, it allows teachers to give high-level advice. The approach receives the trainer input using temporal logic and provides guarantees optimality independent of the advice quality. Contrary to previous work that used LTL as hard constraints, the authors use LTL formulas to shape the environment reward. Instead of removing actions violating the LTL formulae and possibly causing suboptimal behavior, they are used to guide the RL agent in learning an optimal policy. The proposed framework speeds up the learning rate in comparison to differential Q-learning (the baseline RL algorithm). Poor quality advice causes the agent to learn suboptimal behavior when actions that violate the LTL formula are disallowed. The current approach still allows the agent to learn an optimal behavior regardless of the advice quality type.}
}

@article{Jung:2020:ExplainableEmpiricalRiskMinimization,
  title={Explainable empirical risk minimization},
  author={Jung, Alexander},
  journal={arXiv preprint arXiv:2009.01492},
  year={2020},
  abstract={The successful application of machine learning (ML) methods becomes increasingly dependent on their interpretability or explainability. Designing explainable ML systems is instrumental to ensuring transparency of automated decision-making that targets humans. The explainability of ML methods is also an essential ingredient for trustworthy artificial intelligence. A key challenge in ensuring explainability is its dependence on the specific human user ("explainee"). The users of machine learning methods might have vastly different background knowledge about machine learning principles. One user might have a university degree in machine learning or related fields, while another user might have never received formal training in high-school mathematics. This paper applies information-theoretic concepts to develop a novel measure for the subjective explainability of the predictions delivered by a ML method. We construct this measure via the conditional entropy of predictions, given a user signal. This user signal might be obtained from user surveys or biophysical measurements. Our main contribution is the explainable empirical risk minimization (EERM) principle of learning a hypothesis that optimally balances between the subjective explainability and risk. The EERM principle is flexible and can be combined with arbitrary machine learning models. We present several practical implementations of EERM for linear models and decision trees. Numerical experiments demonstrate the application of EERM to detecting the use of inappropriate language on social media.}
}

%%% KKK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Karalus:2021:HITL-counterfactuals,
  author    = {Jakob Karalus and
               Felix Lindner},
  title     = {Accelerating the Convergence of Human-in-the-Loop Reinforcement Learning
               with Counterfactual Explanations},
  journal   = {CoRR},
  volume    = {abs/2108.01358},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.01358},
  eprinttype = {arXiv},
  eprint    = {2108.01358},
  timestamp = {Thu, 05 Aug 2021 14:27:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-01358.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kartoun:10,
  title={A human-robot collaborative reinforcement learning algorithm},
  author={Kartoun, Uri and Stern, Helman and Edan, Yael},
  journal={Journal of Intelligent \& Robotic Systems},
  volume={60},
  number={2},
  pages={217--239},
  year={2010},
  publisher={Springer},
  abstract = {This paper presents a new reinforcement learning algorithm that enables collaborative learning between a robot and a human. The algorithm which is based on the Q(λ) approach expedites the learning process by taking advantage of human intelligence and expertise. The algorithm denoted as CQ(λ) provides the robot with self awareness to adaptively switch its collaboration level from autonomous (self performing, the robot decides which actions to take, according to its learning function) to semi-autonomous (a human advisor guides the robot and the robot combines this knowledge into its learning function). This awareness is represented by a self test of its learning performance. The approach of variable autonomy is demonstrated and evaluated using a fixed-arm robot for finding the optimal shaking policy to empty the contents of a plastic bag. A comparison between the CQ(λ) and the traditional Q(λ)-reinforcement learning algorithm, resulted in faster convergence for the CQ(λ) collaborative reinforcement learning algorithm.}
}
@inproceedings{KhatibEtAl:1999:RihEnvironment,
  title={Robots in human environments}, 
  author={Khatib, O. and Yokoi, K. and Brock, O. and Chang, K. and Casal, A.},
  year={1999},
  booktitle={Proceedings of the First Workshop on Robot Motion and Control. RoMoCo'99 (Cat. No.99EX353)}, 
  publisher={IEEE},
  pages={213--221},
  abstract={Discusses the basic capabilities needed to enable robots to operate in human populated environments for accomplishing both autonomous tasks and human-guided tasks. These capabilities are key to many new emerging robotic applications in service, construction, field, underwater, and space. An important characteristic of these robots is the "assistance" ability they can bring to humans in performing various physical tasks. To interact with humans and operate in their environments, these robots must be provided with the functionality of mobility and manipulation. The article presents developments of models, strategies, and algorithms concerned with a number of autonomous capabilities that are essential for robot operations in human environments. These capabilities include: integrated mobility and manipulation, cooperative skills between multiple robots, interaction ability with humans, and efficient techniques for real-time modification of collision-free path. These capabilities are demonstrated on two holonomic mobile platforms designed and built at Stanford University in collaboration with Oak Ridge National Laboratories and Nomadic Technologies.},
  doi={10.1109/ROMOCO.1999.791078}
}

@article{kim2013learning,
  title={Learning from limited demonstrations},
  author={Kim, Beomjoon and Farahmand, Amir-massoud and Pineau, Joelle and Precup, Doina},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013},
  abstract={This method incorporates  demonstrations as linear constraints in the Approximate Policy Iteration framework, posing it as a constraint convex optimization problem. Experiments are shown with optimal and sub-optimal demonstrations on car brake control simulation and Robot path finding task.}
}

@INPROCEEDINGS{Knox:2008:TAMER,
  author={Bradley Knox, W. and Stone, Peter},
  booktitle={2008 7th IEEE International Conference on Development and Learning}, 
  title={TAMER: Training an Agent Manually via Evaluative Reinforcement}, 
  year={2008},
  volume={},
  number={},
  pages={292-297},
  doi={10.1109/DEVLRN.2008.4640845}}
  
@inproceedings{knox:13,
  title={Training a robot via human feedback: A case study},
  author={Knox, W. Bradley and Stone, Peter and Breazeal, Cynthia},
  booktitle={International Conference on Social Robotics},
  pages={460--470},
  year={2013},
  organization={Springer},
  abstract={This paper uses Tamer in real robot training. The setup is simple;  the robot has four possible actions: turn left, right, stop, or move forward. However, the robot learned five different behaviors in relation to a training artifact, i.e., following the artifact or moving away from it. The authors report that the main issues causing unsuccessful learning sessions were the lack of transparency between the robot and the teacher. In one case, the actions seemed ambiguous at the beginning of the execution, and the misunderstanding of the action duration caused rewards to the wrong actions. In other cases, the teacher did not realize that the training artifact was out of range from the robot's sensors. The problems encountered when applying the Tamer framework in physical environments illustrate the need to provide feedback to the teacher, especially if we aim to have non-expert teachers.}
}

@article{KoberBagnellPeters:2013:RLRoboticsSurvey,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England},
  abstract={Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
  doi={10.1177/0278364913495721}
}

@article{Koditschek:2021:RoboticsCompositionalLanguage,
  title={What Is Robotics? Why Do We Need It and How Can We Get It?},
  author={Koditschek, Daniel E},
  journal={Annual Review of Control, Robotics, and Autonomous Systems},
  volume={4},
  pages={1--33},
  year={2021},
  publisher={Annual Reviews}
}



@book{Koller:2009:ProbabilisticGraphicalModelsBook,
  title={Probabilistic graphical models: principles and techniques},
  author={Koller, Daphne and Friedman, Nir},
  year={2009},
  publisher={MIT press}
}


@article{Kulkarni:2021:EducationAIDashboard,
  title={Demonstrating REACT: a Real-time Educational AI-powered Classroom Tool},
  author={Kulkarni, Ajay and Gkountouna, Olga},
  journal={arXiv preprint arXiv:2108.07693},
  year={2021},
  abstract={We present a demonstration of REACT, a new Real-time Educational AI-powered Classroom Tool that employs EDM techniques for supporting the decision-making process of educators. REACT is a data-driven tool with a user-friendly graphical interface. It analyzes students' performance data and provides context-based alerts as well as recommendations to educators for course planning. Furthermore, it incorporates model-agnostic explanations for bringing explainability and interpretability in the process of decision making. This paper demonstrates a use case scenario of our proposed tool using a real-world dataset and presents the design of its architecture and user interface. This demonstration focuses on the agglomerative clustering of students based on their performance (i.e., incorrect responses and hints used) during an in-class activity. This formation of clusters of students with similar strengths and weaknesses may help educators to improve their course planning by identifying at-risk students, forming study groups, or encouraging tutoring between students of different strengths.}
}


%%% LLL %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{LapuschkinEtAl:2016:LRP,
   year = {2016},
   author = {Lapuschkin, Sebastian and Binder, Alexander and Montavon, Gregoire and Müller, Klaus-Robert and Samek, Wojciech},
   title = {The LRP toolbox for artificial neural networks},
   journal = {The Journal of Machine Learning Research (JMLR)},
   volume = {17},
   number = {1},
   pages = {3938--3942},
   abstract = {The Layer-wise Relevance Propagation (LRP) algorithm explains a classifier's prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself. With the LRP Toolbox we provide platform-agnostic implementations for explaining the predictions of pretrained state of the art Caffe networks and stand-alone implementations for fully connected Neural Network models. The implementations for Matlab and python shall serve as a playing field to familiarize oneself with the LRP algorithm and are implemented with readability and transparency in mind. Models and data can be imported and exported using raw text formats, Matlab's .mat files and the .npy format for numpy or plain text.}
}

@article{Lapuschkin:2019:UnmaskingCleverHans,
  title={Unmasking Clever Hans predictors and assessing what machines really learn},
  author={Lapuschkin, Sebastian and W{\"a}ldchen, Stephan and Binder, Alexander and Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={1--8},
  year={2019},
  publisher={Nature Publishing Group},
  abstract={Current learning machines have successfully solved hard application problems, reaching high accuracy and displaying seemingly intelligent behavior. Here we apply recent techniques for explaining decisions of state-of-the-art learning machines and analyze various tasks from computer vision and arcade games. This showcases a spectrum of problem-solving behaviors ranging from naive and short-sighted, to well-informed and strategic. We observe that standard performance evaluation metrics can be oblivious to distinguishing these diverse problem solving behaviors. Furthermore, we propose our semi-automated Spectral Relevance Analysis that provides a practically effective way of characterizing and validating the behavior of nonlinear learning machines. This helps to assess whether a learned model indeed delivers reliably for the problem that it was conceived for. Furthermore, our work intends to add a voice of caution to the ongoing excitement about machine intelligence and pledges to evaluate and judge some of these recent successes in a more nuanced manner.}
}

@inproceedings{le2018hierarchical,
  title={Hierarchical imitation and reinforcement learning},
  author={Le, Hoang and Jiang, Nan and Agarwal, Alekh and Dudik, Miroslav and Yue, Yisong and Daum{\'e} III, Hal},
  booktitle={International conference on machine learning},
  pages={2917--2926},
  year={2018},
  organization={PMLR}
}

@article{LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE,
  title={PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training},
  author={Lee, Kimin and Smith, Laura and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2106.05091},
  year={2021},
  abstract={Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.}
}

@article{Li:2017:DRLSurvey,
  title={Deep reinforcement learning: An overview},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1701.07274},
  year={2017},
  abstract = {We give an overview of recent exciting achievements of deep reinforcement learn- ing (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value func- tion, in particular, Deep Q-Network (DQN), policy, reward, model and planning, exploration, and knowledge. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi- agent RL, hierarchical RL, and learning to learn. Then we discuss various appli- cations of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, business management, finance, healthcare, education, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.}
}

@article{Li:2019:HumanCenteredRLSurvey,
  title={Human-centered reinforcement learning: A survey},
  author={Li, Guangliang and Gomez, Randy and Nakamura, Keisuke and He, Bo},
  journal={IEEE Transactions on Human-Machine Systems},
  volume={49},
  number={4},
  pages={337--349},
  year={2019},
  publisher={IEEE},
  abstract={Human-centered reinforcement learning (RL), in which an agent learns how to perform a task from evaluative feed- back delivered by a human observer, has become more and more popular in recent years. The advantage of being able to learn from human feedback for a RL agent has led to increasing applicability to real-life problems. This paper describes the state-of-the-art human- centered RL algorithms and aims to become a starting point for researchers who are initiating their endeavors in human-centered RL. Moreover, the objective of this paper is to present a compre- hensive survey of the recent breakthroughs in this field and provide references to the most interesting and successful works. After start- ing with an introduction of the concepts of RL from environmental reward, this paper discusses the origins of human-centered RL and its difference from traditional RL. Then we describe different in- terpretations of human evaluative feedback, which have produced many human-centered RL algorithms in the past decade. In ad- dition, we describe research on agents learning from both human evaluative feedback and environmental rewards as well as on im- proving the efficiency of human-centered RL. Finally, we conclude with an overview of application areas and a discussion of future work and open questions.}
}


@inproceedings{LiangEtAl:2017:HITLReinforcementLearn,
  author = {Liang, Huanghuang and Yang, Lu and Cheng, Hong and Tu, Wenzhe and Xu, Mengjie},
  booktitle = {2017 Chinese Automation Congress (CAC)}, 
  title = {Human-in-the-loop reinforcement learning}, 
  year = {2017},
  pages = {4511--4518},
  abstract = {This paper focuses on presenting a human-in-the-loop reinforcement learning theory framework and foreseeing its application to driving decision making. Currently, the technologies in human-vehicle collaborative driving face great challenges, and do not consider the Human-in-the-loop learning framework and Driving Decision-Maker optimization under the complex road conditions. The main content of this paper aimed at presenting a study framework as follows: (1) the basic theory and model of the hybrid reinforcement learning; (2) hybrid reinforcement learning algorithm for human drivers; (3)hybrid reinforcement learning algorithm for autopilot; (4) Driving decision-maker verification platform. This paper aims at setting up the human-machine hybrid reinforcement learning theory framework and foreseeing its solutions to two kinds of typical difficulties about human-machine collaborative Driving Decision-Maker, which provides the basic theory and key technologies for the future of intelligent driving. The paper serves as a potential guideline for the study of human-in-the-loop reinforcement learning.},
  doi={10.1109/CAC.2017.8243575}
}


@article{lin:20,
  title={A review on interactive reinforcement learning from human social feedback},
  author={Lin, Jinying and Ma, Zhen and Gomez, Randy and Nakamura, Keisuke and He, Bo and Li, Guangliang},
  journal={IEEE Access},
  volume={8},
  pages={120757--120765},
  year={2020},
  publisher={IEEE},
  abstract={Reinforcement learning agent learns how to perform a task by interacting with the environment.The use of reinforcement learning in real-life applications has been limited because of the sample efficiencyproblem. Interactive reinforcement learning has been developed to speed up the agent’s learning and facilitateto learn from ordinary people by allowing them to provide social feedback, e.g, evaluative feedback, advice orinstruction. Inspired by real-life biological learning scenarios, there could be many ways to provide feedbackfor agent learning, such as via hardware delivered, natural interaction like facial expressions, speech orgestures. The agent can even learn from feedback via unimodal or multimodal sensory input. This paperreviews methods for interactive reinforcement learning agent to learn from human social feedback and theways of delivering feedback. Finally, we discuss some open problems and possible future research directions.}
}

@article{LiuAbbeel:2020:UnsupervisedActivePreTraining,
  title={Unsupervised Active Pre-Training for Reinforcement Learning},
  journal={ICLR 2021},
  note={Paper was rejected due to lack of novelty},
  author={Liu, Hao and Abbeel, Pieter},
  year={2020},
  abstract={We introduce a new unsupervised pre-training method for reinforcement learning called APT, which stands for Active Pre Training. APT learns a representation and a policy initialization by actively searching for novel states in reward-free environments. We use the contrastive learning framework for learning the representation from collected transitions. The key novel idea is to collect data during pre-training by maximizing a particle based entropy computed in the learned latent representation space. By doing particle based entropy maximization, we alleviate the need for challenging density modeling and are thus able to scale our approach to image observations. APT successfully learns meaningful representations as well as policy initializations without using any reward. We empirically evaluate APT on the Atari game suite and DMControl suite by exposing task-specific reward to agent after a long unsupervised pre-training phase. On Atari games, APT achieves human-level performance on 12 games and obtains highly competitive performance compared to canonical fully supervised RL algorithms. On DMControl suite, APT beats all baselines in terms of asymptotic performance and data efficiency and dramatically improves performance on tasks that are extremely difficult for training from scratch. Importantly, the pre-trained models can be fine-tuned to solve different tasks as long as the environment does not change. Finally, we also pre-train multi-environment encoders on data from multiple environments and show generalization to a broad set of RL tasks.}
}

@article{liu2021demonstration,
  title={Demonstration actor critic},
  author={Liu, Guoqing and Zhao, Li and Zhang, Pushi and Bian, Jiang and Qin, Tao and Yu, Nenghai and Liu, Tie-Yan},
  journal={Neurocomputing},
  volume={434},
  pages={194--202},
  year={2021},
  publisher={Elsevier},
  abstract={}
}

@inproceedings{LiuEtAl:2018:LinearModelUTrees,
  title={Toward interpretable deep reinforcement learning with linear model u-trees},
  author={Liu, Guiliang and Schulte, Oliver and Zhu, Wang and Li, Qingcan},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={414--429},
  year={2018},
  organization={Springer},
  abstract={Deep Reinforcement Learning (DRL) has achieved impressive success in many applications. A key component of many DRL models is a neural network representing a Q function, to estimate the expected cumulative reward following a state-action pair. The Q function neural network contains a lot of implicit knowledge about the RL problems, but often remains unexamined and uninterpreted. To our knowledge, this work develops the first mimic learning framework for Q functions in DRL. We introduce Linear Model U-trees (LMUTs) to approximate neural network predictions. An LMUT is learned using a novel on-line algorithm that is well-suited for an active play setting, where the mimic learner observes an ongoing interaction between the neural net and the environment. Empirical evaluation shows that an LMUT mimics a Q function substantially better than five baseline methods. The transparent tree structure of an LMUT facilitates understanding the network’s learned strategic knowledge by analyzing feature influence, extracting rules, and highlighting the super-pixels in image inputs.},
  doi={10.1007/978-3-030-10928-8_25}
}

@article{LiuGuoMahmud:2021:HITLErrorDetectionFramework,
  title={When and Why does a Model Fail? A Human-in-the-loop Error Detection Framework for Sentiment Analysis},
  author={Liu, Zhe and Guo, Yufan and Mahmud, Jalal},
  journal={arXiv:2106.00954},
  year={2021},
  abstract={Although deep neural networks have been widely employed and proven effective in sen- timent analysis tasks, it remains challenging for model developers to assess their models for erroneous predictions that might exist prior to deployment. Once deployed, emergent er- rors can be hard to identify in prediction run- time and impossible to trace back to their sources. To address such gaps, in this paper we propose an error detection framework for sentiment analysis based on explainable fea- tures. We perform global-level feature valida- tion with human-in-the-loop assessment, fol- lowed by an integration of global and local- level feature contribution analysis. Experimen- tal results show that, given limited human-in- the-loop intervention, our method is able to identify erroneous model predictions on un- seen data with high precision.}
}

@article{LuetjensEverettHow:2018:RLModelUncertainty,
  author    = {Bjoern Lutjens and
               Michael Everett and
               Jonathan P. How},
  title     = {Safe Reinforcement Learning with Model Uncertainty Estimates},
  journal   = {CoRR},
  volume    = {abs/1810.08700},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.08700},
  eprinttype = {arXiv},
  eprint    = {1810.08700},
  timestamp = {Wed, 31 Oct 2018 14:24:29 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-08700.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract  = {Many current autonomous systems are being designed with a strong reliance on black box predictions from deep neural networks (DNNs). However, DNNs tend to be overconfident in predictions on unseen data and can give unpredictable results for far-from-distribution test data. The importance of predictions that are robust to this distributional shift is evident for safety-critical applications, such as collision avoidance around pedestrians. Measures of model uncertainty can be used to identify unseen data, but the state-of-the-art extraction methods such as Bayesian neural networks are mostly intractable to compute. This paper uses MC-Dropout and Bootstrapping to give computationally tractable and parallelizable uncertainty estimates. The methods are embedded in a Safe Reinforcement Learning framework to form uncertainty-aware navigation around pedestrians. The result is a collision avoidance policy that knows what it does not know and cautiously avoids pedestrians that exhibit unseen behavior. The policy is demonstrated in simulation to be more robust to novel observations and take safer actions than an uncertainty-unaware baseline.}
}
%%% MMM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{macglashan2017interactive,
  title={Interactive learning from policy-dependent human feedback},
  author={MacGlashan, James and Ho, Mark K and Loftin, Robert and Peng, Bei and Wang, Guan and Roberts, David L and Taylor, Matthew E and Littman, Michael L},
  booktitle={International Conference on Machine Learning},
  pages={2285--2294},
  year={2017},
  organization={PMLR}
  }
  
@inproceedings{MadumalEtAl:2020:CausalRLCFs,
  title={Explainable reinforcement learning through a causal lens},
  author={Madumal, Prashan and Miller, Tim and Sonenberg, Liz and Vetere, Frank},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={2493--2500},
  year={2020}
}

@article{Madumal:2020:DistalEF,
  title={Distal Explanations for Model-free Explainable Reinforcement Learning},
  author={Prashan Madumal and Tim Miller and Liz Sonenberg and Frank Vetere},
  journal={arXiv: Artificial Intelligence},
  year={2020}
}


% there is no doi for the following paper
@inproceedings{MandelEtAl:2017ActionsInHITL,
  title={Where to add actions in human-in-the-loop reinforcement learning},
  author={Mandel, Travis and Liu, Yun-En and Brunskill, Emma and Popovi{\'c}, Zoran},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017},
  abstract={In order for reinforcement learning systems to learn quickly in vast action spaces such as the space of all possible pieces of text or the space of all images, leveraging human intuition and creativity is key. However, a human-designed action space is likely to be initially imperfect and limited; furthermore, humans may improve at creating useful actions with practice or new information. Therefore, we propose a framework in which a human adds actions to a reinforcement learning system over time to boost performance. In this setting, however, it is key that we use human effort as efficiently as possible, and one significant danger is that humans waste effort adding actions at places (states) that aren't very important. Therefore, we propose Expected Local Improvement (ELI), an automated method which selects states at which to query humans for a new action. We evaluate ELI on a variety of simulated domains adapted from the literature, including domains with over a million actions and domains where the simulated experts change over time. We find ELI demonstrates excellent empirical performance, even in settings where the synthetic "experts" are quite poor.}
}

@misc{Marletto:2021:ScienceCanCant,
  title={The Science of Can and Can’t: A Physicist’s Journey through the Land of Counterfactuals},
  author={Marletto, C},
  year={2021},
  publisher={Vintage Books: New York, NY, USA}
}

@misc{MatthewsonPilarski:2022:DesigningAndEvaluatingHCIML,
      title={A Brief Guide to Designing and Evaluating Human-Centered Interactive Machine Learning}, 
      author={Kory W. Mathewson and Patrick M. Pilarski},
      year={2022},
      eprint={2204.09622},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      abstract = {Interactive machine learning (IML) is a field of research that explores how to leverage both human and computational abilities in decision making systems. IML represents a collaboration between multiple complementary human and machine intelligent systems working as a team, each with their own unique abilities and limitations. This teamwork might mean that both systems take actions at the same time, or in sequence. Two major open research questions in the field of IML are: "How should we design systems that can learn to make better decisions over time with human interaction?" and "How should we evaluate the design and deployment of such systems?" A lack of appropriate consideration for the humans involved can lead to problematic system behaviour, and issues of fairness, accountability, and transparency. Thus, our goal with this work is to present a human-centred guide to designing and evaluating IML systems while mitigating risks. This guide is intended to be used by machine learning practitioners who are responsible for the health, safety, and well-being of interacting humans. An obligation of responsibility for public interaction means acting with integrity, honesty, fairness, and abiding by applicable legal statutes. With these values and principles in mind, we as a machine learning research community can better achieve goals of augmenting human skills and abilities. This practical guide therefore aims to support many of the responsible decisions necessary throughout the iterative design, development, and dissemination of IML systems. }
}

@article{Mehrabi:2021:SurveyBiasFairness,
  title={A survey on bias and fairness in machine learning},
  author={Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={6},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA},
  abstract={With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.}
}

@article{Mnih:2013:PlayingAtariDeepRL,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013},
  abstract={We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.}
}

@article{MuratoreEtAl:2021:RobotLearningReview,
  title={Robot Learning from Randomized Simulations: A Review},
  author={Muratore, Fabio and Ramos, Fabio and Turk, Greg and Yu, Wenhao and Gienger, Michael and Peters, Jan},
  journal={arXiv preprint arXiv:2111.00956},
  year={2021},
  abstract={The rise of deep learning has caused a paradigm shift in robotics research, favoring methods that require large amounts of data. It is prohibitively expensive to generate such data sets on a physical platform. Therefore, state-of-the-art approaches learn in simulation where data generation is fast as well as inexpensive and subsequently transfer the knowledge to the real robot (sim-to-real). Despite becoming increasingly realistic, all simulators are by construction based on models, hence inevitably imperfect. This raises the question of how simulators can be modified to facilitate learning robot control policies and overcome the mismatch between simulation and reality, often called the 'reality gap'. We provide a comprehensive review of sim-to-real research for robotics, focusing on a technique named 'domain randomization' which is a method for learning from randomized simulations. }
}

%%% NNN %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{nair2018overcoming,
  title={Overcoming exploration in reinforcement learning with demonstrations},
  author={Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={6292--6299},
  year={2018},
  organization={IEEE},
  abstract={Leveraging demonstrations in Deep RL using auxiliary behavior cloning objective function, handling sub-optimal demonstrations by considering behavior cloning loss where demonstrator policy is better than the actor network policy. The proposed approach shows promise on even harder robotics task like Pushing, Sliding, Pick and place as compared to the tasks considered in previous work}
}

@article{Najar:2021:RLWithHumanAdvice,
  title={Reinforcement learning with human advice: a survey},
  author={Najar, Anis and Chetouani, Mohamed},
  journal={Frontiers in Robotics and AI},
  volume={8},
  year={2021},
  publisher={Frontiers Media SA},
  abstract={In this paper, we provide an overview of the existing methods for integrating human advice into a reinforcement learning process. We first propose a taxonomy of the different forms of advice that can be provided to a learning agent. We then describe the methods that can be used for interpreting advice when its meaning is not determined beforehand. Finally, we review different approaches for integrating advice into the learning process.}
}

@article{NeftciAverbeck:2019:RLBiologicalSystems,
  title={Reinforcement learning in artificial and biological systems},
  author={Neftci, Emre O and Averbeck, Bruno B},
  journal={Nature Machine Intelligence},
  volume={1},
  number={3},
  pages={133--143},
  year={2019},
  publisher={Nature Publishing Group},
  abstract={There is and has been a fruitful flow of concepts and ideas between studies of learning in biological and artificial systems. Much early work that led to the development of reinforcement learning (RL) algorithms for artificial systems was inspired by learning rules first developed in biology by Bush and Mosteller, and Rescorla and Wagner. More recently, temporal-difference RL, devel- oped for learning in artificial agents, has provided a foundational framework for interpreting the activity of dopamine neurons. In this Review, we describe state-of-the-art work on RL in biological and artificial agents. We focus on points of contact between these disciplines and identify areas where future research can benefit from information flow between these fields. Most work in biological systems has focused on simple learning problems, often embedded in dynamic environments where flexibility and ongoing learning are important, similar to real-world learning problems faced by biological systems. In contrast, most work in artificial agents has focused on learning a single complex problem in a static environment. Moving forward, work in each field will benefit from a flow of ideas that represent the strengths within each discipline.}
}


@article{NieBrunskillWagner:2021:LearningPolicies,
  title={Learning when-to-treat policies},
  author={Nie, Xinkun and Brunskill, Emma and Wager, Stefan},
  journal={Journal of the American Statistical Association},
  volume={116},
  number={533},
  pages={392--409},
  year={2021},
  publisher={Taylor \& Francis},
  doi={10.1080/01621459.2020.1831925},
  abstract={Many applied decision-making problems have a dynamic component: The policymaker needs not only to choose whom to treat, but also when to start which treatment. For example, a medical doctor may choose between postponing treatment (watchful waiting) and prescribing one of several available treatments during the many visits from a patient. We develop an “advantage doubly robust” estimator for learning such dynamic treatment rules using observational data under the assumption of sequential ignorability. We prove welfare regret bounds that generalize results for doubly robust learning in the single-step setting, and show promising empirical performance in several different contexts. Our approach is practical for policy optimization, and does not need any structural (e.g., Markovian) assumptions.}
}

@inproceedings{ng:99,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={Icml},
  volume={99},
  pages={278--287},
  year={1999},
  abstract ={This paper investigates conditions under which
modifications to the reward function of a Markov decision process preserve the optimal policy. It is shown that, besides the positive linear transformation familiar from utility theory, one can add a reward for transitions between states that is expressible as the difference in value of an arbitrary potential function applied to those states. Furthermore, this is shown to be a necessary condition for invariance, in the sense that any other transformation may yield suboptimal policies unless further assumptions are made about the underlying MDP. These results shed light on the practice of reward shaping, a method used in reinforcement learning whereby additional training rewards are used to guide the learning agent. In particular, some well-known "bugs" in reward shaping procedures are shown to arise from non-potential-based rewards, and methods are given for constructing shaping potentials corresponding to distance-based and subgoal-based heuristics. We show that such potentials can lead to substantial reductions in learning time.}
}

@inproceedings{nguyen2019review,
  title={Review of deep reinforcement learning for robot manipulation},
  author={Nguyen, Hai and La, Hung},
  booktitle={2019 Third IEEE International Conference on Robotic Computing (IRC)},
  pages={590--595},
  year={2019},
  organization={IEEE}
}
%%% PPP %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{PapallasEtAl:2020:OnlineReplanningTrajectories,
  title={Online Replanning with Human-in-The-Loop for Non-Prehensile Manipulation in Clutter—A Trajectory Optimization based Approach},
  author={Papallas, Rafael and Cohn, Anthony G and Dogar, Mehmet R},
  journal={IEEE Robotics and Automation Letters},
  volume={5},
  number={4},
  pages={5377--5384},
  year={2020},
  publisher={IEEE},
  abstract={We are interested in the problem where a number of robots, in parallel, are trying to solve reaching through clutter problems in a simulated warehouse setting. In such a setting, we investigate the performance increase that can be achieved by using a human-in-the-loop providing guidance to robot planners. These manipulation problems are challenging for autonomous planners as they have to search for a solution in a high-dimensional space. In addition, physics simulators suffer from the uncertainty problem where a valid trajectory in simulation can be invalid when executing the trajectory in the real-world. To tackle these problems, we propose an online-replanning method with a human-in-the-loop. This system enables a robot to plan and execute a trajectory autonomously, but also to seek high-level suggestions from a human operator if required at any point during execution. This method aims to minimize the human effort required, thereby increasing the number of robots that can be guided in parallel by a single human operator. We performed experiments in simulation and on a real robot, using an experienced and a novice operator. Our results show a significant increase in performance when using our approach in a simulated warehouse scenario and six robots.},
  doi={10.1109/LRA.2020.3006826}
}

@book{Pearl:2009:Causality,
   year = {2009},
   author = {Pearl, Judea},
   title = {Causality: Models, Reasoning, and Inference (2nd Edition)},
   publisher = {Cambridge University Press},
   address = {Cambridge},
   abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 3,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.}
}

@article{Pearl:2018:CausalCounterfactualInference,
  title={Causal and counterfactual inference},
  author={Pearl, Judea},
  journal={The Handbook of Rationality},
  pages={1--41},
  year={2018},
  abstract={All accounts of rationality presuppose knowledge of how actions affect the state of the world and how the world would change had alternative actions been taken. The paper presents a framework called Structural Causal Model (SCM) which operationalizes this knowledge and explicates how it can be derived from both theories and data. In particular, we show how counterfactuals are computed and how they can be embedded in a calculus that solves critical problems in the empirical sciences}
}

@book{Pearl:2018:TheBookOfWhy,
  title={The book of why: the new science of cause and effect},
  author={Pearl, Judea and Mackenzie, Dana},
  year={2018},
  publisher={Basic books}
}

@article{Pearl:2000:ModelsReasoningInference,
  title={Models, reasoning and inference},
  author={Pearl, Judea and others},
  journal={Cambridge, UK: CambridgeUniversityPress},
  volume={19},
  pages={2},
  year={2000}
}

@article{Pfeifer:2021:NetworkModuleDetection,
  title={Network module detection from multi-modal node features with a greedy decision forest for actionable explainable ai},
  author={Pfeifer, Bastian and Saranti, Anna and Holzinger, Andreas},
  journal={arXiv preprint arXiv:2108.11674},
  year={2021},
  abstract={Network-based algorithms are used in most domains of research and industry in a wide variety of applications and are of great practical use. In this work, we demonstrate subnetwork detection based on multi-modal node features using a new Greedy Decision Forest for better interpretability. The latter will be a crucial factor in retaining experts and gaining their trust in such algorithms in the future. To demonstrate a concrete application example, we focus in this paper on bioinformatics and systems biology with a special focus on biomedicine. However, our methodological approach is applicable in many other domains as well. Systems biology serves as a very good example of a field in which statistical data-driven machine learning enables the analysis of large amounts of multi-modal biomedical data. This is important to reach the future goal of precision medicine, where the complexity of patients is modeled on a system level to best tailor medical decisions, health practices and therapies to the individual patient. Our glass-box approach could help to uncover disease-causing network modules from multi-omics data to better understand diseases such as cancer.}
}

@article{popova2018deep,
  title={Deep reinforcement learning for de novo drug design},
  author={Popova, Mariya and Isayev, Olexandr and Tropsha, Alexander},
  journal={Science advances},
  volume={4},
  number={7},
  pages={eaap7885},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{PujolDustdar:2021:FogRobotics,
  title={Fog Robotics—Understanding the Research Challenges},
  author={Pujol, Victor Casamayor and Dustdar, Schahram},
  journal={IEEE Internet Computing},
  volume={25},
  number={5},
  pages={10--17},
  year={2021},
  publisher={IEEE},
  abstract={Fog robotics is an emerging topic that derives from cloud robotics, but similarly as fog computing, the applications require low latency connections in order to be useful in real life environments. This article presents a new perspective to the topic in which not only robotics takes advantage of the fog computing paradigm, but fog computing is able to leverage the robotics technology in order to enhance its features. This work highlights the benefits obtained by each technology when it is mixed with the other and sketches the relevant topics to research in order to make this partnership possible.},
  doi={10.1109/MIC.2021.3060963}
}

@inproceedings{PuiuttaVeith:2020:xAIRLSurvey,
  title={Explainable reinforcement learning: A survey},
  author={Puiutta, Erika and Veith, Eric MSP},
  booktitle={International Cross-Domain Conference for Machine Learning and Knowledge Extraction},
  pages={77--95},
  year={2020},
  organization={Springer},
  abstract={Explainable Artificial Intelligence (XAI), i.e., the development of more transparent and interpretable AI models, has gained increased traction over the last few years. This is due to the fact that, in conjunction with their growth into powerful and ubiquitous tools, AI models exhibit one detrimental characteristic: a performance-transparency trade-off. This describes the fact that the more complex a model’s inner workings, the less clear it is how its predictions or decisions were achieved. But, especially considering Machine Learning (ML) methods like Reinforcement Learning (RL) where the system learns autonomously, the necessity to understand the underlying reasoning for their decisions becomes apparent. Since, to the best of our knowledge, there exists no single work offering an overview of Explainable Reinforcement Learning (XRL) methods, this survey attempts to address this gap. We give a short summary of the problem, a definition of important terms, and offer a classification and assessment of current XRL methods. We found that a) the majority of XRL methods function by mimicking and simplifying a complex model instead of designing an inherently simple one, and b) XRL (and XAI) methods often neglect to consider the human side of the equation, not taking into account research from related fields like psychology or philosophy. Thus, an interdisciplinary effort is needed to adapt the generated explanations to a (non-expert) human user in order to effectively progress in the field of XRL and XAI in general.},
  doi={doi.org/10.1007/978-3-030-57321-8_5}
}

@book{Pumperla:2019:DeepLearningGameOfGo,
  title={Deep learning and the game of Go},
  author={Pumperla, Max and Ferguson, Kevin},
  volume={231},
  year={2019},
  publisher={Manning Publications Company},
}

%%% RRR %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{Ribeiro:2016:WhyShouldITrustYou,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016},
  abstract={Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.}
}
@article{RodriguezEtAl:2021:DeepCovidxAI,
  title={Deepcovid: An operational deep learning-driven framework for explainable real-time covid-19 forecasting},
  author={Rodriguez, Alexander and Tabassum, Anika and Cui, Jiaming and Xie, Jiajia and Ho, Javen and Agarwal, Pulak and Adhikari, Bijaya and Prakash, B Aditya},
  journal={medRxiv},
  pages={2020--09},
  year={2021},
  publisher={Cold Spring Harbor Laboratory Press},
  abstract={How do we forecast an emerging pandemic in real time in a purely data-driven manner? How to leverage rich heterogeneous data based on various signals such as mobility, testing, and/or disease exposure for forecasting? How to handle noisy data and generate uncertainties in the forecast? In this paper, we present DEEPCOVID, an operational deep learning framework designed for real-time COVID-19 forecasting. DEEP-COVID works well with sparse data and can handle noisy heterogeneous data signals by propagating the uncertainty from the data in a principled manner resulting in meaningful uncertainties in the forecast. The deployed framework also consists of modules for both real-time and retrospective exploratory analysis to enable interpretation of the forecasts. Results from real-time predictions (featured on the CDC website and FiveThirtyEight.com) since April 2020 indicates that our approach is competitive among the methods in the COVID-19 Forecast Hub, especially for short-term predictions.}
}


@article{RoyEtAl:2021:RLRoboticsChallenges,
  title={From Machine Learning to Robotics: Challenges and Opportunities for Embodied Intelligence},
  author={Roy, Nicholas and Posner, Ingmar and Barfoot, Tim and Beaudoin, Philippe and Bengio, Yoshua and Bohg, Jeannette and Brock, Oliver and Depatie, Isabelle and Fox, Dieter and Koditschek, Dan and others},
  journal={arXiv preprint arXiv:2110.15245},
  year={2021},
  abstract={Machine learning has long since become a keystone technology, accelerating science and applications in a broad range of domains. Consequently, the notion of applying learning methods to a particular problem set has become an established and valuable modus operandi to advance a particular field. In this article we argue that such an approach does not straightforwardly extended to robotics — or to embodied intelligence more generally: systems which engage in a purposeful exchange of energy and information with a physical environment. In particular, the purview of embodied intelligent agents extends significantly beyond the typical considerations of main-stream machine learning approaches, which typically (i) do not consider operation under conditions significantly different from those encountered during training; (ii) do not consider the often substantial, long-lasting and potentially safety-critical nature of interactions during learning and deployment; (iii) do not require ready adaptation to novel tasks while at the same time (iv) effectively and efficiently curating and extending their models of the world through targeted and deliberate actions. In reality, therefore, these limitations result in learning-based systems which suffer from many of the same operational shortcomings as more traditional, engineering-based approaches when deployed on a robot outside a well defined, and often narrow operating envelope. Contrary to viewing embodied intelligence as another application domain for machine learning, here we argue that it is in fact a key driver for the advancement of machine learning technology. In this article our goal is to highlight challenges and opportunities that are specific to embodied intelligence and to propose research directions which may significantly advance the state-of-the-art in robot learning.}
}

@inproceedings{RuanEtAl:2019:Quizbot,
  title={Quizbot: A dialogue-based adaptive learning system for factual knowledge},
  author={Ruan, Sherry and Jiang, Liwei and Xu, Justin and Tham, Bryce Joe-Kun and Qiu, Zhengneng and Zhu, Yeshuang and Murnane, Elizabeth L and Brunskill, Emma and Landay, James A},
  booktitle={Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages={1--13},
  year={2019},
  doi={10.1145/3290605.3300587},
  abstract={
  Advances in conversational AI have the potential to enable more engaging and effective ways to teach factual knowledge. To investigate this hypothesis, we created QuizBot, a dialogue-based agent that helps students learn factual knowledge in science, safety, and English vocabulary. We evaluated QuizBot with 76 students through two within-subject studies against a flashcard app, the traditional medium for learning factual knowledge. Though both systems used the same algorithm for sequencing materials, QuizBot led to students recognizing (and recalling) over 20\% more correct answers than when students used the flashcard app. Using a conversational agent is more time consuming to practice with, but in a second study, of their own volition, students spent 2.6x more time learning with QuizBot than with flashcards and reported preferring it strongly for casual learning. Our results in this second study showed QuizBot yielded improved learning gains over flashcards on recall. These results suggest that educational chatbot systems may have beneficial use, particularly for learning outside of traditional settings.}
}

%%% SSS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{SalviniEtAl:2021:ISO13482:2014,
  author = {Salvini, Pericle and Paez-Granados, Diego and Billard, Aude},
  title = {On the Safety of Mobile Robots Serving in Public Spaces: Identifying Gaps in EN ISO 13482:2014 and Calling for a New Standard},
  year = {2021},
  journal = {Transactions on Human-Robot Interaction},
  issue_date = {September 2021},
  publisher = {Association for Computing Machinery},
  volume = {10},
  number = {3},
  numpages = {27},
  abstract = {Since 2014, a specific standard has been dedicated for the safety certification of personal care robots, which operate in close proximity to humans. These robots serve as information providers, object transporters, personal mobility carriers, and security patrollers. In this article, we point out the shortcomings concerning EN ISO 13482:2014, which encompasses guidelines regarding the safety and design of personal care robots. In particular, we argue that the current standard is not suitable for guaranteeing people's safety when these robots operate in public spaces. Specifically, the standard lacks requirements to protect pedestrians and bystanders. The guideline implicitly assumes that private spaces, such as households and offices, present the same hazards as in public spaces. We highlight the existence of at least three properties pertaining to robots’ use in public spaces. These properties include (1) crowds, (2) social norms and proxemics rules, and (3) people's misbehaviours. We discuss how these properties impact robots’ safety. This article aims to raise stakeholders’ awareness on individuals’ safety when robots are deployed in public spaces. This could be achieved by integrating the gaps present in EN ISO 13482:2014 or by creating a new dedicated standard.},
  doi = {10.1145/3442678}
}

@inproceedings{Saranti:2019:LearningCompetencePGMs,
  title={Insights into learning competence through probabilistic graphical models},
  author={Saranti, Anna and Taraghi, Behnam and Ebner, Martin and Holzinger, Andreas},
  booktitle={International cross-domain conference for machine learning and knowledge extraction},
  pages={250--271},
  year={2019},
  organization={Springer},
  abstract={One-digit multiplication problems is one of the major fields in learning mathematics at the level of primary school that has been studied over and over. However, the majority of related work is focusing on descriptive statistics on data from multiple surveys. The goal of our research is to gain insights into multiplication misconceptions by applying machine learning techniques. To reach this goal, we trained a probabilistic graphical model of the students’ misconceptions from data of an application for learning multiplication. The use of this model facilitates the exploration of insights into human learning competence and the personalization of tutoring according to individual learner’s knowledge states. The detection of all relevant causal factors of the erroneous students answers as well as their corresponding relative weight is a valuable insight for teachers. Furthermore, the similarity between different multiplication problems - according to the students behavior - is quantified and used for their grouping into clusters. Overall, the proposed model facilitates real-time learning insights that lead to more informed decisions.}
}

@incollection{Saranti:2009:QuantumHarmonicOscSonification,
  title={Quantum harmonic oscillator sonification},
  author={Saranti, Anna and Eckel, Gerhard and Pirr{\'o}, David},
  booktitle={Auditory Display},
  pages={184--201},
  year={2009},
  publisher={Springer},
  abstract={This work deals with the sonification of a quantum mechanical system and the processes that occur as a result of its quantum mechanical nature and interactions with other systems. The quantum harmonic oscillator is not only regarded as a system with sonifiable characteristics but also as a storage medium for quantum information. By representing sound information quantum mechanically and storing it in the system, every process that unfolds on this level is inherited and reflected by the sound. The main profit of this approach is that the sonification can be used as a first insight for two models: a quantum mechanical system model and a quantum computation model.}
}


@article{SchnakeMontavon:2020:XAIgraphs,
   year = {2020},
   author = {Schnake, Thomas and Eberle, Oliver and Lederer, Jonas and Nakajima, Shinichi and Schütt, Kristof T. and Müller, Klaus-Robert and Montavon, Grégoire},
   title = {XAI for Graphs: Explaining Graph Neural Network Predictions by Identifying Relevant Walks},
   journal = {arXiv:2006.03589},
   abstract = {Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph into the neural network structure, common explainable AI (XAI) approaches are not applicable. To a large extent, GNNs have remained black-boxes for the user so far. In this paper, we contribute by proposing a new XAI approach for GNNs. Our approach is derived from high-order Taylor expansions and is able to generate a decomposition of the GNN prediction as a collection of relevant walks on the input graph. We find that these high-order Taylor expansions can be equivalently (and more simply) computed using multiple backpropagation passes from the top layer of the GNN to the first layer. The explanation can then be further robustified and generalized by using layer-wise-relevance propagation (LRP) in place of the standard equations for gradient propagation. Our novel method which we denote as `GNN-LRP' is tested on scale-free graphs, sentence parsing trees, molecular graphs, and pixel lattices representing images. In each case, it performs stably and accurately, and delivers interesting and novel application insights.}
}



@incollection{Schneeberger:2020:legalAI,
   year = {2020},
   author = {Schneeberger, David and Stoeger, Karl and Holzinger, Andreas},
   title = {The European legal framework for medical AI},
   booktitle = {International Cross-Domain Conference for Machine Learning and Knowledge Extraction, Springer LNCS 12279},
   publisher = {Springer},
   pages = {209--226},
   doi = {10.1007/978-3-030-57321-8_12}
}


@article{Scurto:2021:DesigningDeepRLHumanParameterExploration,
  title={Designing deep reinforcement learning for human parameter exploration},
  author={Scurto, Hugo and Kerrebroeck, Bavo Van and Caramiaux, Baptiste and Bevilacqua, Fr{\'e}d{\'e}ric},
  journal={ACM Transactions on Computer-Human Interaction (TOCHI)},
  volume={28},
  number={1},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA},
  abstract={Software tools for generating digital sound often present users with high-dimensional, parametric interfaces, that may not facilitate exploration of diverse sound designs. In this article, we propose to investigate artificial agents using deep reinforcement learning to explore parameter spaces in partnership with users for sound design. We describe a series of user-centred studies to probe the creative benefits of these agents and adapting their design to exploration. Preliminary studies observing users’ exploration strategies with parametric interfaces and testing different agent exploration behaviours led to the design of a fully-functioning prototype, called Co-Explorer, that we evaluated in a workshop with professional sound designers. We found that the Co-Explorer enables a novel creative workflow centred on human–machine partnership, which has been positively received by practitioners. We also highlight varied user exploration behaviours throughout partnering with our system. Finally, we frame design guidelines for enabling such co-exploration workflow in creative digital applications.}
}

@book{ShneidermanEtAl:2016:GoldenRulesHCI,
  title={Designing the user interface: strategies for effective human-computer interaction},
  author={Shneiderman, Ben and Plaisant, Catherine and Cohen, Maxine S and Jacobs, Steven and Elmqvist, Niklas and Diakopoulos, Nicholas},
  year={2016},
  publisher={Pearson}
}

@article{ShteingartLoewenstein:2014:RLHumanBehavior,
title = {Reinforcement learning and human behavior},
journal = {Current Opinion in Neurobiology},
volume = {25},
pages = {93-98},
year = {2014},
note = {Theoretical and computational neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2013.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959438813002286},
author = {Hanan Shteingart and Yonatan Loewenstein},
abstract = {The dominant computational approach to model operant learning and its underlying neural activity is model-free reinforcement learning (RL). However, there is accumulating behavioral and neuronal-related evidence that human (and animal) operant learning is far more multifaceted. Theoretical advances in RL, such as hierarchical and model-based RL extend the explanatory power of RL to account for some of these findings. Nevertheless, some other aspects of human behavior remain inexplicable even in the simplest tasks. Here we review developments and remaining challenges in relating RL models to human operant learning. In particular, we emphasize that learning a model of the world is an essential step before or in parallel to learning the policy in RL and discuss alternative models that directly learn a policy without an explicit world model in terms of state-action pairs.}
}

@article{ShuXiongSocher:2017:HierarchicalTaskExplanation,
  title={Hierarchical and interpretable skill acquisition in multi-task reinforcement learning},
  author={Shu, Tianmin and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1712.07294},
  year={2017}
}


@article{Silver:2018:AlphaZero,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science},
  abstract={The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.}
}

@article{SongEtAl:2019:ExplainableGraphBasedRecommendations,
  author    = {Weiping Song and
               Zhijian Duan and
               Ziqing Yang and
               Hao Zhu and
               Ming Zhang and
               Jian Tang},
  title     = {Explainable Knowledge Graph-based Recommendation via Deep Reinforcement
               Learning},
  journal   = {CoRR},
  volume    = {abs/1906.09506},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.09506},
  eprinttype = {arXiv},
  eprint    = {1906.09506},
  timestamp = {Sun, 14 Nov 2021 15:51:43 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-09506.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {This paper studies recommender systems with knowledge graphs, which can effectively address the problems of data sparsity and cold start. Recently, a variety of methods have been developed for this problem, which generally try to learn effective representations of users and items and then match items to users according to their representations. Though these methods have been shown quite effective, they lack good explanations, which are critical to recommender systems. In this paper, we take a different route and propose generating recommendations by finding meaningful paths from users to items. Specifically, we formulate the problem as a sequential decision process, where the target user is defined as the initial state, and the edges on the graphs are defined as actions. We shape the rewards according to existing state-of-the-art methods and then train a policy function with policy gradient methods. Experimental results on three real-world datasets show that our proposed method not only provides effective recommendations but also offers good explanations}
}


@article{SrinivasLaskinAbbeel:2020:ContrastiveUnsupervisedRL,
  title={Curl: Contrastive unsupervised representations for reinforcement learning},
  author={Srinivas, Aravind and Laskin, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2004.04136},
  year={2020},
  abstract={We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features}
}

@article{Stoeger:2021:MedicalAI,
   year = {2021},
   author = {Stoeger, Karl and Schneeberger, David and Holzinger, Andreas},
   title = {Medical Artificial Intelligence: The European Legal Perspective},
   journal = {Communications of the ACM},
   volume = {64},
   number = {11},
   pages = {34--36},
   abstract = {Although the European Commission proposed new legislation for the use of "high-risk artificial intelligence" earlier this year, the existing European fundamental rights framework already provides some clear guidance on the use of medical AI.},
   doi = {10.1145/3458652}
}

@inproceedings{SturmEtAl:2015:InteractiveHeatmap,
   year = {2015},
   author = {Sturm, Werner and Schaefer, Till and Schreck, Tobias and Holzinger, Andeas and Ullrich, Torsten},
   title = {Extending the Scaffold Hunter Visualization Toolkit with Interactive Heatmaps },
   booktitle = {EG UK Computer Graphics and Visual Computing CGVC 2015},
   editor = {Borgo, Rita and Turkay, Cagatay},
   publisher = {Euro Graphics (EG)},
   pages = {77--84},
   abstract = {In many application areas, large amounts of data arise, which are often hard to interpret or make use of by humans. Interactive visualization can help to overview and explore large amounts of data. An example is in the life sciences, where databases of chemical compounds need to be analyzed in terms of similarities of molecular properties. Scientists then need to explore this data in an efficient way. The Scaffold Hunter framework is an Open Source software system for interactive visualization of high dimensional data. In this paper, we present an extension of Scaffold Hunter with an interactive heatmap, which ties in tightly with a dendrogram visualization. We added specific interaction modalities and views tailored to the analysis of chemical compounds. Zooming capabilities allow to start from an overview of the data (showing all data elements at once) down to a detail-on-demand view which includes chemical structural views of molecules. We show how the interactive heatmap with clustered rows and columns can bring new insights into the data regarding various properties. The implementation is made available for researchers and practitioners to use. },
   keywords = {interactive visualization, interactive heatmaps, visusal analytics, chemical compounds, chemical molecules},
   doi = {10.2312/cgvc.20151247}
}

@inproceedings{suay:11,
  title={Effect of human guidance and state space size on interactive reinforcement learning},
  author={Suay, Halit Bener and Chernova, Sonia},
  booktitle={2011 Ro-Man},
  pages={1--6},
  year={2011},
  organization={IEEE},
  abstract ={The Interactive Reinforcement Learning algorithm enables a human user to train a robot by providing rewards in response to past actions and anticipatory guidance to guide the selection of future actions. Past work with soft- ware agents has shown that incorporating user guidance into the policy learning process through Interactive Reinforcement Learning significantly improves the policy learning time by reducing the number of states the agent explores. We present the first study of Interactive Reinforcement Learning in real- world robotic systems. We report on four experiments that study the effects that teacher guidance and state space size have on policy learning performance. We discuss modifications made to apply Interactive Reinforcement Learning to a real- world system and show that guidance significantly reduces the learning rate, and that its positive effects increase with state space size.
}
}
@article{Sun:2021:TopologyPerturbationGNNs,
  title={Preserve, Promote, or Attack? GNN Explanation via Topology Perturbation},
  author={Sun, Yi and Valente, Abel and Liu, Sijia and Wang, Dakuo},
  journal={arXiv preprint arXiv:2103.13944},
  year={2021},
  abstract={Prior works on formalizing explanations of a graph neural network (GNN) focus on a single use case - to preserve the prediction results through identifying important edges and nodes. In this paper, we develop a multi-purpose interpretation framework by acquiring a mask that indicates topology perturbations of the input graphs. We pack the framework into an interactive visualization system (GNNViz) which can fulfill multiple purposes: Preserve,Promote, or Attack GNN's predictions. We illustrate our approach's novelty and effectiveness with three case studies: First, GNNViz can assist non expert users to easily explore the relationship between graph topology and GNN's decision (Preserve), or to manipulate the prediction (Promote or Attack) for an image classification task on MS-COCO; Second, on the Pokec social network dataset, our framework can uncover unfairness and demographic biases; Lastly, it compares with state-of-the-art GNN explainer baseline on a synthetic dataset.}
}

@book{SuttonBarto:2018:RLIntroduction,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

%%% TTT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{TabrezHayes:2019:xRLTextualExplanations,
  title={Improving human-robot interaction through explainable reinforcement learning},
  author={Tabrez, Aaquib and Hayes, Bradley},
  booktitle={2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  pages={751--753},
  year={2019},
  organization={IEEE},
  doi={10.1109/HRI.2019.8673198},
  abstract={Gathering the most informative data from humans without overloading them remains an active research area in AI, and is closely coupled with the problems of determining how and when information should be communicated to others [12]. Current decision support systems (DSS) are still overly simple and static, and cannot adapt to changing environments we expect to deploy in modern systems [3], [4], [9], [11]. They are intrinsically limited in their ability to explain rationale versus merely listing their future behaviors, limiting a human's understanding of the system [2], [7]. Most probabilistic assessments of a task are conveyed after the task/skill is attempted rather than before [10], [14], [16]. This limits failure recovery and danger avoidance mechanisms. Existing work on predicting failures relies on sensors to accurately detect explicitly annotated and learned failure modes [13]. As such, important non-obvious pieces of information for assessing appropriate trust and/or course-of-action (COA) evaluation in collaborative scenarios can go overlooked, while irrelevant information may instead be provided that increases clutter and mental workload. Understanding how AI models arrive at specific decisions is a key principle of trust [8]. Therefore, it is critically important to develop new strategies for anticipating, communicating, and explaining justifications and rationale for AI driven behaviors via contextually appropriate semantics.}
}

@inproceedings{taylor2011integrating,
  title={Integrating reinforcement learning with human demonstrations of varying ability},
  author={Taylor, Matthew E and Suay, Halit Bener and Chernova, Sonia},
  booktitle={The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2},
  pages={617--624},
  year={2011},
  organization={Citeseer}
}
@article{TaylorEtAl:2021:InteractiveRLHIPPOGym,
  title={Improving Reinforcement Learning with Human Assistance: An Argument for Human Subject Studies with HIPPO Gym},
  author={Taylor, Matthew E and Nissen, Nicholas and Wang, Yuan and Navidi, Neda},
  journal={arXiv preprint arXiv:2102.02639},
  year={2021},
  abstract={Reinforcement learning (RL) is a popular machine learning paradigm for game playing, robotics control, and other sequential decision tasks. However, RL agents often have long learning times with high data requirements because they begin by acting randomly. In order to better learn in complex tasks, this article argues that an external teacher can often significantly help the RL agent learn. OpenAI Gym is a common framework for RL research, including a large number of standard environments and agents, making RL research significantly more accessible. This article introduces our new open-source RL framework, the Human Input Parsing Platform for Openai Gym (HIPPO Gym), and the design decisions that went into its creation. The goal of this platform is to facilitate human-RL research, again lowering the bar so that more researchers can quickly investigate different ways that human teachers could assist RL agents, including learning from demonstrations, learning from feedback, or curriculum learning.}
}

@inproceedings{torrey2013teaching,
  title={Teaching on a budget: Agents advising agents in reinforcement learning},
  author={Torrey, Lisa and Taylor, Matthew},
  booktitle={Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems},
  pages={1053--1060},
  year={2013}
}

@article{Miller:2019:xAISocialSciencesInsights,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  author={Miller, Tim},
  journal={Artificial intelligence},
  volume={267},
  pages={1--38},
  year={2019},
  publisher={Elsevier},
  doi={10.1016/j.artint.2018.07.007},
  abstract={There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.}
}

@inproceedings{Thomaz:2006:RLWithHumanTeachers,
  title={Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance},
  author={Thomaz, Andrea Lockerd and Breazeal, Cynthia and others},
  booktitle={Aaai},
  volume={6},
  pages={1000--1005},
  year={2006},
  organization={Boston, MA},
  abstract={While reinforcement learning (RL) is not traditionally designed for interactive supervisory input from a human teacher, several works in both robot and software agents have adapted it for human input by letting a human trainer control the reward signal. In this work, we experimentally examine the assumption underlying these works, namely that the human-given reward is compatible with the traditional RL reward signal. We describe an experimental platform with a simulated RL robot and present an analysis of real-time human teaching behavior found in a study in which untrained subjects taught the robot to perform a new task. We report three main observations on how people administer feedback when teaching a robot a task through reinforcement learning: (a) they use the reward channel not only for feedback, but also for future-directed guidance; (b) they have a positive bias to their feedback -possibly using the signal as a motivational channel; and (c) they change their behavior as they develop a mental model of the robotic learner. In conclusion, we discuss future extensions to RL to accommodate these lessons.}
}

@article{TomarEtAl:2021:LearnPixelControlRepresentations,
  title={Learning Representations for Pixel-based Control: What Matters and Why?},
  author={Tomar, Manan and Mishra, Utkarsh A and Zhang, Amy and Taylor, Matthew E},
  journal={arXiv preprint arXiv:2111.07775},
  year={2021},
  abstract={Learning representations for pixel-based control has garnered significant attention recently in reinforcement learning. A wide range of methods have been proposed to enable efficient learning, leading to sample complexities similar to those in the full state setting. However, moving beyond carefully curated pixel data sets (centered crop, appropriate lighting, clear background, etc.) remains challenging. In this paper, we adopt a more difficult setting, incorporating background distractors, as a first step towards addressing this challenge. We present a simple baseline approach that can learn meaningful representations with no metric-based learning, no data augmentations, no world-model learning, and no contrastive learning. We then analyze when and why previously proposed methods are likely to fail or reduce to the same performance as the baseline in this harder setting and why we should think carefully about extending such methods beyond the well curated environments. Our results show that finer categorization of benchmarks on the basis of characteristics like density of reward, planning horizon of the problem, presence of task-irrelevant components, etc., is crucial in evaluating algorithms. Based on these observations, we propose different metrics to consider when evaluating an algorithm on benchmark tasks. We hope such a data-centric view can motivate researchers to rethink representation learning when investigating how to best apply RL to real-world tasks. }
}

%%% VVV %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{vecerik2017leveraging,
  title={Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards},
  author={Vecerik, Mel and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1707.08817},
  year={2017},
  abstract={Leveraging human demonstrations along with an off-policy RL algorithm for continuous actions (DDPG) for robotics task like peg insertion, clip insertion, cable insertion etc. A separate replay buffer for demonstrations is maintained, prioritized replay is used to sample both agent and human demonstrations along with some other techniques}
}

@InProceedings{VermaEtAl:2018:ProgrammaticallyInterpretableRL,
  title = 	 {Programmatically Interpretable Reinforcement Learning},
  author =       {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5045--5054},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v80/verma18a.html},
  abstract = 	 {We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural “oracle”. We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.}
}


@article{Vu:2020:PGMExplainer,
  title={Pgm-explainer: Probabilistic graphical model explanations for graph neural networks},
  author={Vu, Minh N and Thai, My T},
  journal={arXiv preprint arXiv:2010.05788},
  year={2020},
  abstract={In Graph Neural Networks (GNNs), the graph structure is incorporated into the learning of node representations. This complex structure makes explaining GNNs' predictions become much more challenging. In this paper, we propose PGM-Explainer, a Probabilistic Graphical Model (PGM) model-agnostic explainer for GNNs. Given a prediction to be explained, PGM-Explainer identifies crucial graph components and generates an explanation in form of a PGM approximating that prediction. Different from existing explainers for GNNs where the explanations are drawn from a set of linear functions of explained features, PGM-Explainer is able to demonstrate the dependencies of explained features in form of conditional probabilities. Our theoretical analysis shows that the PGM generated by PGM-Explainer includes the Markov-blanket of the target prediction, i.e. including all its statistical information. We also show that the explanation returned by PGM-Explainer contains the same set of independence statements in the perfect map. Our experiments on both synthetic and real-world datasets show that PGM-Explainer achieves better performance than existing explainers in many benchmark tasks.}
}

%%% WWW %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{Wang:2022:SkillPreferences,
  title={Skill preferences: Learning to extract and execute robotic skills from human feedback},
  author={Wang, Xiaofei and Lee, Kimin and Hakhamaneshi, Kourosh and Abbeel, Pieter and Laskin, Michael},
  booktitle={Conference on Robot Learning},
  pages={1259--1268},
  year={2022},
  organization={PMLR},
  abstract={A promising approach to solving challenging long-horizon tasks has been to extract behavior priors (skills) by fitting generative models to large offline datasets of demonstrations. However, such generative models inherit the biases of the underlying data and result in poor and unusable skills when trained on imperfect demonstration data. To better align skill extraction with human intent we present Skill Preferences (SkiP), an algorithm that learns a model over human preferences and uses it to extract human-aligned skills from offline data. After extracting human-preferred skills, SkiP also utilizes human feedback to solve downstream tasks with RL. We show that SkiP enables a simulated kitchen robot to solve complex multi-step manipulation tasks and substantially outperforms prior leading RL algorithms with human preferences as well as leading skill extraction algorithms without human preferences.}
}

@article{Warnell:2018:DeepTAMER, 
title={Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces}, 
volume={32}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/11485}, 
abstractNote={While recent advances in deep reinforcement learning have allowed autonomous learning agents to succeed at a variety of complex tasks, existing algorithms generally require a lot of training data. One way to increase the speed at which agents are able to learn to perform tasks is by leveraging the input of human trainers. Although such input can take many forms, real-time, scalar-valued feedback is especially useful in situations where it proves difficult or impossible for humans to provide expert demonstrations. Previous approaches have shown the usefulness of human input provided in this fashion (e.g., the TAMER framework), but they have thus far not considered high-dimensional state spaces or employed the use of deep learning. In this paper, we do both: we propose DeepTAMER, an extension of the TAMER framework that leverages the representational power of deep neural networks in order to learn complex tasks in just a short amount of time with a human trainer. We demonstrate Deep TAMER’s success by using it and just 15 minutes of human-provided feedback to train an agent that performs better than humans on the Atari game of Bowling - a task that has proven difficult for even state-of-the-art reinforcement learning methods.}, 
number={1}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter}, year={2018}, 
month={Apr.} }


@article{wells:2021:explainable,
   year = {2021},
   author = {Wells, Lindsay and Bednarz, Tomasz},
   title = {Explainable ai and reinforcement learning—a systematic review of current approaches and trends},
   journal = {Frontiers in artificial intelligence},
   volume = {4},
   pages = {48},
   doi = {10.3389/frai.2021.550030}
}



@article{WellsBednarz:2021:xAIRLSurvey,
  title={Explainable ai and reinforcement learning—a systematic review of current approaches and trends},
  author={Wells, Lindsay and Bednarz, Tomasz},
  journal={Frontiers in artificial intelligence},
  volume={4},
  pages={48},
  year={2021},
  publisher={Frontiers},
  doi={10.3389/frai.2021.550030},
  abstract={Research into Explainable Artificial Intelligence (XAI) has been increasing in recent years as a response to the need for increased transparency and trust in AI. This is particularly important as AI is used in sensitive domains with societal, ethical, and safety implications. Work in XAI has primarily focused on Machine Learning (ML) for classification, decision, or action, with detailed systematic reviews already undertaken. This review looks to explore current approaches and limitations for XAI in the area of Reinforcement Learning (RL). From 520 search results, 25 studies (including 5 snowball sampled) are reviewed, highlighting visualization, query-based explanations, policy summarization, human-in-the-loop collaboration, and verification as trends in this area. Limitations in the studies are presented, particularly a lack of user studies, and the prevalence of toy-examples and difficulties providing understandable explanations. Areas for future study are identified, including immersive visualization, and symbolic representation.}
}

@article{wu:21,
  title={Human-in-the-loop deep reinforcement learning with application to autonomous driving},
  author={Wu, Jingda and Huang, Zhiyu and Huang, Chao and Hu, Zhongxu and Hang, Peng and Xing, Yang and Lv, Chen},
  journal={arXiv preprint arXiv:2104.07246},
  year={2021},
  abstract ={Due to the limited smartness and abilities of machine intelligence, currently autonomous vehicles are still unable to handle all kinds of situations and completely replace drivers. Because humans exhibit strong robustness and adaptability in complex driving scenarios, it is of great importance to introduce humans into the training loop of artificial intelligence, leveraging human intelligence to further advance machine learning algorithms. In this study, a real-time human-guidance-based deep reinforcement learning (Hug-DRL) method is developed for policy training of autonomous driving. Leveraging a newly designed control transfer mechanism between human and automation, human is able to intervene and correct the agent’s unreasonable actions in real time when necessary during the model training process. Based on this human-in-the-loop guidance mechanism, an improved actor-critic architecture with modified policy and value networks is developed. The fast convergence of the proposed Hug- DRL allows real-time human guidance actions to be fused into the agent’s training loop, further improving the efficiency and performance of deep reinforcement learning. The developed method is validated by human-in-the-loop experiments with 40 subjects and compared with other state-of-the-art learning approaches. The results suggest that the proposed method can effectively enhance the training efficiency and performance of the deep reinforcement learning algorithm under human guidance, without imposing specific requirements on participants’ expertise and experience.}
}

@article{WuEtAl:2021:HITLMLSurvey,
  title={A Survey of Human-in-the-loop for Machine Learning},
  author={Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang},
  journal={arXiv preprint arXiv:2108.00941},
  year={2021},
  abstract={Human-in-the-loop aims to train an accurate prediction model with minimum cost by integrating human knowledge and experience. Humans can provide training data for machine learning applications and directly accomplish tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent human-in-the-loop. Using the above categorization, we summarize major approaches in the field; along with their technical strengths/ weaknesses, we have simple classification and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for human-in-the-loop and motivates interested readers to consider approaches for designing effective human-in-the-loop solutions.}
}

@article{WuEtAl:2021:HITLDRLAutonomousDriving,
  title={Human-in-the-Loop Deep Reinforcement Learning with Application to Autonomous Driving},
  author={Wu, Jingda and Huang, Zhiyu and Huang, Chao and Hu, Zhongxu and Hang, Peng and Xing, Yang and Lv, Chen},
  journal={arXiv preprint arXiv:2104.07246},
  year={2021},
  abstract={Due to the limited smartness and abilities of machine intelligence, currently autonomous vehicles are still unable to handle all kinds of situations and completely replace drivers. Because humans exhibit strong robustness and adaptability in complex driving scenarios, it is of great importance to introduce humans into the training loop of artificial intelligence, leveraging human intelligence to further advance machine learning algorithms. In this study, a real-time human-guidance-based deep reinforcement learning (Hug-DRL) method is developed for policy training of autonomous driving. Leveraging a newly designed control transfer mechanism between human and automation, human is able to intervene and correct the agent's unreasonable actions in real time when necessary during the model training process. Based on this human-in-the-loop guidance mechanism, an improved actor-critic architecture with modified policy and value networks is developed. The fast convergence of the proposed Hug-DRL allows real-time human guidance actions to be fused into the agent's training loop, further improving the efficiency and performance of deep reinforcement learning. The developed method is validated by human-in-the-loop experiments with 40 subjects and compared with other state-of-the-art learning approaches. The results suggest that the proposed method can effectively enhance the training efficiency and performance of the deep reinforcement learning algorithm under human guidance, without imposing specific requirements on participant expertise and experience. }
}

%%% XXX %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{XinEtAl:2018:HITLMLFeedbackLoop,
  title={Accelerating human-in-the-loop machine learning: Challenges and opportunities},
  author={Xin, Doris and Ma, Litian and Liu, Jialin and Macke, Stephen and Song, Shuchen and Parameswaran, Aditya},
  booktitle={Proceedings of the second workshop on data management for end-to-end machine learning},
  pages={1--4},
  year={2018},
  abstract={Development of machine learning (ML) workflows is a tedious process of iterative experimentation: developers repeatedly make changes to workflows until the desired accuracy is attained. We describe our vision for a "human-in-the-loop" ML system that accelerates this process: by intelligently tracking changes and intermediate results over time, such a system can enable rapid iteration, quick responsive feedback, introspection and debugging, and background execution and automation. We finally describe Helix, our preliminary attempt at such a system that has already led to speedups of upto 10x on typical iterative workflows against competing systems.},
  doi={doi.org/10.1145/3209889.3209897}
}

@article{XiongEtAl:2020:Robustness,
  title={Robustness to adversarial attacks in learning-enabled controllers},
  author={Xiong, Zikang and Eappen, Joe and Zhu, He and Jagannathan, Suresh},
  journal={arXiv preprint arXiv:2006.06861},
  year={2020},
  abstract={Learning-enabled controllers used in cyber-physical systems (CPS) are known to be susceptible to adversarial attacks. Such attacks manifest as perturbations to the states generated by the controller's environment in response to its actions. We consider state perturbations that encompass a wide variety of adversarial attacks and describe an attack scheme for discovering adversarial states. To be useful, these attacks need to be natural, yielding states in which the controller can be reasonably expected to generate a meaningful response. We consider shield-based defenses as a means to improve controller robustness in the face of such perturbations. Our defense strategy allows us to treat the controller and environment as black-boxes with unknown dynamics. We provide a two-stage approach to construct this defense and show its effectiveness through a range of experiments on realistic continuous control domains such as the navigation control-loop of an F16 aircraft and the motion control system of humanoid robots. }
}


%%% YYY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Yang:2021:TrailNearOptimalSuboptimal,
  title={TRAIL: Near-Optimal Imitation Learning with Suboptimal Data},
  author={Yang, Mengjiao and Levine, Sergey and Nachum, Ofir},
  journal={arXiv preprint arXiv:2110.14770},
  year={2021},
  abstract={The aim in imitation learning is to learn effective policies by utilizing near-optimal expert demonstrations. However, high-quality demonstrations from human experts can be expensive to obtain in large numbers. On the other hand, it is often much easier to obtain large quantities of suboptimal or task-agnostic trajectories, which are not useful for direct imitation, but can nevertheless provide insight into the dynamical structure of the environment, showing what could be done in the environment even if not what should be done. We ask the question, is it possible to utilize such suboptimal offline datasets to facilitate provably improved downstream imitation learning? In this work, we answer this question affirmatively and present training objectives that use offline datasets to learn a factored transition model whose structure enables the extraction of a latent action space. Our theoretical analysis shows that the learned latent action space can boost the sample-efficiency of downstream imitation learning, effectively reducing the need for large near-optimal expert datasets through the use of auxiliary non-expert data. To learn the latent action space in practice, we propose TRAIL (Transition-Reparametrized Actions for Imitation Learning), an algorithm that learns an energy-based transition model contrastively, and uses the transition model to reparametrize the action space for sample-efficient imitation learning. We evaluate the practicality of our objective through experiments on a set of navigation and locomotion tasks. Our results verify the benefits suggested by our theory and show that TRAIL is able to improve baseline imitation learning by up to 4x in performance.}
}

@article{YangEtAl:2021:AutoCurriculumMARS,
  title={Diverse Auto-Curriculum is Critical for Successful Real-World Multiagent Learning Systems},
  author={Yang, Yaodong and Luo, Jun and Wen, Ying and Slumbers, Oliver and Graves, Daniel and Ammar, Haitham Bou and Wang, Jun and Taylor, Matthew E},
  journal={arXiv preprint arXiv:2102.07659},
  year={2021},
  abstract={Multiagent reinforcement learning (MARL) has achieved a remarkable amount of success in solving various types of video games. A cornerstone of this success is the auto-curriculum framework, which shapes the learning process by continually creating new challenging tasks for agents to adapt to, thereby facilitating the acquisition of new skills. In order to extend MARL methods to real-world domains outside of video games, we envision in this blue sky paper that maintaining a diversity-aware auto-curriculum is critical for successful MARL applications. Specifically, we argue that behavioural diversity is a pivotal, yet under-explored, component for real-world multiagent learning systems, and that significant work remains in understanding how to design a diversity-aware auto-curriculum. We list four open challenges for auto-curriculum techniques, which we believe deserve more attention from this community. Towards validating our vision, we recommend modelling realistic interactive behaviours in autonomous driving as an important test bed, and recommend the SMARTS/ULTRA benchmark. }
}

%%% ZZZ %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{ZagalJavierVallejos:2004:RealityGap,
title = {Back to reality: Crossing the reality gap in evolutionary robotics},
journal = {IFAC Proceedings Volumes},
volume = {37},
number = {8},
pages = {834-839},
year = {2004},
note = {IFAC/EURON Symposium on Intelligent Autonomous Vehicles, Lisbon, Portugal, 5-7 July 2004},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)32084-0},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017320840},
author = {Juan Cristóbal Zagal and Javier Ruiz-del-Solar and Paul Vallejos},
keywords = {Evolutionary Robotics, Autonomous Systems, On-line Learning},
abstract = {In this work a new method to evolutionary robotics is proposed, it combines into asingle framework, learning from reality and simulations. An illusory sub-system is incorporated as an integral part of an autonomous system. The adaptation of the illusory system results from minimizing differences of robot behavior evaluations in reality and in simulations. Behavior guides the illusory adaptation by sampling task-relevant instances of the world. Thus explicit calibration is not required. We remark two attributes of the presented methodology: (i) it is a promising approach for crossing the reality-gap among simulation and reality in evolutionary robotics, and (ii) it allows to generate automatically models and theories of the real robot environment expressed as simulations. We present validation experiments on locomotive behavior acquisition for legged robots.}
}

@article{zanzotto2019human,
  title={Human-in-the-loop artificial intelligence},
  author={Zanzotto, Fabio Massimo},
  journal={Journal of Artificial Intelligence Research},
  volume={64},
  pages={243--252},
  year={2019}
}

@article{zhang2020explainable,
  title={Explainable recommendation: A survey and new perspectives},
  author={Zhang, Yongfeng and Chen, Xu and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={14},
  number={1},
  pages={1--101},
  year={2020},
  publisher={Now Publishers, Inc.}
}

@incollection{Zhang:2020:AlphaZero,
  title={AlphaZero},
  author={Zhang, Hongming and Yu, Tianyang},
  booktitle={Deep Reinforcement Learning},
  pages={391--415},
  year={2020},
  publisher={Springer},
  abstract={In this chapter, we introduce combinatorial games such as chess and Go and take Gomoku as an example to introduce the AlphaZero algorithm, a general algorithm that has achieved superhuman performance in many challenging games. This chapter is divided into three parts: the first part introduces the concept of combinatorial games, the second part introduces the family of algorithms known as Monte Carlo Tree Search, and the third part takes Gomoku as the game environment to demonstrate the details of the AlphaZero algorithm, which combines Monte Carlo Tree Search and deep reinforcement learning from self-play.}
}

@inproceedings{Zhang:2020:human_out_loop,
  title={Can Humans Be out of the Loop?},
  author={Junzhe Zhang and Elias Bareinboim},
  booktitle={First Conference on Causal Learning and Reasoning (CLeaR 2022},
  year={2020},
  url={https://openreview.net/forum?id=P0f91v5fTK}
}
}

@inproceedings{zhang2019leveraging,
  title={Leveraging human guidance for deep reinforcement learning tasks},
  author={Zhang, Ruohan and Torabi, Faraz and Guan, Lin and Ballard, Dana H and Stone, Peter},
  booktitle={Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)},
  year={2019}
}

@article{Zhang:2020:CausalImitationLearning,
  title={Causal imitation learning with unobserved confounders},
  author={Zhang, Junzhe and Kumor, Daniel and Bareinboim, Elias},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12263--12274},
  year={2020},
  abstract={One of the common ways children learn is by mimicking adults. Imitation learning focuses on learning policies with suitable performance from demonstrations generated by an expert, with an unspecified performance measure, and unobserved reward signal. Popular methods for imitation learning start by either directly mimicking the behavior policy of an expert (behavior cloning) or by learning a reward function that prioritizes observed expert trajectories (inverse reinforcement learning). However, these methods rely on the assumption that covariates used by the expert to determine her/his actions are fully observed. In this paper, we relax this assumption and study imitation learning when sensory inputs of the learner and the expert differ. First, we provide a non-parametric, graphical criterion that is complete (both necessary and sufficient) for determining the feasibility of imitation from the combinations of demonstration data and qualitative assumptions about the underlying environment, represented in the form of a causal model. We then show that when such a criterion does not hold, imitation could still be feasible by exploiting quantitative knowledge of the expert trajectories. Finally, we develop an efficient procedure for learning the imitating policy from experts' trajectories.}
}

@inproceedings{Zhou:2019:CgcNet,
  title={Cgc-net: Cell graph convolutional network for grading of colorectal cancer histology images},
  author={Zhou, Yanning and Graham, Simon and Alemi Koohbanani, Navid and Shaban, Muhammad and Heng, Pheng-Ann and Rajpoot, Nasir},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},
  pages={0--0},
  year={2019},
  abstract={Colorectal cancer (CRC) grading is typically carried out by assessing the degree of gland formation within histology images. To do this, it is important to consider the overall tissue micro-environment by assessing the cell-level information along with the morphology of the gland. However, current automated methods for CRC grading typically utilise small image patches and therefore fail to incorporate the entire tissue micro-architecture for grading purposes. To overcome the challenges of CRC grading, we present a novel cell-graph convolutional neural network (CGC-Net) that converts each large histology image into a graph, where each node is represented by a nucleus within the original image and cellular interactions are denoted as edges between these nodes according to node similarity. The CGC-Net utilises nuclear appearance features in addition to the spatial location of nodes to further boost the performance of the algorithm. To enable nodes to fuse multi-scale information, we introduce Adaptive GraphSage, which is a graph convolution technique that combines multi-level features in a data-driven way. Furthermore, to deal with redundancy in the graph, we propose a sampling technique that removes nodes in areas of dense nuclear activity. We show that modeling the image as a graph enables us to effectively consider a much larger image (around 16x larger) than traditional patch-based approaches and model the complex structure of the tissue micro-environment. We construct cell graphs with an average of over 3,000 nodes on a large CRC histology image dataset and report state-of-the-art results as compared to recent patch-based as well as contextual patch-based techniques, demonstrating the effectiveness of our method.}
}

@article{ZhouEtAl:2021:QualitySurvey,
   year = {2021},
   author = {Zhou, Jianlong and Gandomi, Amir H. and Chen, Fang and Holzinger, Andreas},
   title = {Evaluating the Quality of Machine Learning Explanations: A Survey on Methods and Metrics},
   journal = {Electronics},
   volume = {10},
   number = {5},
   pages = {593},
   abstract = {The most successful Machine Learning (ML) systems remain complex black boxes to end-users, and even experts are often unable to understand the rationale behind their decisions. The lack of transparency of such systems can have severe consequences or poor uses of limited valuable resources in medical diagnosis, financial decision-making, and in other high-stake domains. Therefore, the issue of ML explanation has experienced a surge in interest from the research community to application domains. While numerous explanation methods have been explored, there is a need for evaluations to quantify the quality of explanation methods to determine whether and to what extent the offered explainability achieves the defined objective, and compare available explanation methods and suggest the best explanation from the comparison for a specific task. This survey paper presents a comprehensive overview of methods proposed in the current literature for the evaluation of ML explanations. We identify properties of explainability from the review of definitions of explainability. The identified properties of explainability are used as objectives that evaluation metrics should achieve. The survey found that the quantitative metrics for both model-based and example-based explanations are primarily used to evaluate the parsimony/simplicity of interpretability, while the quantitative metrics for attribution-based explanations are primarily used to evaluate the soundness of fidelity of explainability. The survey also demonstrated that subjective measures, such as trust and confidence, have been embraced as the focal point for the human-centered evaluation of explainable systems. The paper concludes that the evaluation of ML explanations is a multidisciplinary research topic. It is also not possible to define an implementation of evaluation metrics, which can be applied to all explanation methods. },
   doi = {10.3390/electronics10050593}
}

@inproceedings{ZiebartEtAl:2008:ImitationLearningNavigation,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K and others},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA},
  abstract={Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Prob- lems. This approach reduces learning to the problem of re- covering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behav- ior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling real- world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.}
}


@inproceedings{hasanbeig2021deepsynth,
  title={DeepSynth: Automata synthesis for automatic task segmentation in deep reinforcement learning},
  author={Hasanbeig, Mohammadhosein and Jeppu, Natasha Yogananda and Abate, Alessandro and Melham, Tom and Kroening, Daniel},
  booktitle={The Thirty-Fifth $\{$AAAI$\}$ Conference on Artificial Intelligence,$\{$AAAI$\}$},
  volume={2},
  pages={36},
  year={2021}
}

@article{martinez2017relational,
  title={Relational reinforcement learning with guided demonstrations},
  author={Mart{\'\i}nez, David and Alenya, Guillem and Torras, Carme},
  journal={Artificial Intelligence},
  volume={247},
  pages={295--312},
  year={2017},
  publisher={Elsevier}
}

@article{battaglia2018relational, title={Relational inductive biases, deep learning, and graph networks}, author={Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others}, journal={arXiv preprint arXiv:1806.01261}, year={2018} }

@inproceedings{andreas2017modular,
  title={Modular multitask reinforcement learning with policy sketches},
  author={Andreas, Jacob and Klein, Dan and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={166--175},
  year={2017},
  organization={PMLR}
}

@inproceedings{lyu2019sdrl,
  title={SDRL: interpretable and data-efficient deep reinforcement learning leveraging symbolic planning},
  author={Lyu, Daoming and Yang, Fangkai and Liu, Bo and Gustafson, Steven},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={2970--2977},
  year={2019}
}

@inproceedings{martinez2016learning,
  title={Learning relational dynamics of stochastic domains for planning},
  author={Mart{\'\i}nez, David and Alenya, Guillem and Torras, Carme and Ribeiro, Tony and Inoue, Katsumi},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={26},
  pages={235--243},
  year={2016}
}

@article{milani2022survey,
  title={A Survey of Explainable Reinforcement Learning},
  author={Milani, Stephanie and Topin, Nicholay and Veloso, Manuela and Fang, Fei},
  journal={arXiv preprint arXiv:2202.08434},
  year={2022}
}

@inproceedings{zhu2020object,
  title={Object-oriented dynamics learning through multi-level abstraction},
  author={Zhu, Guangxiang and Wang, Jianhao and Ren, Zhizhou and Lin, Zichuan and Zhang, Chongjie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={6989--6998},
  year={2020}
}

@article{toro2019learning,
  title={Learning reward machines for partially observable reinforcement learning},
  author={Toro Icarte, Rodrigo and Waldie, Ethan and Klassen, Toryn and Valenzano, Rick and Castro, Margarita and McIlraith, Sheila},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{icarte2018using,
  title={Using reward machines for high-level task specification and decomposition in reinforcement learning},
  author={Icarte, Rodrigo Toro and Klassen, Toryn and Valenzano, Richard and McIlraith, Sheila},
  booktitle={International Conference on Machine Learning},
  pages={2107--2116},
  year={2018},
  organization={PMLR}
}

@inproceedings{zhang2021kogun,
  title={KoGuN: accelerating deep reinforcement learning via integrating human suboptimal knowledge},
  author={Zhang, Peng and Hao, Jianye and Wang, Weixun and Tang, Hongyao and Ma, Yi and Duan, Yihai and Zheng, Yan},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={2291--2297},
  year={2021}
}

@inproceedings{akrour2019towards,
  title={Towards Reinforcement Learning of Human Readable Policies},
  author={Akrour, Riad and Tateo, Davide and Peters, Jan},
  booktitle={Workshop on Deep Continuous-Discrete Machine Learning},
  year={2019}
}
@article{hein2017particle,
  title={Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies},
  author={Hein, Daniel and Hentschel, Alexander and Runkler, Thomas and Udluft, Steffen},
  journal={Engineering Applications of Artificial Intelligence},
  volume={65},
  pages={87--98},
  year={2017},
  publisher={Elsevier}
}
@article{hein2018interpretable,
  title={Interpretable policies for reinforcement learning by genetic programming},
  author={Hein, Daniel and Udluft, Steffen and Runkler, Thomas A},
  journal={Engineering Applications of Artificial Intelligence},
  volume={76},
  pages={158--169},
  year={2018},
  publisher={Elsevier}
}
@inproceedings{hein2019generating,
  title={Generating interpretable reinforcement learning policies using genetic programming},
  author={Hein, Daniel and Udluft, Steffen and Runkler, Thomas A},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  pages={23--24},
  year={2019}
}
@article{likmeta2020combining,
  title={Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving},
  author={Likmeta, Amarildo and Metelli, Alberto Maria and Tirinzoni, Andrea and Giol, Riccardo and Restelli, Marcello and Romano, Danilo},
  journal={Robotics and Autonomous Systems},
  volume={131},
  pages={103568},
  year={2020},
  publisher={Elsevier}
}
@inproceedings{silva2020optimization,
  title={Optimization methods for interpretable differentiable decision trees applied to reinforcement learning},
  author={Silva, Andrew and Gombolay, Matthew and Killian, Taylor and Jimenez, Ivan and Son, Sung-Hyun},
  booktitle={International conference on artificial intelligence and statistics},
  pages={1855--1865},
  year={2020},
  organization={PMLR}
}
@inproceedings{topin2021iterative,
  title={Iterative Bounding MDPs: Learning Interpretable Policies via Non-Interpretable Methods},
  author={Topin, Nicholay and Milani, Stephanie and Fang, Fei and Veloso, Manuela},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={9923--9931},
  year={2021}
}

@inproceedings{jiang2019neural,
  title={Neural logic reinforcement learning},
  author={Jiang, Zhengyao and Luo, Shan},
  booktitle={International Conference on Machine Learning},
  pages={3110--3119},
  year={2019},
  organization={PMLR}
}

@article{verma2019imitation,
  title={Imitation-projected programmatic reinforcement learning},
  author={Verma, Abhinav and Le, Hoang and Yue, Yisong and Chaudhuri, Swarat},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{sun2019program,
  title={Program guided agent},
  author={Sun, Shao-Hua and Wu, Te-Lin and Lim, Joseph J},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


@article{bastani2018verifiable,
  title={Verifiable reinforcement learning via policy extraction},
  author={Bastani, Osbert and Pu, Yewen and Solar-Lezama, Armando},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{DBLP:journals/corr/abs-1912-12191,
  author    = {Piyush Gupta and
               Nikaash Puri and
               Sukriti Verma and
               Dhruv Kayastha and
               Shripad Deshmukh and
               Balaji Krishnamurthy and
               Sameer Singh},
  title     = {Explain Your Move: Understanding Agent Actions Using Focused Feature
               Saliency},
  journal   = {CoRR},
  volume    = {abs/1912.12191},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.12191},
  eprinttype = {arXiv},
  eprint    = {1912.12191},
  timestamp = {Mon, 24 Feb 2020 12:40:24 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-12191.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-1912-05743,
  author    = {Akanksha Atrey and
               Kaleigh Clary and
               David D. Jensen},
  title     = {Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps
               for Deep {RL}},
  journal   = {CoRR},
  volume    = {abs/1912.05743},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.05743},
  eprinttype = {arXiv},
  eprint    = {1912.05743},
  timestamp = {Thu, 02 Jan 2020 18:08:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-05743.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{fukuchi2017autonomous,
  title={Autonomous self-explanation of behavior for interactive reinforcement learning agents},
  author={Fukuchi, Yosuke and Osawa, Masahiko and Yamakawa, Hiroshi and Imai, Michita},
  booktitle={Proceedings of the 5th International Conference on Human Agent Interaction},
  pages={97--101},
  year={2017}
}

@inproceedings{fukuchi2017application,
  title={Application of instruction-based behavior explanation to a reinforcement learning agent with changing policy},
  author={Fukuchi, Yosuke and Osawa, Masahiko and Yamakawa, Hiroshi and Imai, Michita},
  booktitle={International Conference on Neural Information Processing},
  pages={100--108},
  year={2017},
  organization={Springer}
}