% Sort all references alphabetically, so you can easily search and organize
% Please keep all references complete and include the doi where available (arxiv do not have it)
% Please check if the copied reference is correct! 
% 04.03.2022

%%% AAA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{Abbeel:2004:InverseRL,
  title={Apprenticeship Learning via Inverse Reinforcement Learning},
  author={Abbeel, Pieter and Ng, Andrew Y.},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  publisher = {Association for Computing Machinery},
  pages={1},
  year={2004},
  abstract={We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
  doi={10.1145/1015330.1015430}
}

@article{Abel:2017:AgentAgnosticHumanInTheLoopRL,
  title={Agent-Agnostic Human-in-the-Loop Reinforcement Learning},
  author={Abel, David and Salvatier, John and Stuhlm{\"u}ller, Andreas and Evans, Owain},
  journal={arXiv preprint arXiv:1701.04079},
  year={2017},
  abstract={Providing Reinforcement Learning agents with expert advice can dramatically improve various aspects of learning. Prior work has developed teaching protocols that enable agents to learn efficiently in complex environments; many of these methods tailor the teacher's guidance to agents with a particular representation or underlying learning scheme, offering effective but specialized teaching procedures. In this work, we explore protocol programs, an agent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is to incorporate the beneficial properties of a human teacher into Reinforcement Learning without making strong assumptions about the inner workings of the agent. We show how to represent existing approaches such as action pruning, reward shaping, and training in simulation as special cases of our schema and conduct preliminary experiments on simple domains.}
}

@article{akalin:21,
  title={Reinforcement Learning Approaches in Social Robotics},
  author={Akalin, Neziha and Loutfi, Amy},
  journal={Sensors},
  volume={21},
  number={4},
  pages={1292},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute},
  abstract={This survey analyzes RL applied to social robotics. A relevant outcome is the identification of challenges in interactive learning observed in this specific domain. Human-centered explainability can learn from social robotics and help reduce some of those challenges. For instance, human teachers tend to reduce the feedback frequency as the training progresses, resulting in a diminished cumulative reward. Reducing the feedback frequency could be caused by fatigue, boredom, or the belief that the robot "remembers" and "understands" all previous feedback, requiring less advice later. Transparency issues may also arise during the training of a physical robot via human reward, causing the teacher to give incorrect feedback. Moreover, ambiguous robot behavior might affect the willingness of a human to interact again. Inexperienced users usually take more time training. Transparency or explainability could reduce confusion and help guide human trainers. Further, human trainers tend to give more positive feedback, and the learning agent should be aware of this bias. Making the agent's assumptions transparent to the trainer can improve the process.},
  doi = {10.3390/s21041292}
}

% TODO: please add doi if possible
@inproceedings{akrour2019towards,
  title={Towards Reinforcement Learning of Human Readable Policies},
  author={Akrour, Riad and Tateo, Davide and Peters, Jan},
  booktitle={Workshop on Deep Continuous-Discrete Machine Learning},
  year={2019}
}

% no doi found
@article{Alber:2019:Innvestigate,
  title={iNNvestigate Neural Networks!},
  author={Alber, Maximilian and Lapuschkin, Sebastian and Seegerer, Philipp and H{\"a}gele, Miriam and Sch{\"u}tt, Kristof T and Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert and D{\"a}hne, Sven and Kindermans, Pieter-Jan},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={93},
  pages={1--8},
  year={2019},
  abstract={In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and predictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this shortcoming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-the-box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.}
}

@article{alharin2020reinforcement,
  title={Reinforcement Learning Interpretation Methods: A Survey},
  author={Alharin, Alnour and Doan, Thanh-Nam and Sartipi, Mina},
  journal={IEEE Access},
  volume={8},
  pages={171058--171077},
  year={2020},
  publisher={IEEE},
  doi={10.1109/ACCESS.2020.3023394}
}

% no doi found
@inproceedings{AlonsoEtAl:2018:xAINLBeerClassifier,
  title={Explainable AI Beer Style Classifier.},
  author={Alonso, Jos{\'e} Maria and Ramos-Soto, Alejandro and Castiello, Ciro and Mencar, Corrado},
  url={https://ceur-ws.org/Vol-2151/Paper_S1.pdf},
  year={2018},
  booktitle={SICSA ReaLX},
  abstract={This paper describes how to build an eXplainable Artificial Intelligence (XAI) classifier for a real use case related to beer style classification. It combines an opaque machine learning algorithm (Random Forest) with an interpretable machine learning algorithm (Decision Tree). The result is a XAI classifier which provides users with a good interpretability-accuracy trade-off but also with explanation capabilities. First, the opaque algorithm acts as an “oracle” which finds out the most plausible output. Then, we generate a textual explanation of the given output which emerges as an automatic interpretation of the inference process carried out by the related decision tree, if the outputs from both classifiers coincide. We apply a Natural Language Generation Approach to generate the textual explanations}
}

% no doi found
@inproceedings{amir2016interactive,
  author = {Amir, Ofra and Kamar, Ece and Kolobov, Andrey and Grosz, Barbara},
  title = {Interactive Teaching Strategies for Agent Training},
  booktitle = {In Proceedings of IJCAI 2016},
  year = {2016},
  month = {May},
  abstract = {Agents learning how to act in new environments can benefit from input from more experienced agents or humans. This paper studies interactive teaching strategies for identifying when a student can benefit from teacher-advice in a reinforcement learning framework. In student-teacher learning, a teacher agent can advise the student on which action to take. Prior work has considered heuristics for the teacher to choose advising opportunities. While these approaches effectively accelerate agent training, they assume that the teacher constantly monitors the student. This assumption may not be satisfied with human teachers, as people incur cognitive costs of monitoring and might not always pay attention. We propose strategies for a teacher and a student to jointly identify advising opportunities so that the teacher is not required to constantly monitor the student. Experimental results show that these approaches reduce the amount of attention required from the teacher compared to teacher-initiated strategies, while maintaining similar learning gains. The empirical evaluation also investigates the effect of the information communicated to the teacher and the quality of the student’s initial policy on teaching outcomes.}
}

@inproceedings{AndersonBischof:2013:PerformanceGestureGuides,
  author = {Anderson, Fraser and Bischof, Walter F.},
  title = {Learning and Performance with Gesture Guides},
  year = {2013},
  isbn = {9781450318990},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2470654.2466143},
  doi = {10.1145/2470654.2466143},
  abstract = {Gesture-based interfaces are becoming more prevalent and complex, requiring non-trivial learning of gesture sets. Many methods for learning gestures have been proposed, but they are often evaluated with short-term recall tests that measure user performance, rather than learning. We evaluated four types of gesture guides using a retention and transfer paradigm common in motor learning experiments and found results different from those typically reported with recall tests. The results indicate that many guide systems with higher levels of guidance exhibit high performance benefits while the guide is being used, but are ultimately detrimental to user learning. We propose an adaptive guide that does not suffer from these drawbacks, and that enables a smooth transition from novice to expert. The results contrasting learning and performance can be explained by the guidance hypothesis. They have important implications for the design and evaluation of future gesture learning systems.},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages = {1109–-1118},
  numpages = {10},
  keywords = {learning, guides, evaluation, gestures},
  location = {Paris, France},
  series = {CHI '13}
}

@article{anderson:20,
  title={Mental Models of Mere Mortals with Explanations of Reinforcement Learning},
  author={Anderson, Andrew and Dodge, Jonathan and Sadarangani, Amrita and Juozapaitis, Zoe and Newman, Evan and Irvine, Jed and Chattopadhyay, Souti and Olson, Matthew and Fern, Alan and Burnett, Margaret},
  journal={ACM Transactions on Interactive Intelligent Systems (TiiS)},
  volume={10},
  number={2},
  pages={1--37},
  year={2020},
  publisher={ACM New York, NY, USA}, 
  abstract={How should reinforcement learning (RL) agents explain themselves to humans not trained in AI? To gain insights into this question, we conducted a 124-participant, four-treatment experiment to compare participants’ mental models of an RL agent in the context of a simple Real-Time Strategy (RTS) game. The four treatments isolated two types of explanations vs. neither vs. both together. The two types of explanations were as follows: (1) saliency maps (an “Input Intelligibility Type” that explains the AI’s focus of attention) and (2) reward-decomposition bars (an “Output Intelligibility Type” that explains the AI’s predictions of future types of rewards). Our results show that a combined explanation that included saliency and reward bars was needed to achieve a statistically significant difference in participants’ mental model scores over the no-explanation treatment. However, this combined explanation was far from a panacea: It exacted dis- proportionately high cognitive loads from the participants who received the combined explanation. Further, in some situations, participants who saw both explanations predicted the agent’s next action worse than all other treatments’ participants.},
  doi = {10.1145/3366485}
}

% no doi found
@inproceedings{andreas2017modular,
  title={Modular Multitask Reinforcement Learning with Policy Sketches},
  author={Andreas, Jacob and Klein, Dan and Levine, Sergey},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages={166--175},
  year={2017},
  editor = {Precup, Doina and Teh, Yee Whye},
  volume = {70},
  series = {Proceedings of Machine Learning Research},
  organization={PMLR},
  abstract = {We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them—specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor–critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.}
}

@article{Andrychowicz:2017:HERHindsightExperienceReplay,
  title={Hindsight Experience Replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1707.01495},
  year={2017},
  abstract={Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.}
}

@incollection{araiza2019safe,
   year = {2019},
   author = {Araiza-Illan, Dejanira and Eder, Kerstin},
   title = {Safe and Trustworthy Human-Robot Interaction},
   booktitle = {Humanoid Robotics: A Reference},
   editor = {Goswami, Ambarish and Vadakkepat, Prahlad},
   publisher = {Springer},
   address = {Dortrecht},
   pages = {2397--2419},
   abstract = {To be genuinely useful, robotic assistants must be both smart and powerful. This makes them potentially dangerous, and we must consider safety and trustworthiness as primary design goals for human-assistive robots. This chapter is focused on techniques that can be used to gain confidence in the safety of code used to control robots that directly interact with humans. We include formal methods and simulation-based testing techniques as well as experimental evaluation to determine how much users actually trust robots when interacting with them in a practical setting. The complexity of verifying and validating the behavior of robots in human-robot interactions requires combining different techniques, and we discuss the benefits of doing so. As robots are being equipped with increasingly sophisticated reasoning capabilities to operate fully autonomously in open environments, it becomes more and more important that verification methods are being developed to match that level of artificial intelligence.},
   doi = {10.1007/978-94-007-6046-2_131}
}

@article{arakawa:18,
  title={DQN-Tamer: Human-in-the-Loop Reinforcement Learning with Intractable Feedback},
  author={Arakawa, Riku and Kobayashi, Sosuke and Unno, Yuya and Tsuboi, Yuta and Maeda, Shin-ichi},
  journal={arXiv preprint arXiv:1810.11748},
  year={2018},
  abstract={Exploration has been one of the greatest chal- lenges in reinforcement learning (RL), which is a large obstacle in the application of RL to robotics. Even with state-of-the-art RL algorithms, building a well-learned agent often requires too many trials, mainly due to the difficulty of matching its actions with rewards in the distant future. A remedy for this is to train an agent with real-time feedback from a human observer who immediately gives rewards for some actions. This study tackles a series of challenges for introducing such a human- in-the-loop RL scheme. The first contribution of this work is our experiments with a precisely modeled human observer: BINARY, DELAY, STOCHASTICITY, UNSUSTAINABILITY, and NATURAL REACTION. We also propose an RL method called DQN-TAMER, which efficiently uses both human feedback and distant rewards. We find that DQN-TAMER agents outperform their baselines in Maze and Taxi simulated environments. Furthermore, we demonstrate a real-world human-in-the-loop RL application where a camera automatically recognizes a user’s facial expressions as feedback to the agent while the agent explores a maze.}
}

@article{Arras:2017:ExplainingRNNsPerturbationAnalysis,
  title={Explaining Recurrent Neural Network Predictions in Sentiment Analysis},
  author={Arras, Leila and Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={arXiv preprint arXiv:1706.07206},
  year={2017},
  abstract={Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.}
}

@inproceedings{Arzate:2020:SurveyInteractiveRL,
  title={A Survey on Interactive Reinforcement Learning: Design Principles and Open Challenges},
  author={Arzate Cruz, Christian and Igarashi, Takeo},
  booktitle={Proceedings of the 2020 ACM Designing Interactive Systems Conference},
  publisher = {Association for Computing Machinery},
  pages={1195--1209},
  year={2020},
  abstract={Interactive reinforcement learning (RL) has been successfully used in various applications in different fields, which has also motivated HCI researchers to contribute in this area. In this paper, we survey interactive RL to empower human-computer interaction (HCI) researchers with the technical background in RL needed to design new interaction techniques and propose new applications. We elucidate the roles played by HCI researchers in interactive RL, identifying ideas and promising research directions. Furthermore, we propose generic design principles that will provide researchers with a guide to effectively implement interactive RL applications.},
  doi={10.1145/3357236.3395525}
}
% no doi found
@article{AudibertMunosSzepesv:2009:ExplorationExploitation,
  title={Exploration--exploitation tradeoff using variance estimates in multi-armed bandits},
  author={Audibert, Jean-Yves and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Theoretical Computer Science},
  volume={410},
  number={19},
  pages={1876--1902},
  year={2009},
  publisher={Elsevier},
  doi={10.1016/j.tcs.2009.01.016},
  abstract={Algorithms based on upper confidence bounds for balancing exploration and exploitation are gaining popularity since they are easy to implement, efficient and effective. This paper considers a variant of the basic algorithm for the stochastic, multi-armed bandit problem that takes into account the empirical variance of the different arms. In earlier experimental works, such algorithms were found to outperform the competing algorithms. We provide the first analysis of the expected regret for such algorithms. As expected, our results show that the algorithm that uses the variance estimates has a major advantage over its alternatives that do not use such estimates provided that the variances of the payoffs of the suboptimal arms are low. We also prove that the regret concentrates only at a polynomial rate. This holds for all the upper confidence bound based algorithms and for all bandit problems except those special ones where with probability one the payoff obtained by pulling the optimal arm is larger than the expected payoff for the second best arm. Hence, although upper confidence bound bandit algorithms achieve logarithmic expected regret rates, they might not be suitable for a risk-averse decision maker. We illustrate some of the results by computer simulations.}
}

@article{DBLP:journals/corr/abs-1912-05743,
  author    = {Akanksha Atrey and
               Kaleigh Clary and
               David D. Jensen},
  title     = {Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps
               for Deep Reinforcement Learning},
  journal   = {arXiv preprint arXiv:1912.05743},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.05743}
}

@article{AzarLazaricBrunskill:2013:LifelongLearning,
  title={Sequential Transfer in Multi-armed Bandit with Finite Set of Models},
  author={Azar, Mohammad Gheshlaghi and Lazaric, Alessandro and Brunskill, Emma},
  journal={arXiv preprint arXiv:1307.6887},
  year={2013},
  abstract={Learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents. Although results in supervised and reinforcement learning show that transfer may significantly improve the learning performance, most of the literature on transfer is focused on batch learning tasks. In this paper we study the problem of sequential transfer in online learning, notably in the multi-armed bandit framework, where the objective is to minimize the total regret over a sequence of tasks by transferring knowledge from prior tasks. We introduce a novel bandit algorithm based on a method-of-moments approach for estimating the possible tasks and derive regret bounds for it.}
}

%%% BBB %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Bach:2015:LayerWiseRelevancePropagation,
  title={On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise relevance propagation},
  author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={PloS one},
  volume={10},
  number={7},
  pages={e0130140},
  year={2015},
  publisher={Public Library of Science San Francisco, CA USA},
  abstract={Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  doi={10.1371/journal.pone.0130140}
}

@inproceedings{bastani2018verifiable,
  title={Verifiable Reinforcement Learning via Policy Extraction},
  author={Bastani, Osbert and Pu, Yewen and Solar-Lezama, Armando},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  volume={31},
  publisher = {Curran Associates, Inc.},
  year={2018}
}



@inproceedings{Bapst:2019:AgentsPhysicalGNN,
  title={Structured agents for physical construction},
  author={Bapst, Victor and Sanchez-Gonzalez, Alvaro and Doersch, Carl and Stachenfeld, Kimberly and Kohli, Pushmeet and Battaglia, Peter and Hamrick, Jessica},
  booktitle={International Conference on Machine Learning},
  pages={464--474},
  year={2019},
  organization={PMLR},
  abstract={Physical construction—the ability to compose objects, subject to physical dynamics, to serve some function—is fundamental to human intelligence. We introduce a suite of challenging physical construction tasks inspired by how children play with blocks, such as matching a target configuration, stacking blocks to connect objects together, and creating shelter-like structures over target objects. We examine how a range of deep reinforcement learning agents fare on these challenges, and introduce several new approaches which provide superior performance. Our results show that agents which use structured representations (e.g., objects and scene graphs) and structured policies (e.g., object-centric actions) outperform those which use less structured representations, and generalize better beyond their training when asked to reason about larger scenes. Model-based agents which use Monte-Carlo Tree Search also outperform strictly model-free agents in our most challenging construction problems. We conclude that approaches which combine structured representations and reasoning with powerful learning are a key path toward agents that possess rich intuitive physics, scene understanding, and planning.}
}

@article{battaglia2018relational, 
  title={Relational inductive biases, deep learning, and graph networks}, author={Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan}, 
  journal={arXiv preprint arXiv:1806.01261}, 
  year={2018} 
}

@article{bellemare2020autonomous,
  title={Autonomous navigation of stratospheric balloons using reinforcement learning},
  author={Bellemare, Marc G and Candido, Salvatore and Castro, Pablo Samuel and Gong, Jun and Machado, Marlos C and Moitra, Subhodeep and Ponda, Sameera S and Wang, Ziyu},
  journal={Nature},
  volume={588},
  number={7836},
  pages={77--82},
  year={2020},
  publisher={Nature Publishing Group},
  doi={10.1038/s41586-020-2939-8}
}

@article{Ben-YounesEtAl:2022:DrivingBehaviorEx,
  title={Driving behavior explanation with multi-level fusion},
  author={Ben-Younes, H{\'e}di and Zablocki, {\'E}loi and P{\'e}rez, Patrick and Cord, Matthieu},
  journal={Pattern Recognition},
  volume={123},
  pages={108421},
  year={2022},
  publisher={Elsevier},
  abstract = {In this era of active development of autonomous vehicles, it becomes crucial to provide driving systems with the capacity to explain their decisions. In this work, we focus on generating high-level driving explanations as the vehicle drives. We present BEEF, for BEhavior Explanation with Fusion, a deep architecture which explains the behavior of a trajectory prediction model. Supervised by annotations of human driving decisions justifications, BEEF learns to fuse features from multiple levels. Leveraging recent advances in the multi-modal fusion literature, BEEF is carefully designed to model the correlations between high-level decisions features and mid-level perceptual features. The flexibility and efficiency of our approach are validated with extensive experiments on the HDD and BDD-X datasets.},
  doi = {10.1016/j.patcog.2021.108421}
}

@article{biyik:21,
  title={Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences},
  author={B{\i}y{\i}k, Erdem and Losey, Dylan P and Palan, Malayandi and Landolfi, Nicholas C and Shevchuk, Gleb and Sadigh, Dorsa},
  journal={The International Journal of Robotics Research},
  pages={45--67},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England},
  abstract={The paper proposes an algorithm that uses demonstrations (passive information) to initialize a belief of the reward function and actively queries the user's preferences to learn the actual reward function efficiently. The algorithm decides when to use each type of information and when to stop querying the teacher. More importantly, it accounts for the human's skill to provide data by maximizing the utility of questions while minimizing the human's uncertainty over their answer. Using gain information is more data-efficient than the previous approach for generating queries.},
  doi={10.1177/02783649211041652}
}

% not found in ACM! (organization={ACM})
% TODO: please check if this is the correct version!
@article{bruneau2002eyes,
  title={The Eyes Never Lie: The Use of Eye Tracking Data in HCI Research},
  author={Bruneau, Daniel and Sasse, M. Angela and McCarthy, John},
  year={2002},
  publisher={University College London}
}


%%% CCC %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{Caltagarione:2017:DrivingPathGeneration,
  title={LIDAR-based Driving Path Generation Using Fully Convolutional Neural Networks},
  author={Caltagirone, Luca and Bellone, Mauro and Svensson, Lennart and Wahde, Mattias},
  booktitle={2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)},
  pages={1--6},
  year={2017},
  organization={IEEE},
  abstract={In this work, a novel learning-based approach has been developed to generate driving paths by integrating LIDAR point clouds, GPS-IMU information, and Google driving directions. The system is based on a fully convolutional neural network that jointly learns to carry out perception and path generation from real-world driving sequences and that is trained using automatically generated training examples. Several combinations of input data were tested in order to assess the performance gain provided by specific information modalities. The fully convolutional neural network trained using all the available sensors together with driving directions achieved the best MaxF score of 88.13\% when considering a region of interest of 60 × 60 meters. By considering a smaller region of interest, the agreement between predicted paths and ground-truth increased to 92.60\%. The positive results obtained in this work indicate that the proposed system may help fill the gap between low-level scene parsing and behavior-reflex approaches by generating outputs that are close to vehicle control and at the same time human-interpretable.},
  doi={10.1109/ITSC.2017.8317618}
}

@article{ChakrabortyEtAl:2021:SurveyAdversarialAttacks,
  author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
  title = {A survey on adversarial attacks and defences},
  year = {2021},
  journal = {CAAI Transactions on Intelligence Technology},
  volume = {6},
  number = {1},
  pages = {25--45},
  url = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/cit2.12028},
  eprint = {https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/cit2.12028},
  abstract = {Abstract Deep learning has evolved as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. The advancement of deep learning has been so radical that today it can surpass human-level performance. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, efficient deep learning systems can be jeopardised by using crafted adversarial samples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. Herein, the authors attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate on the efficiency and challenges of recent countermeasures against them.},
  doi = {10.1049/cit2.12028}
}

@inproceedings{cederborg2015policy,
  title={Policy Shaping with Human Teachers},
  author={Cederborg, Thomas and Grover, Ishaan and Isbell, Charles L and Thomaz, Andrea L},
  booktitle={Twenty-Fourth International Joint Conference on Artificial Intelligence},
  year={2015},
  abstract={In this work we evaluate the performance of a policy shaping algorithm using 26 human teachers. We examine if the algorithm is suitable for human generated data on two different boards in a pac-man domain, comparing performance to an oracle that provides critique based on one known winning policy. Perhaps surprisingly, we show that the data generated by our 26 participants yields even better performance for the agent than data generated by the oracle. This might be because humans do not discourage exploring multiple winning policies. Additionally, we evaluate the impact of different verbal instructions, and different interpretations of silence, finding that the usefulness of data is affected both by what instructions is given to teachers, and how the data is interpreted.},
  pages = {3366--3372}
}

@article{Christiano:2017:DeepRLHumanPreferences,
  title={Deep Reinforcement Learning from Human Preferences},
  author={Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={arXiv preprint arXiv:1706.03741},
  year={2017},
  abstract={For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.}
}

@article{Collins:2019:Cognition,
  title = {Reinforcement learning: bringing together computation and cognition},
  author = {Anne Gabrielle Eva Collins},
  year = {2019},
  journal = {Current Opinion in Behavioral Sciences},
  volume = {29},
  pages = {63--68},
  abstract = {A key aspect of human intelligence is our ability to learn very quickly. This ability is still lacking in artificial intelligence. This article will highlight recent research showing how bringing together the fields of artificial intelligence and cognitive science may benefit both. Ideas from artificial intelligence have provided helpful formal theories to account for aspects of human learning. In return, ideas from cognitive science and neuroscience can also inform artificial intelligence research with directions to make algorithms more human-like. For example, recent work shows that human learning can only be understood in the context of multiple separate, interacting memory systems, rather than as a single, complex learner. This insight is starting to show promise in improving artificial agents’ learning efficiency.},
  doi = {10.1016/j.cobeha.2019.04.011}
}

% There was no journal/ source
% TODO: please check if this is the correct article!
@article{CuiEtAl:2020:EMPATHICFrameworkHumanFeedback,
  title = {The EMPATHIC Framework for Task Learning from Implicit Human Feedback},
  author = {Cui, Yuchen and Zhang, Qiping and Allievi, Alessandro and Stone, Peter and Niekum, Scott and Knox, W. Bradley},
  journal={arXiv preprint arXiv:2009.13649},
  year = {2020},
  publisher = {arXiv},
  month = {09},  
  pages = {}
} 

%%% DDD %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{da2020uncertainty,
  title={Uncertainty-Aware Action Advising for Deep Reinforcement Learning Agents},
  author={Da Silva, Felipe Leno and Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={5792--5799},
  year={2020},
  doi={10.1609/aaai.v34i04.6036}
}
@article{damiano2018anthropomorphism,
  title={Anthropomorphism in human--robot co-evolution},
  author={Damiano, Luisa and Dumouchel, Paul},
  journal={Frontiers in psychology},
  volume={9},
  pages={468},
  year={2018},
  publisher={Frontiers},
  doi={10.3389/fpsyg.2018.00468}
}

@article{daniel2016hierarchical,
  title={Hierarchical Relative Entropy Policy Search},
  author={Daniel, Christian and Neumann, Gerhard and Kroemer, Oliver and Peters, Jan},
  journal={Journal of Machine Learning Research},
  volume={17},
  pages={1--50},
  year={2016},
  publisher={Massachusetts Institute of Technology Press (MIT Press)/Microtome Publishing}
}

@article{dazeley2021explainable,
  title={Explainable Reinforcement Learning for Broad-XAI: A Conceptual Framework and Survey},
  author={Dazeley, Richard and Vamplew, Peter and Cruz, Francisco},
  journal={arXiv preprint arXiv:2108.09003},
  year={2021},
  abstract = {Broad Explainable Artificial Intelligence moves away from interpreting individual decisions based on a single datum and aims to provide integrated explanations from multiple machine learning algorithms into a coherent explanation of an agent's behaviour that is aligned to the communication needs of the explainee. Reinforcement Learning (RL) methods, we propose, provide a potential backbone for the cognitive model required for the development of Broad-XAI. RL represents a suite of approaches that have had increasing success in solving a range of sequential decision-making problems. However, these algorithms all operate as black-box problem solvers, where they obfuscate their decision-making policy through a complex array of values and functions. EXplainable RL (XRL) is relatively recent field of research that aims to develop techniques to extract concepts from the agent's: perception of the environment; intrinsic/extrinsic motivations/beliefs; Q-values, goals and objectives. This paper aims to introduce a conceptual framework, called the Causal XRL Framework (CXF), that unifies the current XRL research and uses RL as a backbone to the development of Broad-XAI. Additionally, we recognise that RL methods have the ability to incorporate a range of technologies to allow agents to adapt to their environment. CXF is designed for the incorporation of many standard RL extensions and integrated with external ontologies and communication facilities so that the agent can answer questions that explain outcomes and justify its decisions.}
}

@inproceedings{de:17,
  title={How People Explain Action (and Autonomous Intelligent Systems Should Too)},
  author={De Graaf, Maartje MA and Malle, Bertram F},
  booktitle={2017 AAAI Fall Symposium Series},
  year={2017},
  abstract ={To make Autonomous Intelligent Systems (AIS), such as virtual agents and embodied robots, “explainable” we need to understand how people respond to such systems and what expectations they have of them. Our thesis is that people will regard most AIS as intentional agents and apply the conceptual framework and psychological mechanisms of human behavior explanation to them. We present a well supported theory of how people explain human behavior and sketch what it would take to implement the underlying framework of explanation in AIS. The benefits will be considerable: When an AIS is able to explain its behavior in ways that people find comprehensible, people are more likely to form correct mental models of such a system and calibrate their trust in the system.}
}

@article{DeSaintsEtAl:2008:phri,
  title = {An atlas of physical human–robot interaction},
  author = {{De Santis}, Agostino and Siciliano, Bruno and {De Luca}, Alessandro and Bicchi, Antonio},
  year = {2008},
  journal = {Mechanism and Machine Theory},
  volume = {43},
  number = {3},
  pages = {253--270},
  abstract = {A broad spectrum of issues have to be addressed in order to tackle the problem of a safe and dependable physical Human–Robot Interaction (pHRI). In the immediate future, metrics related to safety and dependability have to be found in order to successfully introduce robots in everyday enviornments. While there are certainly also “cognitive” issues involved, due to the human perception of the robot (and vice versa), and other objective metrics related to fault detection and isolation, our discussion focuses on the peculiar aspects of “physical” interaction with robots. In particular, safety and dependability are the underlying evaluation criteria for mechanical design, actuation, and control architectures. Mechanical and control issues are discussed with emphasis on techniques that provide safety in an intrinsic way or by means of control components. Attention is devoted to dependability, mainly related to sensors, control architectures, and fault handling and tolerance. Suggestions are provided to draft metrics for evaluating safety and dependability in pHRI, and references to the works of the scientific groups involved in the pHRI research complete the study. The present atlas is a result of the EURON perspective research project “Physical Human–Robot Interaction in anthropic DOMains (PHRIDOM)”, aimed at charting the new territory of pHRI, and constitutes the scientific basis for the ongoing STReP project “Physical Human–Robot Interaction: depENDability and Safety (PHRIENDS)”, aimed at developing key components for the next generation of robots, designed to share their environment with people.},
  doi = {10.1016/j.mechmachtheory.2007.03.003}
}

@article{DoroudiThomasBrunskill:2017:ImportanceSampling,
  title={Importance Sampling for Fair Policy Selection},
  author={Doroudi, Shayan and Thomas, Philip S and Brunskill, Emma},
  journal={Grantee Submission},
  year={2017},
  publisher={ERIC},
  abstract={We consider the problem of off-policy policy selection in reinforcement learning: using historical data generated from running one policy to compare two or more policies. We show that approaches based on importance sampling can be "unfair"--they can select the worse of two policies more often than not. We give two examples where the unfairness of importance sampling could be practically concerning. We then present sufficient conditions to theoretically guarantee fairness and a related notion of safety. Finally, we provide a practical importance sampling-based estimator to help mitigate one of the systematic sources of unfairness resulting from using importance sampling for policy selection},
  doi={10.24963/ijcai.2018/729}
}

@inproceedings{dovsilovic2018explainable,
  title={Explainable Artificial Intelligence: A Survey},
  author={Do{\v{s}}ilovi{\'c}, Filip Karlo and Br{\v{c}}i{\'c}, Mario and Hlupi{\'c}, Nikica},
  booktitle={2018 41st International convention on information and communication technology, electronics and microelectronics (MIPRO)},
  pages={0210--0215},
  year={2018},
  organization={IEEE},
  doi={10.23919/MIPRO.2018.8400040}
}

@article{Dragan:2015:LegibleRobotMotion, author = "Anca D. Dragan", title = "{Legible Robot Motion Planning}", year = "2015", month = "7", url = "https://kilthub.cmu.edu/articles/thesis/Legible_Robot_Motion_Planning/6720419", doi = "10.1184/R1/6720419.v1" }

%%% EEE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{EderHarperLeonards:2014:HITLRoboticsSafetyAssurance,
  title={Towards the Safety of Human-in-the-Loop Robotics: Challenges and Opportunities for Safety Assurance of Robotic Co-Workers},
  author={Eder, Kerstin and Harper, Chris and Leonards, Ute},
  booktitle={The 23rd IEEE International Symposium on Robot and Human Interactive Communication},
  pages={660--665},
  year={2014},
  organization={IEEE},
  abstract={The success of the human-robot co-worker team in a flexible manufacturing environment where robots learn from demonstration heavily relies on the correct and safe operation of the robot. How this can be achieved is a challenge that requires addressing both technical as well as human-centric research questions. In this paper we discuss the state of the art in safety assurance, existing as well as emerging standards in this area, and the need for new approaches to safety assurance in the context of learning machines. We then focus on robotic learning from demonstration, the challenges these techniques pose to safety assurance and outline opportunities to integrate safety considerations into algorithms “by design”. Finally, from a human-centric perspective, we stipulate that, to achieve high levels of safety and ultimately trust, the robotic co-worker must meet the innate expectations of the humans it works with. It is our aim to stimulate a discussion focused on the safety aspects of human-in-the-loop robotics, and to foster multidisciplinary collaboration to address the research challenges identified.},
  doi={10.1109/ROMAN.2014.6926328}
}

@misc{EC:LegalFramework,
  key={Regulatory framework proposal on artificial intelligence},
  title = {Regulatory framework proposal on artificial intelligence},
  year={2021},
  howpublished = {\url{https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai}},
  note = {(07.06.2022)}
}

@misc{EUResolution:2020:ethicalAI,
  key={Resolution on a framework of ethical artificial intelligence, robotics and related technologies},
  title = {European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020/2012(INL)},
  year={2020},
  howpublished = {\url{https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52020IP0275}},
  note = {(07.06.2022)}
}

@misc{EUDirective:2006:Machinery,
  key = {Machinery Directive 2006/42/EC},
  year = {2006},
  title = {Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on machinery, and amending Directive 95/16/EC},
  howpublished = {\url{https://eur-lex.europa.eu/eli/dir/2006/42/2019-07-26}},
  note = {(07.06.2022)}
}

@article{EvansEtAl:2021:ExplainabilityParadox,
   year = {2022},
   author = {Evans, Theodore and Retzlaff, Carl Orge and Geißler, Christian and Kargl, Michaela and Plass, Markus and Müller, Heimo and Kiehl, Tim-Rasmus and Zerbe, Norman and Holzinger, Andreas},
   title = {The explainability paradox: Challenges for xAI in digital pathology},
   journal = {Future Generation Computer Systems},
   volume = {133},
   number = {8},
   pages = {281--296},
   abstract = {The increasing prevalence of digitised workflows in diagnostic pathology opens the door to life-saving applications of artificial intelligence (AI). Explainability is identified as a critical component for the safety, approval and acceptance of AI systems for clinical use. Despite the cross-disciplinary challenge of building explainable AI (xAI), very few application- and user-centric studies in this domain have been carried out. We conducted the first mixed-methods study of user interaction with samples of state-of-the-art AI explainability techniques for digital pathology. This study reveals challenging dilemmas faced by developers of xAI solutions for medicine and proposes empirically-backed principles for their safer and more effective design.},
   doi = {10.1016/j.future.2022.03.009}
}

@article{eysenbach2018diversity,
  title={Diversity is All You Need: Learning Skills without a Reward Function},
  author={Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.06070},
  year={2018},
  abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.}
}

@inproceedings{NEURIPS2019_5c48ff18,
 author = {Eysenbach, Ben and Salakhutdinov, Russ R and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 publisher = {Curran Associates, Inc.},
 title = {Search on the Replay Buffer: Bridging Planning and Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/5c48ff18e0a47baaf81d8b8ea51eec92-Paper.pdf},
 volume = {32},
 year = {2019}
}


%%% FFF %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{florensa2017stochastic,
  title={Stochastic Neural Networks for Hierarchical Reinforcement Learning},
  author={Florensa, Carlos and Duan, Yan and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1704.03012},
  year={2017},
  abstract = {Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.}
}

@inproceedings{fukuchi2017autonomous,
  title={Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents},
  author={Fukuchi, Yosuke and Osawa, Masahiko and Yamakawa, Hiroshi and Imai, Michita},
  booktitle={Proceedings of the 5th International Conference on Human Agent Interaction},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  pages={97--101},
  year={2017},
  doi = {10.1145/3125739.3125746}
}

@inproceedings{fukuchi2017application,
  title={Application of Instruction-Based Behavior Explanation to a Reinforcement Learning Agent with Changing Policy},
  author={Fukuchi, Yosuke and Osawa, Masahiko and Yamakawa, Hiroshi and Imai, Michita},
  booktitle={International Conference on Neural Information Processing},
  editor={Derong Liu and Shengli Xie and Yuanqing Li and Dongbin Zhao and El-Sayed M. El-Alfy},
  pages={100--108},
  year={2017},
  organization={Springer},
  doi={10.1007/978-3-319-70087-8_11}
}

%%% GGG %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{gao2018reinforcement,
  title={Reinforcement Learning from Imperfect Demonstrations},
  author={Gao, Yang and Xu, Huazhe and Lin, Ji and Yu, Fisher and Levine, Sergey and Darrell, Trevor},
  journal={arXiv preprint arXiv:1802.05313},
  year={2018},
  abstract={proposes a normalized actor-critic algorithm that learns from demonstrations, no supervised loss function is used in the framework. When demonstrations are biased sample from environment, other learning from demonstration methods will not penalize the bad actions explicitly, thus making the undesirable actions from demonstrator favorable. In the normalized actor-critic framework, the Q-function can be normalized over all the actions thus scaling down the Q-values of unfavourable actions. However, the effectiveness is demonstrated on toy Minecraft and Torcs (racing game). This approach can deal with imperfect demonstrations.}
}

@article{GeirhosEtAl:2020:ShortcutLearningDNN,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
  publisher={Nature Publishing Group},
  abstract={Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today’s machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this Perspective we seek to distil how many of deep learning’s failures can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in comparative psychology, education and linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
  doi={10.1038/s42256-020-00257-z}
}

@article{GlanoisEtAl:2021:SurveyInterpretableRL,
  title={A Survey on Interpretable Reinforcement Learning},
  author={Glanois, Claire and Weng, Paul and Zimmer, Matthieu and Li, Dong and Yang, Tianpei and Hao, Jianye and Liu, Wulong},
  journal={arXiv preprint arXiv:2112.13112},
  year={2021},
  abstract={Although deep reinforcement learning has become a promising machine learning approach for sequential decision-making problems, it is still not mature enough for high-stake domains such as autonomous driving or medical applications. In such contexts, a learned policy needs for instance to be interpretable, so that it can be inspected before any deployment (e.g., for safety and verifiability reasons). This survey provides an overview of various approaches to achieve higher interpretability in reinforcement learning (RL). To that aim, we distinguish interpretability (as a property of a model) and explainability (as a post-hoc operation, with the intervention of a proxy) and discuss them in the context of RL with an emphasis on the former notion. In particular, we argue that interpretable RL may embrace different facets: interpretable inputs, interpretable (transition/reward) models, and interpretable decision-making. Based on this scheme, we summarize and analyze recent work related to interpretable RL with an emphasis on papers published in the past 10 years. We also discuss briefly some related research areas and point to some potential promising research directions.}
}

@article{GoodfellowShlensSzegedy:2014:AdversarialExamples,
  title={Explaining and Harnessing Adversarial Examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014},
  abstract={Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.}
}

@inproceedings{griffith2013policy,
  title={Policy Shaping: Integrating Human Feedback with Reinforcement Learning},
  author={Griffith, Shane and Subramanian, Kaushik and Scholz, Jonathan and Isbell, Charles L and Thomaz, Andrea L},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
  volume={26},
  year={2013},
  abstract = {A long term goal of Interactive Reinforcement Learning is to incorporate non-expert human feedback to solve complex tasks. State-of-the-art methods have approached this problem by mapping human information to reward and value signals to indicate preferences and then iterating over them to compute the necessary control policy. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct labels on the policy. We compare Advise to state-of-the-art approaches and highlight scenarios where it outperforms them and importantly is robust to infrequent and inconsistent human feedback.}
}

@article{guan2020explanation,
  title={Explanation Augmented Feedback in Human-in-the-Loop Reinforcement Learning},
  author={Guan, Lin and Verma, Mudit and Guo, Sihang and Zhang, Ruohan and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2006.14804v3},
  year={2020},
  abstract = {Human-in-the-loop Reinforcement Learning (HRL) aims to integrate human guid- ance with Reinforcement Learning (RL) algorithms to improve sample efficiency and performance. A common type of human guidance in HRL is binary evaluative “good" or “bad" feedback for queried states and actions. However, this type of learning scheme suffers from the problems of weak supervision and poor efficiency in leveraging human feedback. To address this, we present EXPAND (EXPlanation AugmeNted feeDback) which provides a visual explanation in the form of saliency maps from humans in addition to the binary feedback. EXPAND employs a state perturbation approach based on salient information in the state to augment the binary feedback. We choose five tasks, namely Pixel-Taxi and four Atari games, to evaluate this approach. We demonstrate the effectiveness of our method using two metrics: environment sample efficiency and human feedback sample efficiency. We show that our method significantly outperforms previous methods. We also analyze the results qualitatively by visualizing the agent’s attention. Finally, we present an ablation study to confirm our hypothesis that augmenting binary feedback with state salient information results in a boost in performance.}
}

@inproceedings{guo2021edge,
  title={EDGE: Explaining Deep Reinforcement Learning Policies},
  author={Guo, Wenbo and Wu, Xian and Khan, Usmann and Xing, Xinyu},
  booktitle={Advances in Neural Information Processing Systems},
  editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages = {12222--12236},
  volume={34},
  year={2021}
}

@article{Guo:2022:RLSurveyHumanPriorKnowledge,
  title={Survey of Reinforcement Learning based on Human Prior Knowledge},
  author={Guo, Zijing and Yao, Chendie and Feng, Yanghe and Xu, Yue},
  journal={Journal of Uncertain Systems},
  volume={15},
  number={01},
  pages={2230001},
  year={2022},
  publisher={World Scientific},
  abstract={At present, task planning is mainly solved by rules or operational research methods. The time complexity and space complexity of these methods increase exponentially with the growth of problem size. Thus, it is challenging to solve large-scale problems. Moreover, it is helpless to solve dynamic task planning problems. Reinforcement learning (RL) is often used to solve the dynamic planning problem of continuous decision making. RL agent constantly interacts with the environment to achieve the expected goal. However, the RL method meets challenges when handling the problem with ample state space. More sampling and exploration are needed to update the strategy gradient, and the convergence speed is slow. Humans ensure a quick start of learning by using prior knowledge, which reduces the exploration time of problems. Thus, we review the methods which combine human prior knowledge with RL through temporal node classification of human prior knowledge combined with RL. In this way, agents effectively reduce the sampling and exploration of the environment and eventually get the optimal strategy faster. Finally, we propose the development direction.},
  doi = {10.1142/S1752890922300011}
}

@article{GunningAha:2019:DARPA,
   year = {2019},
   author = {Gunning, David and Aha, David W.},
   title = {DARPA's Explainable Artificial Intelligence Program},
   journal = {AI Magazine},
   volume = {40},
   number = {2},
   pages = {44--58},
   abstract = {Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA’s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychological requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychological theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems’ explanations improve user understanding, user trust, and user task performance.},
   doi = {10.1609/aimag.v40i2.2850 }
}

@article{GuptaEtAl:2021:MARLCoordinationHAMMER,
  title={HAMMER: Multi-Level Coordination of Reinforcement Learning Agents via Learned Messaging},
  author={Gupta, Nikunj and Srinivasaraghavan, G and Mohalik, Swarup Kumar and Taylor, Matthew E},
  journal={arXiv preprint arXiv:2102.00824},
  year={2021},
  abstract={Cooperative multi-agent reinforcement learning (MARL) has achieved significant results, most notably by leveraging the representation learning abilities of deep neural networks. However, large centralized approaches quickly become infeasible as the number of agents scale, and fully decentralized approaches can miss important opportunities for information sharing and coordination. Furthermore, not all agents are equal - in some cases, individual agents may not even have the ability to send communication to other agents or explicitly model other agents. This paper considers the case where there is a single, powerful, central agent that can observe the entire observation space, and there are multiple, low powered, local agents that can only receive local observations and cannot communicate with each other. The job of the central agent is to learn what message to send to different local agents, based on the global observations, not by centrally solving the entire problem and sending action commands, but by determining what additional information an individual agent should receive so that it can make a better decision. After explaining our MARL algorithm, hammer, and where it would be most applicable, we implement it in the cooperative navigation and multi-agent walker domains. Empirical results show that 1) learned communication does indeed improve system performance, 2) results generalize to multiple numbers of agents, and 3) results generalize to different reward structures.}
}

@article{DBLP:journals/corr/abs-1912-12191,
  author    = {Piyush Gupta and
               Nikaash Puri and
               Sukriti Verma and
               Dhruv Kayastha and
               Shripad Deshmukh and
               Balaji Krishnamurthy and
               Sameer Singh},
  title     = {Explain Your Move: Understanding Agent Actions Using Focused Feature Saliency},
  journal   = {arXiv preprint arXiv:1912.12191v2},
  year      = {2019},
  publisher = {arXiv} 
}

%%% HHH %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{HaSchmidhuber:2018:CoreKnowledgeWorldModels,
  title={World Models},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1803.10122},
  year={2018},
  abstract={We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. },
  doi = {10.5281/ZENODO.1207631}
}

@article{habibian:21,
  title={Here's What I've Learned: Asking Questions that Reveal Reward Learning},
  author={Habibian, Soheil and Jonnavittula, Ananth and Losey, Dylan P},
  journal={arXiv preprint arXiv:2107.01995},
  year={2021},
  abstract={The state-of-the-art preference learning focuses on informative questions; however, most people assume that the robot's questions reveal the robot's knowledge. This paper studies how robot questions influence humans' perception of a robot. The robot chooses informative questions that at the same time reveal its learning. A user study shows that people perceived that the robot learned similarly with purely informative queries or with the combined approach. However, users prefer revealing+informative questions since they find these questions easier to answer, more revealing about the robot's learning, and better focused on uncertain aspects of the task. A follow-up study determines that accounting what the human learns from the robot's learning helps decide which features are still unclear to the robot and whether the robot is ready to be deployed}
}

@inproceedings{hadfield:17,
  title={Inverse Reward Design},
  author={Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart J and Dragan, Anca},
  booktitle={Advances in Neural Information Processing Systems},
  editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  volume={30},
  year={2017},
  abstract={Autonomous agents optimize the reward function we give them. What they don’tknow is how hard it is for us to design a reward function that actually captureswhat we want.   When designing the reward,  we might think of some specifictraining scenarios, and make sure that the reward will lead to the right behaviorinthosescenarios. Inevitably, agents encounternewscenarios (e.g., new types ofterrain) where optimizing that same reward may lead to undesired behavior. Ourinsight is that reward functions are merelyobservationsabout what the designeractuallywants, and that they should be interpreted in the context in which they weredesigned. We introduceinverse reward design(IRD) as the problem of inferring thetrue objective based on the designed reward and the training MDP. We introduceapproximate methods for solving IRD problems, and use their solution to planrisk-averse behavior in test MDPs. Empirical results suggest that this approach canhelp alleviate negative side effects of misspecified reward functions and mitigatereward hacking.}
}

@article{Harmon:1997:ReinforcementLearningATutorial,
  title={Reinforcement Learning: A Tutorial.},
  author={Harmon, Mance E and Harmon, Stephanie S},
  journal = {Technical Report Defense Technical Information Center},
  year={1997},
  publisher={WRIGHT LAB WRIGHT-PATTERSON AFB OH},
  abstract={The purpose of this tutorial is to provide an introduction to reinforcement learning RL at a level easily understood by students and researchers in a wide range of disciplines. The intent is not to present a rigorous mathematical discussion that requires a great deal of effort on the part of the reader, but rather to present a conceptual framework that might serve as an introduction to a more rigorous study of RL. The fundamental principles and techniques used to solve RL problems are presented. The most popular RL algorithms are presented. Section 1 presents an overview of RL and provides a simple example to develop intuition of the underlying dynamic programming mechanism. In Section 2 the parts of a reinforcement learning problem are discussed. These include the environment, reinforcement function, and value function. Section 3 gives a description of the most widely used reinforcement learning algorithms. These include TDlambda and both the residual and direct forms of value iteration, Q-learning, and advantage learning. In Section 4 some of the ancillary issues of RL are briefly discussed, such as choosing an exploration strategy and a discount factor. The conclusion is given in Section 5. Finally, Section 6 is a glossary of commonly used terms followed by references and bibliography.}
}

@inproceedings{hasanbeig2021deepsynth,
  title={DeepSynth: Automata Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning},
  author={Hasanbeig, Mohammadhosein and Jeppu, Natasha Yogananda and Abate, Alessandro and Melham, Tom and Kroening, Daniel},
  booktitle={The Thirty-Fifth $\{$AAAI$\}$ Conference on Artificial Intelligence,$\{$AAAI$\}$},
  volume={2},
  pages={36},
  year={2021}
}

@inproceedings{HayesShah:2017:AutonomousPolicyExplanation,
  title={Improving Robot Controller Transparency Through Autonomous Policy Explanation},
  author={Hayes, Bradley and Shah, Julie A},
  booktitle={2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI},
  pages={303--312},
  year={2017},
  organization={IEEE},
  abstract={Shared expectations and mutual understanding are critical facets of teamwork. Achieving these in human-robot collaborative contexts can be especially challenging, as humans and robots are unlikely to share a common language to convey intentions, plans, or justifications. Even in cases where human co-workers can inspect a robot's control code, and particularly when statistical methods are used to encode control policies, there is no guarantee that meaningful insights into a robot's behavior can be derived or that a human will be able to efficiently isolate the behaviors relevant to the interaction. We present a series of algorithms and an accompanying system that enables robots to autonomously synthesize policy descriptions and respond to both general and targeted queries by human collaborators. We demonstrate applicability to a variety of robot controller types including those that utilize conditional logic, tabular reinforcement learning, and deep reinforcement learning, synthesizing informative policy descriptions for collaborators and facilitating fault diagnosis by non-experts.},
  doi = {10.1145/2909824.3020233}
}

@inproceedings{hazan2019provably,
  title={Provably Efficient Maximum Entropy Exploration},
  author={Hazan, Elad and Kakade, Sham and Singh, Karan and Van Soest, Abby},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = {97},
  series = {Proceedings of Machine Learning Research},
  pages={2681--2691},
  year={2019},
  organization={PMLR},
  abstract = {Suppose an agent is in a (possibly unknown) Markov Decision Process in the absence of a reward signal, what might we hope that an agent can efficiently learn to do? This work studies a broad class of objectives that are defined solely as functions of the state-visitation frequencies that are induced by how the agent behaves. For example, one natural, intrinsically defined, objective problem is for the agent to learn a policy which induces a distribution over state space that is as uniform as possible, which can be measured in an entropic sense. We provide an efficient algorithm to optimize such such intrinsically defined objectives, when given access to a black box planning oracle (which is robust to function approximation). Furthermore, when restricted to the tabular setting where we have sample based access to the MDP, our proposed algorithm is provably efficient, both in terms of its sample and computational complexities. Key to our algorithmic methodology is utilizing the conditional gradient method (a.k.a. the Frank-Wolfe algorithm) which utilizes an approximate MDP solver.}
}

@article{hein2017particle,
  title={Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies},
  author={Hein, Daniel and Hentschel, Alexander and Runkler, Thomas and Udluft, Steffen},
  journal={Engineering Applications of Artificial Intelligence},
  volume={65},
  pages={87--98},
  year={2017},
  publisher={Elsevier},
  abstract = {Fuzzy controllers are efficient and interpretable system controllers for continuous state and action spaces. To date, such controllers have been constructed manually or trained automatically either using expert-generated problem-specific cost functions or incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not found in most real-world reinforcement learning (RL) problems. In such applications, online learning is often prohibited for safety reasons because it requires exploration of the problem’s dynamics during policy training. We introduce a fuzzy particle swarm reinforcement learning (FPSRL) approach that can construct fuzzy RL policies solely by training parameters on world models that simulate real system dynamics. These world models are created by employing an autonomous machine learning technique that uses previously generated transition samples of a real system. To the best of our knowledge, this approach is the first to relate self-organizing fuzzy controllers to model-based batch RL. FPSRL is intended to solve problems in domains where online learning is prohibited, system dynamics are relatively easy to model from previously generated default policy transition samples, and it is expected that a relatively easily interpretable control policy exists. The efficiency of the proposed approach with problems from such domains is demonstrated using three standard RL benchmarks, i.e., mountain car, cart-pole balancing, and cart-pole swing-up. Our experimental results demonstrate high-performing, interpretable fuzzy policies.},
  doi = {10.1016/j.engappai.2017.07.005}
}

@article{hein2018interpretable,
  title={Interpretable policies for reinforcement learning by genetic programming},
  author={Hein, Daniel and Udluft, Steffen and Runkler, Thomas A},
  journal={Engineering Applications of Artificial Intelligence},
  volume={76},
  pages={158--169},
  year={2018},
  publisher={Elsevier},
  abstract = {The search for interpretable reinforcement learning policies is of high academic and industrial interest. Especially for industrial systems, domain experts are more likely to deploy autonomously learned controllers if they are understandable and convenient to evaluate. Basic algebraic equations are supposed to meet these requirements, as long as they are restricted to an adequate complexity. Here we introduce the genetic programming for reinforcement learning (GPRL) approach based on model-based batch reinforcement learning and genetic programming, which autonomously learns policy equations from pre-existing default state–action trajectory samples. GPRL is compared to a straightforward method which utilizes genetic programming for symbolic regression, yielding policies imitating an existing well-performing, but non-interpretable policy. Experiments on three reinforcement learning benchmarks, i.e., mountain car, cart–pole balancing, and industrial benchmark, demonstrate the superiority of our GPRL approach compared to the symbolic regression method. GPRL is capable of producing well-performing interpretable reinforcement learning policies from pre-existing default trajectory data.},
  doi = {10.1016/j.engappai.2018.09.007}
}

@inproceedings{hein2019generating,
  title={Generating Interpretable Reinforcement Learning Policies using Genetic Programming},
  author={Hein, Daniel and Udluft, Steffen and Runkler, Thomas A},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  publisher = {Association for Computing Machinery},
  pages={23--24},
  year={2019},
  abstract = {The search for interpretable reinforcement learning policies is of high academic and industrial interest. Especially for industrial systems, domain experts are more likely to deploy autonomously learned controllers if they are understandable and convenient to evaluate. Basic algebraic equations are supposed to meet these requirements, as long as they are restricted to an adequate complexity. In our recent work "Interpretable policies for reinforcement learning by genetic programming" published in Engineering Applications of Artificial Intelligence 76 (2018), we introduced the genetic programming for reinforcement learning (GPRL) approach. GPRL uses model-based batch reinforcement learning and genetic programming and autonomously learns policy equations from preexisting default state-action trajectory samples. Experiments on three reinforcement learning benchmarks demonstrate that GPRL can produce human-interpretable policies of high control performance.},
  doi = {10.1145/3319619.3326755}
}

@book{Hermann:2011:Sonification,
  title={The Sonification Handbook},
  author={Hermann, Thomas and Hunt, Andy and Neuhoff, John G},
  year={2011},
  publisher={Logos Verlag Berlin}
}

@inproceedings{hester2018deep,
  title={Deep Q-Learning from Demonstrations},
  author={Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Osband, Ian and Dulac-Arnold, Gabriel and Agapiou, John and Leibo, Joel and Gruslys, Audrunas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  year={2018},
  abstract={Using demonstrations in Deep RL using a combination of RL loss and supervised large margin classification loss to imitate the expert actions. However, experiments were done with human demonstrators on Atari games, no robotics domain considered},
  doi={10.1609/aaai.v32i1.11757}
}

@article{heuillet2021explainability,
  title={Explainability in Deep Reinforcement Learning},
  author={Heuillet, Alexandre and Couthouis, Fabien and D{\'\i}az-Rodr{\'\i}guez, Natalia},
  journal={Knowledge-Based Systems},
  volume={214},
  pages={106685},
  issn = {0950-7051},
  year={2021},
  publisher={Elsevier},
  abstract = {A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent’s behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainability. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems.},
  doi = {10.1016/j.knosys.2020.106685}
}

@article{Hochreiter:1997:Lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press},
  abstract={Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O . 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  doi={10.1162/neco.1997.9.8.1735}
}

@article{Hoffman:2018:MetricsXAI,
  title={Metrics for Explainable AI: Challenges and Prospects},
  author={Hoffman, Robert R and Mueller, Shane T and Klein, Gary and Litman, Jordan},
  journal={arXiv preprint arXiv:1812.04608},
  year={2018},
  abstract={The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.}
}

@article{HolzingerEtAl:2019:Wiley-Paper,
  year = {2019},
  author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and Müller, Heimo},
  title = {Causability and explainability of artificial intelligence in medicine},
  journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume = {9},
  number = {4},
  pages = {1--13},
  abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black‐box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use‐case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system.},
  doi = {10.1002/widm.1312}
}

@article{HolzingerMueller:2021:HumanAI,
  year = {2021},
  author = {Holzinger, Andreas and Mueller, Heimo},
  title = {Toward Human-AI Interfaces to Support Explainability and Causability in Medical AI},
  journal = {IEEE COMPUTER},
  volume = {54},
  number = {10},
  pages = {78--86},
  abstract = {Our concept of causability is a measure of whether and to what extent humans can understand a given machine explanation. We motivate causability with a clinical case from cancer research. We argue for using causability in medical artificial intelligence (AI) to develop and evaluate future human-AI interfaces.},
  doi = {10.1109/MC.2021.3092610}
}

@article{HolzingerEtAl:2021:MultiModalCausabilityGNN,
  year = {2021},
  author = {Holzinger, Andreas and Malle, Bernd and Saranti, Anna and Pfeifer, Bastian},
  title = {Towards multi-modal causability with Graph Neural Networks enabling information fusion for explainable AI},
  journal = {Information Fusion},
  volume = {71},
  number = {7},
  pages = {28--37},
  abstract = {AI is remarkably successful and outperforms human experts in certain tasks, even in complex domains such as medicine. Humans on the other hand are experts at multi-modal thinking and can embed new inputs almost instantly into a conceptual knowledge space shaped by experience. In many fields the aim is to build systems capable of explaining themselves, engaging in interactive what-if questions. Such questions, called counterfactuals, are becoming important in the rising field of explainable AI (xAI). Our central hypothesis is that using conceptual knowledge as a guiding model of reality will help to train more explainable, more robust and less biased machine learning models, ideally able to learn from fewer data. One important aspect in the medical domain is that various modalities contribute to one single result. Our main question is “How can we construct a multi-modal feature representation space (spanning images, text, genomics data) using knowledge bases as an initial connector for the development of novel explanation interface techniques?”. In this paper we argue for using Graph Neural Networks as a method-of-choice, enabling information fusion for multi-modal causability (causability – not to confuse with causality – is the measurable extent to which an explanation to a human expert achieves a specified level of causal understanding). The aim of this paper is to motivate the international xAI community to further work into the fields of multi-modal embeddings and interactive explainability, to lay the foundations for effective future human–AI interfaces. We emphasize that Graph Neural Networks play a major role for multi-modal causability, since causal links between features can be defined directly using graph structures.},
  doi = {10.1016/j.inffus.2021.01.008}
}

@article{Holzinger:2016:iML,
  year = {2016},
  author = {Holzinger, Andreas},
  title = {Interactive machine learning for health informatics: when do we need the human-in-the-loop?},
  journal = {Brain Informatics},
  volume = {3},
  number = {2},
  pages = {119--131},
  abstract = {Machine learning (ML) is the fastest growing field in computer science, and health informatics is amongst the greatest challenges. The goal of ML is to develop algorithms which can learn and improve over time and can be used for predictions. Most ML researchers concentrate on automatic Machine Learning (aML), where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from big data with many training sets. However, in the health domain, sometimes we are confronted with a small number of data sets or rare events, where aML-approaches suffer of insufficient training samples. Here interactive Machine Learning (iML) may be of help, having its roots in Reinforcement Learning (RL), Preference Learning (PL) and Active Learning (AL). The term iML is not yet well used, so we define it as algorithms that can interact with agents and can optimize their learning behaviour through these interactions, where the agents can also be human. This human-in-the-loop can be beneficial in solving computationally hard problems, e.g., subspace clustering, protein folding, or k-anonymization of health data, where human expertise can help to reduce an exponential search space through heuristic selection of samples. Therefore, what would otherwise be an NP-hard problem, reduces greatly in complexity through the input and the assistance of a human agent involved in the learning phase.},
  keywords = {interactive Machine learning, health informatics},
  doi = {10.1007/s40708-016-0042-6}
}

@incollection{HolzingerEtAl:2016:iMLExperiment,
  year = {2016},
  author = {Holzinger, Andreas and Plass, Markus and Holzinger, Katharina and Crisan, Gloria Cerasela and Pintea, Camelia-M. and Palade, Vasile},
  title = {Towards interactive Machine Learning (iML): Applying Ant Colony Algorithms to solve the Traveling Salesman Problem with the Human-in-the-Loop approach},
  booktitle = {Springer Lecture Notes in Computer Science LNCS 9817},
  publisher = {Springer},
  address = {Heidelberg, Berlin, New York},
  pages = {81--95},
  abstract = {Most Machine Learning (ML) researchers focus on automatic Machine Learning (aML) where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from the availability of "big data". However, sometimes, for example in health informatics, we are confronted not a small number of data sets or rare events, and with complex problems where aML-approaches fail or deliver unsatisfactory results. Here, interactive Machine Learning (iML) may be of help and the "human-in-the-loop" approach may be beneficial in solving computationally hard problems, where human expertise can help to reduce an exponential search space through heuristics. In this paper, experiments are discussed which help to evaluate the effectiveness of the iML-"human-in-the-loop" approach, particularly in opening the "black box", thereby enabling a human to directly and indirectly manipulating and interacting with an algorithm. For this purpose, we selected the Ant Colony Optimization (ACO) framework, and use it on the Traveling Salesman Problem (TSP) which is of high importance in solving many practical problems in health informatics, e.g. in the study of proteins.},
  keywords = {interactive Machine Learning, Human-in-the-loop, Traveling Salesman Problem, Ant Colony Optimization},
  doi = {10.1007/978-3-319-45507-56}
}

@article{Holzinger:2019:HumanLoopAPIN,
  year = {2019},
  author = {Holzinger, Andreas and Plass, Markus and Kickmeier-Rust, Michael and Holzinger, Katharina and Crişan, Gloria Cerasela and Pintea, Camelia-M. and Palade, Vasile},
  title = {Interactive machine learning: experimental evidence for the human in the algorithmic loop},
  journal = {Applied Intelligence},
  volume = {49},
  number = {7},
  pages = {2401--2414},
  doi = {10.1007/s10489-018-1361-5}
}

@incollection{HolzWoj:2022:XAIOverview,
  year = {2022},
  author = {Holzinger, Andreas and Saranti, Anna and Molnar, Christoph and Biececk, Prezemyslaw and Samek, Wojciech},
  title = {Explainable AI Methods - A Brief Overview},
  booktitle = {XXAI - Lecture Notes in Artificial Intelligence LNAI 13200},
  publisher = {Springer},
  doi = {10.1007/978-3-031-04083-2_2}
}

@article{HolzingerEtAl:2020:QualityOfExplanations,
  year = {2020},
  author = {Holzinger, Andreas and Carrington, Andre and Mueller, Heimo},
  title = {Measuring the Quality of Explanations: The System Causability Scale (SCS). Comparing Human and Machine Explanations},
  journal = {KI - Künstliche Intelligenz (German Journal of Artificial intelligence), Special Issue on Interactive Machine Learning, Edited by Kristian Kersting, TU Darmstadt},
  volume = {34},
  number = {2},
  pages = {193--198},
  doi = {10.1007/s13218-020-00636-z}
}

@article{HolzingerMueller:2022:PersonasAI,
  year = {2022},
  author = {Holzinger, Andreas and Kargl, Michaela and Kipperer, Bettina and Regitnig, Peter and Plass, Markus and Müller, Heimo},
  title = {Personas for Artificial Intelligence (AI) an Open Source Toolbox},
  journal = {IEEE Access},
  volume = {10},
  pages = {23732--23747},
  doi = {10.1109/ACCESS.2022.3154776}
}

@article{Holzinger:2022:DigitalTrans,
  year = {2022},
  author = {Holzinger, Andreas and Saranti, Anna and Angerschmid, Alessa and Retzlaff, Carl Orge and Gronauer, Andreas and Pejakovic, Vladimir and Medel, Francisco and Krexner, Theresa and Gollob, Christoph and Stampfer, Karl},
  title = {Digital Transformation in Smart Farm and Forest Operations Needs Human-Centered AI: Challenges and Future Directions},
  journal = {Sensors},
  volume = {22},
  number = {8},
  pages = {3043},
  doi = {10.3390/s22083043}
}

@incollection{Holzinger:2021:TrustAI,
  year = {2021},
  author = {Holzinger, Andreas},
  title = {The Next Frontier: AI We Can Really Trust},
  booktitle = {Proceedings of the ECML PKDD 2021, CCIS 1524},
  editor = {Kamp, Michael},
  publisher = {Springer Nature},
  address = {Cham},
  pages = {1--14},
  doi = {10.1007/978-3-030-93736-2_33}
}


@article{HudecEtAl-2021-Interpretable,
  year = {2021},
  author = {Hudec, Miroslav and Minarikova, Erika and Mesiar, Radko and Saranti, Anna and Holzinger, Andreas},
  title = {Classification by ordinal sums of conjunctive and disjunctive functions for explainable AI and interpretable machine learning solutions},
  journal = {Knowledge Based Systems},
  volume = {220},
  pages = {106916},
  abstract = {The main goal of classification is dividing entities into several classes. The classification considering uncertainty of belonging to the classes separates entities into the classes yes, no, maybe, where it is desirable to indicate the inclination towards belonging to yes or no. Neural networks have proven their high performance in sharp classification, but the solution is not traceable and therefore difficult or impossible for a human expert to interpret and to understand. Rule-based systems are explainable in principle, however, are based on formal inference structures and also have problems with interpretability due to their high complexity. We must stress that even human experts sometimes cannot explain, but rather construct mental models of the problem and consult these models to select the best possible solution. In our work, we propose classification by aggregation functions of the mixed behaviour by the variability in ordinal sums of conjunctive and disjunctive functions. In this way, domain experts should only assign the key observations regarding considered attributes. Consequently, the variability of functions provides room for machine learning to learn the best possible option from data. Such a solution is re-traceable, reproducible, and explainable to domain experts. In this paper we discuss the proposed approach on examples and outline the research steps in interactive machine learning with a human-in-the-loop via aggregation functions.},
  doi = {10.1016/j.knosys.2021.106916}
}

@article{HusseinEtAl:2017:ImitationLearning,
  author = {Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
  title = {Imitation Learning: A Survey of Learning Methods},
  year = {2017},
  issue_date = {March 2018},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  journal = {ACM Computing Surveys},
  month = {apr},
  articleno = {21},
  numpages = {35},
  volume = {50},
  number = {2},
  issn = {0360-0300},
  url = {https://doi.org/10.1145/3054912},
  abstract = {Imitation learning techniques aim to mimic human behavior in a given task. An agent (a learning machine) is trained to perform a task from demonstrations by learning a mapping between observations and actions. The idea of teaching by imitation has been around for many years; however, the field is gaining attention recently due to advances in computing and sensing as well as rising demand for intelligent applications. The paradigm of learning by imitation is gaining popularity because it facilitates teaching complex tasks with minimal expert knowledge of the tasks. Generic imitation learning methods could potentially reduce the problem of teaching a task to that of providing demonstrations, without the need for explicit programming or designing reward functions specific to the task. Modern sensors are able to collect and transmit high volumes of data rapidly, and processors with high computational power allow fast processing that maps the sensory data to actions in a timely manner. This opens the door for many potential AI applications that require real-time perception and reaction such as humanoid robots, self-driving vehicles, human computer interaction, and computer games, to name a few. However, specialized algorithms are needed to effectively and robustly learn models as learning by imitation poses its own set of challenges. In this article, we survey imitation learning methods and present design options in different steps of the learning process. We introduce a background and motivation for the field as well as highlight challenges specific to the imitation problem. Methods for designing and evaluating imitation learning tasks are categorized and reviewed. Special attention is given to learning methods in robotics and games as these domains are the most popular in the literature and provide a wide array of problems and methodologies. We extensively discuss combining imitation learning approaches using different sources and methods, as well as incorporating other motion learning methods to enhance imitation. We also discuss the potential impact on industry, present major applications, and highlight current and future research directions.},
  doi = {10.1145/3054912}
}

%%% III %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{ibarz2021train,
  title={How to train your robot with deep reinforcement learning: lessons we have learned},
  author={Ibarz, Julian and Tan, Jie and Finn, Chelsea and Kalakrishnan, Mrinal and Pastor, Peter and Levine, Sergey},
  journal={The International Journal of Robotics Research},
  volume={40},
  number={4-5},
  pages={698--721},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England},
  doi={10.1177/0278364920987859}
}

@article{IsbellEtAl:2006:AdaptiveSocialAgent,
  author = {Isbell, Charles Lee and Kearns, Michael and Singh, Satinder and Shelton, Christian R. and Stone, Peter and Kormann, Dave},
  year = {2006},
  title = {Cobot in LambdaMOO: An Adaptive Social Statistics Agent},
  journal = {Autonomous Agents and Multi-Agent Systems},
  volume = {13},
  pages = {327--354},
  abstract = {We describe our development of Cobot, a novel software agent who lives in LambdaMOO, a popular virtual world frequented by hundreds of users. Cobot’s goal was to become an actual part of that community. Here, we present a detailed discussion of the functionality that made him one of the objects most frequently interacted with in LambdaMOO, human or artificial. Cobot’s fundamental power is that he has the ability to collect social statistics summarizing the quantity and quality of interpersonal interactions. Initially, Cobot acted as little more than a reporter of this information; however, as he collected more and more data, he was able to use these statistics as models that allowed him to modify his own behavior. In particular, cobot is able to use this data to “self-program,” learning the proper way to respond to the actions of individual users, by observing how others interact with one another. Further, Cobot uses reinforcement learning to proactively take action in this complex social environment, and adapts his behavior based on multiple sources of human reward. Cobot represents a unique experiment in building adaptive agents who must live in and navigate social spaces.},
  doi = {10.1007/s10458-006-0005-z}
}

@misc{ISO:13482:2014,
  key={EN ISO 13482:2014},
  title ={{EN ISO 13482:2014}},
  year={2014},
  howpublished = {\url{https://www.iso.org/standard/53820.html}},
  note = {(07.06.2022)}
}

%%% JJJ %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{JainEtAl:2021:EpistemicUncertaintyPrediction,
  author    = {Moksh Jain and
               Salem Lahlou and
               Hadi Nekoei and
               Victor Butoi and
               Paul Bertin and
               Jarrid Rector{-}Brooks and
               Maksym Korablyov and
               Yoshua Bengio},
  title     = {{DEUP:} Direct Epistemic Uncertainty Prediction},
  journal   = {arXiv preprint arXiv:2102.08501},
  year      = {2021},
  publisher = {arXiv},
  abstract  = {Epistemic uncertainty is the part of out-of-sample prediction error due to the lack of knowledge of the learner. Whereas previous work was focusing on model variance, we propose a principled approach for directly estimating epistemic uncertainty by learning to predict generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator of epistemic uncertainty includes the effect of model bias and can be applied in non-stationary learning environments arising in active learning or reinforcement learning. In addition to demonstrating these properties of Direct Epistemic Uncertainty Prediction (DEUP), we illustrate its advantage against existing methods for uncertainty estimation on downstream tasks including sequential model optimization and reinforcement learning. We also evaluate the quality of uncertainty estimates from DEUP for probabilistic classification of images and for estimating uncertainty about synergistic drug combinations.}
}

% TODO: check journal and add doi if possible
% did not find journal:  Good Systems-Published Research,
% publisher={University of Texas at Austin}
@inproceedings{jiang:21,
  title={Temporal-Logic-Based Reward Shaping for Continuing Reinforcement Learning Tasks},
  author={Jiang, Yuqian and Bharadwaj, Suda and Wu, Bo and Shah, Rishi and Topcu, Ufuk and Stone, Peter},
  booktitle={Proceedings of the 35th AAAI Conference on Artificial Intelligence},
  year={2021},
  abstract={The paper proposes a framework for non-experienced users to interact with average-reward RL algorithms. More precisely, it allows teachers to give high-level advice. The approach receives the trainer input using temporal logic and provides guarantees optimality independent of the advice quality. Contrary to previous work that used LTL as hard constraints, the authors use LTL formulas to shape the environment reward. Instead of removing actions violating the LTL formulae and possibly causing suboptimal behavior, they are used to guide the RL agent in learning an optimal policy. The proposed framework speeds up the learning rate in comparison to differential Q-learning (the baseline RL algorithm). Poor quality advice causes the agent to learn suboptimal behavior when actions that violate the LTL formula are disallowed. The current approach still allows the agent to learn an optimal behavior regardless of the advice quality type.}
}

@inproceedings{jiang2019neural,
  title={Neural Logic Reinforcement Learning},
  author={Jiang, Zhengyao and Luo, Shan},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages={3110--3119},
  year={2019},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = {97},
  series = {Proceedings of Machine Learning Research},
  organization={PMLR}
}

@article{Jung:2020:ExplainableEmpiricalRiskMinimization,
  title={Explainable Empirical Risk Minimization},
  author={Jung, Alexander},
  journal={arXiv preprint arXiv:2009.01492},
  year={2020},
  abstract={The successful application of machine learning (ML) methods becomes increasingly dependent on their interpretability or explainability. Designing explainable ML systems is instrumental to ensuring transparency of automated decision-making that targets humans. The explainability of ML methods is also an essential ingredient for trustworthy artificial intelligence. A key challenge in ensuring explainability is its dependence on the specific human user ("explainee"). The users of machine learning methods might have vastly different background knowledge about machine learning principles. One user might have a university degree in machine learning or related fields, while another user might have never received formal training in high-school mathematics. This paper applies information-theoretic concepts to develop a novel measure for the subjective explainability of the predictions delivered by a ML method. We construct this measure via the conditional entropy of predictions, given a user signal. This user signal might be obtained from user surveys or biophysical measurements. Our main contribution is the explainable empirical risk minimization (EERM) principle of learning a hypothesis that optimally balances between the subjective explainability and risk. The EERM principle is flexible and can be combined with arbitrary machine learning models. We present several practical implementations of EERM for linear models and decision trees. Numerical experiments demonstrate the application of EERM to detecting the use of inappropriate language on social media.}
}

%%% KKK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Karalus:2021:HITL-counterfactuals,
  author    = {Jakob Karalus and
               Felix Lindner},
  title     = {Accelerating the Convergence of Human-in-the-Loop Reinforcement Learning with Counterfactual Explanations},
  journal   = {arXiv preprint arXiv:2108.01358},
  year      = {2021},
  publisher = {arXiv},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-01358.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kartoun:10,
  title={A Human-Robot Collaborative Reinforcement Learning Algorithm},
  author={Kartoun, Uri and Stern, Helman and Edan, Yael},
  journal={Journal of Intelligent \& Robotic Systems},
  volume={60},
  number={2},
  pages={217--239},
  year={2010},
  publisher={Springer},
  abstract = {This paper presents a new reinforcement learning algorithm that enables collaborative learning between a robot and a human. The algorithm which is based on the Q(λ) approach expedites the learning process by taking advantage of human intelligence and expertise. The algorithm denoted as CQ(λ) provides the robot with self awareness to adaptively switch its collaboration level from autonomous (self performing, the robot decides which actions to take, according to its learning function) to semi-autonomous (a human advisor guides the robot and the robot combines this knowledge into its learning function). This awareness is represented by a self test of its learning performance. The approach of variable autonomy is demonstrated and evaluated using a fixed-arm robot for finding the optimal shaking policy to empty the contents of a plastic bag. A comparison between the CQ(λ) and the traditional Q(λ)-reinforcement learning algorithm, resulted in faster convergence for the CQ(λ) collaborative reinforcement learning algorithm.},
  doi={10.1007/s10846-010-9422-y}
}

@inproceedings{KhatibEtAl:1999:RihEnvironment,
  title={Robots in human environments}, 
  author={Khatib, O. and Yokoi, K. and Brock, O. and Chang, K. and Casal, A.},
  year={1999},
  booktitle={Proceedings of the First Workshop on Robot Motion and Control. RoMoCo'99 (Cat. No.99EX353)}, 
  publisher={IEEE},
  pages={213--221},
  abstract={Discusses the basic capabilities needed to enable robots to operate in human populated environments for accomplishing both autonomous tasks and human-guided tasks. These capabilities are key to many new emerging robotic applications in service, construction, field, underwater, and space. An important characteristic of these robots is the "assistance" ability they can bring to humans in performing various physical tasks. To interact with humans and operate in their environments, these robots must be provided with the functionality of mobility and manipulation. The article presents developments of models, strategies, and algorithms concerned with a number of autonomous capabilities that are essential for robot operations in human environments. These capabilities include: integrated mobility and manipulation, cooperative skills between multiple robots, interaction ability with humans, and efficient techniques for real-time modification of collision-free path. These capabilities are demonstrated on two holonomic mobile platforms designed and built at Stanford University in collaboration with Oak Ridge National Laboratories and Nomadic Technologies.},
  doi={10.1109/ROMOCO.1999.791078}
}

@inproceedings{kim2013learning,
  title={Learning from limited demonstrations},
  author={Kim, Beomjoon and Farahmand, Amir-massoud and Pineau, Joelle and Precup, Doina},
  booktitle={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013},
  publisher = {Curran Associates, Inc.},
  abstract={This method incorporates  demonstrations as linear constraints in the Approximate Policy Iteration framework, posing it as a constraint convex optimization problem. Experiments are shown with optimal and sub-optimal demonstrations on car brake control simulation and Robot path finding task.}
}

@inproceedings{Knox:2008:TAMER,
  author={Knox, W. Bradley and Stone, Peter},
  booktitle={2008 7th IEEE International Conference on Development and Learning}, 
  title={TAMER: Training an Agent Manually via Evaluative Reinforcement}, 
  year={2008},
  volume={},
  number={},
  publisher={IEEE},
  pages={292--297},
  doi={10.1109/DEVLRN.2008.4640845}}
  
@inproceedings{knox:13,
  title={Training a Robot via Human Feedback: A Case Study},
  author={Knox, W. Bradley and Stone, Peter and Breazeal, Cynthia},
  booktitle={International Conference on Social Robotics},
  editor={Guido Herrmann and Martin J. Pearson and Alexander Lenz and Paul Bremner and Adam Spiers and Ute Leonards},
  pages={460--470},
  year={2013},
  organization={Springer},
  abstract={This paper uses Tamer in real robot training. The setup is simple;  the robot has four possible actions: turn left, right, stop, or move forward. However, the robot learned five different behaviors in relation to a training artifact, i.e., following the artifact or moving away from it. The authors report that the main issues causing unsuccessful learning sessions were the lack of transparency between the robot and the teacher. In one case, the actions seemed ambiguous at the beginning of the execution, and the misunderstanding of the action duration caused rewards to the wrong actions. In other cases, the teacher did not realize that the training artifact was out of range from the robot's sensors. The problems encountered when applying the Tamer framework in physical environments illustrate the need to provide feedback to the teacher, especially if we aim to have non-expert teachers.},
  doi={10.1007/978-3-319-02675-6_46}
}

@article{KoberBagnellPeters:2013:RLRoboticsSurvey,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England},
  abstract={Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
  doi={10.1177/0278364913495721}
}

@article{Koditschek:2021:RoboticsCompositionalLanguage,
  title={What Is Robotics? Why Do We Need It and How Can We Get It?},
  author={Koditschek, Daniel E},
  journal={Annual Review of Control, Robotics, and Autonomous Systems},
  volume={4},
  number = {1},
  pages={1--33},
  year={2021},
  publisher={Annual Reviews},
  abstract = {Robotics is an emerging synthetic science concerned with programming work. Robot technologies are quickly advancing beyond the insights of the existing science. More secure intellectual foundations will be required to achieve better, more reliable, and safer capabilities as their penetration into society deepens. Presently missing foundations include the identification of fundamental physical limits, the development of new dynamical systems theory, and the invention of physically grounded programming languages. The new discipline needs a departmental home in the universities, which it can justify both intellectually and by its capacity to attract new diverse populations inspired by the age-old human fascination with robots.},
  doi = {10.1146/annurev-control-080320-011601}
}

@book{Koller:2009:ProbabilisticGraphicalModelsBook,
  title={Probabilistic Graphical Models: Principles and Techniques},
  author={Koller, Daphne and Friedman, Nir},
  year={2009},
  publisher={MIT press}
}

@inproceedings{kulick2013active,
  title={Active Learning for Teaching a Robot Grounded Relational Symbols},
  author={Kulick, Johannes and Toussaint, Marc and Lang, Tobias and Lopes, Manuel},
  booktitle={Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
  pages={1451--1457},
  numpages = {7},
  location = {Beijing, China},
  series = {IJCAI '13},
  year={2013},
  publisher = {AAAI Press}
}

@article{Kulkarni:2021:EducationAIDashboard,
  title={Demonstrating REACT: a Real-time Educational AI-powered Classroom Tool},
  author={Kulkarni, Ajay and Gkountouna, Olga},
  journal={arXiv preprint arXiv:2108.07693},
  year={2021},
  abstract={We present a demonstration of REACT, a new Real-time Educational AI-powered Classroom Tool that employs EDM techniques for supporting the decision-making process of educators. REACT is a data-driven tool with a user-friendly graphical interface. It analyzes students' performance data and provides context-based alerts as well as recommendations to educators for course planning. Furthermore, it incorporates model-agnostic explanations for bringing explainability and interpretability in the process of decision making. This paper demonstrates a use case scenario of our proposed tool using a real-world dataset and presents the design of its architecture and user interface. This demonstration focuses on the agglomerative clustering of students based on their performance (i.e., incorrect responses and hints used) during an in-class activity. This formation of clusters of students with similar strengths and weaknesses may help educators to improve their course planning by identifying at-risk students, forming study groups, or encouraging tutoring between students of different strengths.}
}


%%% LLL %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{LapuschkinEtAl:2016:LRP,
   year = {2016},
   author = {Lapuschkin, Sebastian and Binder, Alexander and Montavon, Gregoire and Müller, Klaus-Robert and Samek, Wojciech},
   title = {The LRP Toolbox for Artificial Neural Networks},
   journal = {The Journal of Machine Learning Research (JMLR)},
   volume = {17},
   number = {1},
   pages = {3938--3942},
   abstract = {The Layer-wise Relevance Propagation (LRP) algorithm explains a classifier's prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself. With the LRP Toolbox we provide platform-agnostic implementations for explaining the predictions of pretrained state of the art Caffe networks and stand-alone implementations for fully connected Neural Network models. The implementations for Matlab and python shall serve as a playing field to familiarize oneself with the LRP algorithm and are implemented with readability and transparency in mind. Models and data can be imported and exported using raw text formats, Matlab's .mat files and the .npy format for numpy or plain text.}
}

@article{Lapuschkin:2019:UnmaskingCleverHans,
  title={Unmasking Clever Hans predictors and assessing what machines really learn},
  author={Lapuschkin, Sebastian and W{\"a}ldchen, Stephan and Binder, Alexander and Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={1--8},
  year={2019},
  publisher={Nature Publishing Group},
  abstract={Current learning machines have successfully solved hard application problems, reaching high accuracy and displaying seemingly intelligent behavior. Here we apply recent techniques for explaining decisions of state-of-the-art learning machines and analyze various tasks from computer vision and arcade games. This showcases a spectrum of problem-solving behaviors ranging from naive and short-sighted, to well-informed and strategic. We observe that standard performance evaluation metrics can be oblivious to distinguishing these diverse problem solving behaviors. Furthermore, we propose our semi-automated Spectral Relevance Analysis that provides a practically effective way of characterizing and validating the behavior of nonlinear learning machines. This helps to assess whether a learned model indeed delivers reliably for the problem that it was conceived for. Furthermore, our work intends to add a voice of caution to the ongoing excitement about machine intelligence and pledges to evaluate and judge some of these recent successes in a more nuanced manner.},
  doi={10.1038/s41467-019-08987-4}
}

@inproceedings{le2018hierarchical,
  title={Hierarchical Imitation and Reinforcement Learning},
  author={Le, Hoang and Jiang, Nan and Agarwal, Alekh and Dudik, Miroslav and Yue, Yisong and Daum{\'e} III, Hal},
  booktitle={Proceedings of the 35th International Conference on Machine Learning},
  editor = {Dy, Jennifer and Krause, Andreas},
  pages={2917--2926},
  year={2018},
  publisher={PMLR},
  volume = {80},
  series = {Proceedings of Machine Learning Research},
  abstract = {We study how to effectively leverage expert feedback to learn sequential decision-making policies. We focus on problems with sparse rewards and long time horizons, which typically pose significant challenges in reinforcement learning. We propose an algorithmic framework, called hierarchical guidance, that leverages the hierarchical structure of the underlying problem to integrate different modes of expert interaction. Our framework can incorporate different combinations of imitation learning (IL) and reinforcement learning (RL) at different levels, leading to dramatic reductions in both expert effort and cost of exploration. Using long-horizon benchmarks, including Montezuma’s Revenge, we demonstrate that our approach can learn significantly faster than hierarchical RL, and be significantly more label-efficient than standard IL. We also theoretically analyze labeling cost for certain instantiations of our framework.}
}

@article{LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE,
  title={PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training},
  author={Lee, Kimin and Smith, Laura and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2106.05091},
  publisher = {arXiv},
  year={2021},
  abstract={Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.}
}

@article{Li:2017:DRLSurvey,
  title={Deep Reinforcement Learning: An Overview},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1701.07274},
  publisher = {arXiv},
  year={2017},
  abstract = {We give an overview of recent exciting achievements of deep reinforcement learn- ing (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value func- tion, in particular, Deep Q-Network (DQN), policy, reward, model and planning, exploration, and knowledge. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi- agent RL, hierarchical RL, and learning to learn. Then we discuss various appli- cations of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, business management, finance, healthcare, education, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.}
}

@article{Li:2019:HumanCenteredRLSurvey,
  title={Human-Centered Reinforcement Learning: A Survey},
  author={Li, Guangliang and Gomez, Randy and Nakamura, Keisuke and He, Bo},
  journal={IEEE Transactions on Human-Machine Systems},
  volume={49},
  number={4},
  pages={337--349},
  year={2019},
  publisher={IEEE},
  abstract={Human-centered reinforcement learning (RL), in which an agent learns how to perform a task from evaluative feed- back delivered by a human observer, has become more and more popular in recent years. The advantage of being able to learn from human feedback for a RL agent has led to increasing applicability to real-life problems. This paper describes the state-of-the-art human- centered RL algorithms and aims to become a starting point for researchers who are initiating their endeavors in human-centered RL. Moreover, the objective of this paper is to present a compre- hensive survey of the recent breakthroughs in this field and provide references to the most interesting and successful works. After start- ing with an introduction of the concepts of RL from environmental reward, this paper discusses the origins of human-centered RL and its difference from traditional RL. Then we describe different in- terpretations of human evaluative feedback, which have produced many human-centered RL algorithms in the past decade. In ad- dition, we describe research on agents learning from both human evaluative feedback and environmental rewards as well as on im- proving the efficiency of human-centered RL. Finally, we conclude with an overview of application areas and a discussion of future work and open questions.},
  doi={10.1109/THMS.2019.2912447}
}


@inproceedings{LiangEtAl:2017:HITLReinforcementLearn,
  author = {Liang, Huanghuang and Yang, Lu and Cheng, Hong and Tu, Wenzhe and Xu, Mengjie},
  booktitle = {2017 Chinese Automation Congress (CAC)}, 
  title = {Human-in-the-loop Reinforcement Learning}, 
  year = {2017},
  pages = {4511--4518},
  abstract = {This paper focuses on presenting a human-in-the-loop reinforcement learning theory framework and foreseeing its application to driving decision making. Currently, the technologies in human-vehicle collaborative driving face great challenges, and do not consider the Human-in-the-loop learning framework and Driving Decision-Maker optimization under the complex road conditions. The main content of this paper aimed at presenting a study framework as follows: (1) the basic theory and model of the hybrid reinforcement learning; (2) hybrid reinforcement learning algorithm for human drivers; (3)hybrid reinforcement learning algorithm for autopilot; (4) Driving decision-maker verification platform. This paper aims at setting up the human-machine hybrid reinforcement learning theory framework and foreseeing its solutions to two kinds of typical difficulties about human-machine collaborative Driving Decision-Maker, which provides the basic theory and key technologies for the future of intelligent driving. The paper serves as a potential guideline for the study of human-in-the-loop reinforcement learning.},
  doi={10.1109/CAC.2017.8243575}
}

@article{likmeta2020combining,
  title={Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving},
  author={Likmeta, Amarildo and Metelli, Alberto Maria and Tirinzoni, Andrea and Giol, Riccardo and Restelli, Marcello and Romano, Danilo},
  journal={Robotics and Autonomous Systems},
  volume={131},
  pages={103568},
  year={2020},
  publisher={Elsevier},
  abstract = {The design of high-level decision-making systems is a topical problem in the field of autonomous driving. In this paper, we combine traditional rule-based strategies and reinforcement learning (RL) with the goal of achieving transparency and robustness. On the one hand, the use of handcrafted rule-based controllers allows for transparency, i.e., it is always possible to determine why a given decision was made, but they struggle to scale to complex driving scenarios, in which several objectives need to be considered. On the other hand, black-box RL approaches enable us to deal with more complex scenarios, but they are usually hardly interpretable. In this paper, we combine the best properties of these two worlds by designing parametric rule-based controllers, in which interpretable rules can be provided by domain experts and their parameters are learned via RL. After illustrating how to apply parameter-based RL methods (PGPE) to this setting, we present extensive numerical simulations in the highway and in two urban scenarios: intersection and roundabout. For each scenario, we show the formalization as an RL problem and we discuss the results of our approach in comparison with handcrafted rule-based controllers and black-box RL techniques.},
  doi = {10.1016/j.robot.2020.103568}
}

@article{lin:20,
  title={A Review on Interactive Reinforcement Learning From Human Social Feedback},
  author={Lin, Jinying and Ma, Zhen and Gomez, Randy and Nakamura, Keisuke and He, Bo and Li, Guangliang},
  journal={IEEE Access},
  volume={8},
  pages={120757--120765},
  year={2020},
  publisher={IEEE},
  abstract={Reinforcement learning agent learns how to perform a task by interacting with the environment.The use of reinforcement learning in real-life applications has been limited because of the sample efficiencyproblem. Interactive reinforcement learning has been developed to speed up the agent’s learning and facilitateto learn from ordinary people by allowing them to provide social feedback, e.g, evaluative feedback, advice orinstruction. Inspired by real-life biological learning scenarios, there could be many ways to provide feedbackfor agent learning, such as via hardware delivered, natural interaction like facial expressions, speech orgestures. The agent can even learn from feedback via unimodal or multimodal sensory input. This paperreviews methods for interactive reinforcement learning agent to learn from human social feedback and theways of delivering feedback. Finally, we discuss some open problems and possible future research directions.},
  doi={10.1109/ACCESS.2020.3006254}
}

% TODO: other publisher? rejected -> no journal? please check
@article{LiuAbbeel:2020:UnsupervisedActivePreTraining,
  title={Unsupervised Active Pre-Training for Reinforcement Learning},
  journal={ICLR 2021},
  note={Paper was rejected due to lack of novelty},
  author={Liu, Hao and Abbeel, Pieter},
  year={2020},
  abstract={We introduce a new unsupervised pre-training method for reinforcement learning called APT, which stands for Active Pre Training. APT learns a representation and a policy initialization by actively searching for novel states in reward-free environments. We use the contrastive learning framework for learning the representation from collected transitions. The key novel idea is to collect data during pre-training by maximizing a particle based entropy computed in the learned latent representation space. By doing particle based entropy maximization, we alleviate the need for challenging density modeling and are thus able to scale our approach to image observations. APT successfully learns meaningful representations as well as policy initializations without using any reward. We empirically evaluate APT on the Atari game suite and DMControl suite by exposing task-specific reward to agent after a long unsupervised pre-training phase. On Atari games, APT achieves human-level performance on 12 games and obtains highly competitive performance compared to canonical fully supervised RL algorithms. On DMControl suite, APT beats all baselines in terms of asymptotic performance and data efficiency and dramatically improves performance on tasks that are extremely difficult for training from scratch. Importantly, the pre-trained models can be fine-tuned to solve different tasks as long as the environment does not change. Finally, we also pre-train multi-environment encoders on data from multiple environments and show generalization to a broad set of RL tasks.}
}

@article{liu2021demonstration,
  title={Demonstration actor critic},
  author={Liu, Guoqing and Zhao, Li and Zhang, Pushi and Bian, Jiang and Qin, Tao and Yu, Nenghai and Liu, Tie-Yan},
  journal={Neurocomputing},
  volume={434},
  pages={194--202},
  year={2021},
  publisher={Elsevier},
  abstract={We study the problem of Reinforcement Learning from Demonstrations (RLfD), where the agent has access to not only reward signals from the environment, but also some available expert demonstrations. Recent works absorb ingredients from imitation learning and utilize demonstration data as reward reshaping. Despite their success, these methods update policy over these states seen in the demonstration data, in the same way as other states in the state space, overlooking the validity of direct supervision signals on these states. To address this issue, we propose a novel RLfD objective function with a new shaping reward, by optimizing which can directly leverage the supervision signal on these demonstrated states. We propose a general framework for policy optimization of the proposed objective, with convergence guarantees under the classic tabular setting. Based on that, we further make some approximations based on deep neural networks, and then introduce a new practical algorithm, called Demonstration Actor Critic (DAC) in large continuous domains. Extensive experiments on a range of popular benchmark sparse-reward tasks show that our method can lead to significant performance gains over several strong and off-the-shelf baselines.},
  doi = {10.1016/j.neucom.2020.12.116}
}

@inproceedings{LiuEtAl:2018:LinearModelUTrees,
  title={Toward Interpretable Deep Reinforcement Learning with Linear Model U-Trees},
  author={Liu, Guiliang and Schulte, Oliver and Zhu, Wang and Li, Qingcan},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={414--429},
  year={2018},
  organization={Springer},
  abstract={Deep Reinforcement Learning (DRL) has achieved impressive success in many applications. A key component of many DRL models is a neural network representing a Q function, to estimate the expected cumulative reward following a state-action pair. The Q function neural network contains a lot of implicit knowledge about the RL problems, but often remains unexamined and uninterpreted. To our knowledge, this work develops the first mimic learning framework for Q functions in DRL. We introduce Linear Model U-trees (LMUTs) to approximate neural network predictions. An LMUT is learned using a novel on-line algorithm that is well-suited for an active play setting, where the mimic learner observes an ongoing interaction between the neural net and the environment. Empirical evaluation shows that an LMUT mimics a Q function substantially better than five baseline methods. The transparent tree structure of an LMUT facilitates understanding the network’s learned strategic knowledge by analyzing feature influence, extracting rules, and highlighting the super-pixels in image inputs.},
  doi={10.1007/978-3-030-10928-8_25}
}

@article{LiuGuoMahmud:2021:HITLErrorDetectionFramework,
  title={When and Why does a Model Fail? A Human-in-the-loop Error Detection Framework for Sentiment Analysis},
  author={Liu, Zhe and Guo, Yufan and Mahmud, Jalal},
  journal={arXiv preprint arXiv:2106.00954},
  publisher = {arXiv},
  year={2021},
  abstract={Although deep neural networks have been widely employed and proven effective in sen- timent analysis tasks, it remains challenging for model developers to assess their models for erroneous predictions that might exist prior to deployment. Once deployed, emergent er- rors can be hard to identify in prediction run- time and impossible to trace back to their sources. To address such gaps, in this paper we propose an error detection framework for sentiment analysis based on explainable fea- tures. We perform global-level feature valida- tion with human-in-the-loop assessment, fol- lowed by an integration of global and local- level feature contribution analysis. Experimen- tal results show that, given limited human-in- the-loop intervention, our method is able to identify erroneous model predictions on un- seen data with high precision.}
}

@article{LuetjensEverettHow:2018:RLModelUncertainty,
  author    = {Bjoern Lutjens and
               Michael Everett and
               Jonathan P. How},
  title     = {Safe Reinforcement Learning with Model Uncertainty Estimates},
  journal   = {arXiv preprint arXiv:1810.08700},
  year      = {2018},
  abstract  = {Many current autonomous systems are being designed with a strong reliance on black box predictions from deep neural networks (DNNs). However, DNNs tend to be overconfident in predictions on unseen data and can give unpredictable results for far-from-distribution test data. The importance of predictions that are robust to this distributional shift is evident for safety-critical applications, such as collision avoidance around pedestrians. Measures of model uncertainty can be used to identify unseen data, but the state-of-the-art extraction methods such as Bayesian neural networks are mostly intractable to compute. This paper uses MC-Dropout and Bootstrapping to give computationally tractable and parallelizable uncertainty estimates. The methods are embedded in a Safe Reinforcement Learning framework to form uncertainty-aware navigation around pedestrians. The result is a collision avoidance policy that knows what it does not know and cautiously avoids pedestrians that exhibit unseen behavior. The policy is demonstrated in simulation to be more robust to novel observations and take safer actions than an uncertainty-unaware baseline.}
}

@inproceedings{lyu2019sdrl,
  title={SDRL: Interpretable and Data-Efficient Deep Reinforcement Learning Leveraging Symbolic Planning},
  author={Lyu, Daoming and Yang, Fangkai and Liu, Bo and Gustafson, Steven},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  numer={01},
  pages={2970--2977},
  year={2019},
  doi={10.1609/aaai.v33i01.33012970 }
}

%%% MMM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{macglashan2017interactive,
  title={Interactive Learning from Policy-Dependent Human Feedback},
  author={MacGlashan, James and Ho, Mark K and Loftin, Robert and Peng, Bei and Wang, Guan and Roberts, David L and Taylor, Matthew E and Littman, Michael L},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  editor = {Precup, Doina and Teh, Yee Whye},
  volume = {70},
  series = {Proceedings of Machine Learning Research},
  pages={2285--2294},
  year={2017},
  publisher={PMLR},
  abstract = {This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner’s current policy. We present empirical results that show this assumption to be false—whether human trainers give a positive or negative feedback for a decision is influenced by the learner’s current policy. Based on this insight, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that COACH can successfully learn multiple behaviors on a physical robot.}
}
  
@inproceedings{MadumalEtAl:2020:CausalRLCFs,
  title={Explainable Reinforcement Learning through a Causal Lens},
  author={Madumal, Prashan and Miller, Tim and Sonenberg, Liz and Vetere, Frank},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={2493--2500},
  year={2020},
  abstract={Prominent theories in cognitive science propose that humans understand and represent the knowledge of the world through causal relationships. In making sense of the world, we build causal models in our mind to encode cause-effect relations of events and use these to explain why new events happen by referring to counterfactuals — things that did not happen. In this paper, we use causal models to derive causal explanations of the behaviour of model-free reinforcement learning agents. We present an approach that learns a structural causal model during reinforcement learning and encodes causal relationships between variables of interest. This model is then used to generate explanations of behaviour based on counterfactual analysis of the causal model. We computationally evaluate the model in 6 domains and measure performance and task prediction accuracy. We report on a study with 120 participants who observe agents playing a real-time strategy game (Starcraft II) and then receive explanations of the agents' behaviour. We investigate: 1) participants' understanding gained by explanations through task prediction; 2) explanation satisfaction and 3) trust. Our results show that causal model explanations perform better on these measures compared to two other baseline explanation models.},
  doi={10.1609/aaai.v34i03.5631}
}

@article{Madumal:2020:DistalEF,
  title={Distal Explanations for Model-free Explainable Reinforcement Learning},
  author={Prashan Madumal and Tim Miller and Liz Sonenberg and Frank Vetere},
  journal={arXiv preprint arXiv:2001.10284},
  year={2020},
  publisher = {arXiv},
  abstract={In this paper we introduce and evaluate a distal explanation model for model-free reinforcement learning agents that can generate explanations for `why' and `why not' questions. Our starting point is the observation that causal models can generate opportunity chains that take the form of `A enables B and B causes C'. Using insights from an analysis of 240 explanations generated in a human-agent experiment, we define a distal explanation model that can analyse counterfactuals and opportunity chains using decision trees and causal models. A recurrent neural network is employed to learn opportunity chains, and decision trees are used to improve the accuracy of task prediction and the generated counterfactuals. We computationally evaluate the model in 6 reinforcement learning benchmarks using different reinforcement learning algorithms. From a study with 90 human participants, we show that our distal explanation model results in improved outcomes over three scenarios compared with two baseline explanation models.}
}

@inproceedings{NEURIPS2018_df0aab05,
 author = {Marom, Ofir and Rosman, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2018/file/df0aab058ce179e4f7ab135ed4e641a9-Paper.pdf},
 volume = {31},
 year = {2018}
}


% there is no doi for the following paper
@inproceedings{MandelEtAl:2017ActionsInHITL,
  title={Where to Add Actions in Human-in-the-Loop Reinforcement Learning},
  author={Mandel, Travis and Liu, Yun-En and Brunskill, Emma and Popovi{\'c}, Zoran},
  booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
  pages = {2322–-2328},
  numpages = {7},
  location = {San Francisco, California, USA},
  series = {AAAI'17},
  year={2017},
  abstract={In order for reinforcement learning systems to learn quickly in vast action spaces such as the space of all possible pieces of text or the space of all images, leveraging human intuition and creativity is key. However, a human-designed action space is likely to be initially imperfect and limited; furthermore, humans may improve at creating useful actions with practice or new information. Therefore, we propose a framework in which a human adds actions to a reinforcement learning system over time to boost performance. In this setting, however, it is key that we use human effort as efficiently as possible, and one significant danger is that humans waste effort adding actions at places (states) that aren't very important. Therefore, we propose Expected Local Improvement (ELI), an automated method which selects states at which to query humans for a new action. We evaluate ELI on a variety of simulated domains adapted from the literature, including domains with over a million actions and domains where the simulated experts change over time. We find ELI demonstrates excellent empirical performance, even in settings where the synthetic "experts" are quite poor.}
}

@misc{Marletto:2021:ScienceCanCant,
  title={The Science of Can and Can’t: A Physicist’s Journey through the Land of Counterfactuals},
  author={Marletto, C},
  year={2021},
  publisher={Vintage Books: New York, NY, USA}
}

@inproceedings{martinez2016learning,
  title={Learning Relational Dynamics of Stochastic Domains for Planning},
  author={Mart{\'\i}nez, David and Alenya, Guillem and Torras, Carme and Ribeiro, Tony and Inoue, Katsumi},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={26},
  pages={235--243},
  year={2016}
}

@article{martinez2017relational,
  title={Relational reinforcement learning with guided demonstrations},
  author={Mart{\'\i}nez, David and Alenya, Guillem and Torras, Carme},
  journal={Artificial Intelligence},
  volume={247},
  pages={295--312},
  year={2017},
  note = {Special Issue on AI and Robotics},
  publisher={Elsevier},
  abstract = {Model-based reinforcement learning is a powerful paradigm for learning tasks in robotics. However, in-depth exploration is usually required and the actions have to be known in advance. Thus, we propose a novel algorithm that integrates the option of requesting teacher demonstrations to learn new domains with fewer action executions and no previous knowledge. Demonstrations allow new actions to be learned and they greatly reduce the amount of exploration required, but they are only requested when they are expected to yield a significant improvement because the teacher's time is considered to be more valuable than the robot's time. Moreover, selecting the appropriate action to demonstrate is not an easy task, and thus some guidance is provided to the teacher. The rule-based model is analyzed to determine the parts of the state that may be incomplete, and to provide the teacher with a set of possible problems for which a demonstration is needed. Rule analysis is also used to find better alternative models and to complete subgoals before requesting help, thereby minimizing the number of requested demonstrations. These improvements were demonstrated in a set of experiments, which included domains from the international planning competition and a robotic task. Adding teacher demonstrations and rule analysis reduced the amount of exploration required by up to 60\% in some domains, and improved the success ratio by 35\% in other domains.},
  doi = {10.1016/j.artint.2015.02.006}
}

@article{MatthewsonPilarski:2022:DesigningAndEvaluatingHCIML,
  title={A Brief Guide to Designing and Evaluating Human-Centered Interactive Machine Learning}, 
  author={Kory W. Mathewson and Patrick M. Pilarski},
  year={2022},
  journal={arXiv preprint arXiv:2204.09622},
  abstract = {Interactive machine learning (IML) is a field of research that explores how to leverage both human and computational abilities in decision making systems. IML represents a collaboration between multiple complementary human and machine intelligent systems working as a team, each with their own unique abilities and limitations. This teamwork might mean that both systems take actions at the same time, or in sequence. Two major open research questions in the field of IML are: "How should we design systems that can learn to make better decisions over time with human interaction?" and "How should we evaluate the design and deployment of such systems?" A lack of appropriate consideration for the humans involved can lead to problematic system behaviour, and issues of fairness, accountability, and transparency. Thus, our goal with this work is to present a human-centred guide to designing and evaluating IML systems while mitigating risks. This guide is intended to be used by machine learning practitioners who are responsible for the health, safety, and well-being of interacting humans. An obligation of responsibility for public interaction means acting with integrity, honesty, fairness, and abiding by applicable legal statutes. With these values and principles in mind, we as a machine learning research community can better achieve goals of augmenting human skills and abilities. This practical guide therefore aims to support many of the responsible decisions necessary throughout the iterative design, development, and dissemination of IML systems. }
}

@article{Mehrabi:2021:SurveyBiasFairness,
  title={A Survey on Bias and Fairness in Machine Learning},
  author={Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={6},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA},
  abstract={With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  doi = {10.1145/3457607}
}

@article{milani2022survey,
  title={A Survey of Explainable Reinforcement Learning},
  author={Milani, Stephanie and Topin, Nicholay and Veloso, Manuela and Fang, Fei},
  journal={arXiv preprint arXiv:2202.08434},
  publisher = {arXiv},
  year={2022}
}

@article{Miller:2019:xAISocialSciencesInsights,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  author={Miller, Tim},
  journal={Artificial intelligence},
  volume={267},
  pages={1--38},
  year={2019},
  publisher={Elsevier},
  doi={10.1016/j.artint.2018.07.007},
  abstract={There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.}
}

@article{Mnih:2013:PlayingAtariDeepRL,
  title={Playing Atari with Deep Reinforcement Learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013},
  publisher = {arXiv},
  abstract={We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.}
}

@article{MuratoreEtAl:2021:RobotLearningReview,
  title={Robot Learning from Randomized Simulations: A Review},
  author={Muratore, Fabio and Ramos, Fabio and Turk, Greg and Yu, Wenhao and Gienger, Michael and Peters, Jan},
  journal={arXiv preprint arXiv:2111.00956},
  publisher = {arXiv},
  year={2021},
  abstract={The rise of deep learning has caused a paradigm shift in robotics research, favoring methods that require large amounts of data. It is prohibitively expensive to generate such data sets on a physical platform. Therefore, state-of-the-art approaches learn in simulation where data generation is fast as well as inexpensive and subsequently transfer the knowledge to the real robot (sim-to-real). Despite becoming increasingly realistic, all simulators are by construction based on models, hence inevitably imperfect. This raises the question of how simulators can be modified to facilitate learning robot control policies and overcome the mismatch between simulation and reality, often called the 'reality gap'. We provide a comprehensive review of sim-to-real research for robotics, focusing on a technique named 'domain randomization' which is a method for learning from randomized simulations. }
}

%%% NNN %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{nair2018overcoming,
  title={Overcoming Exploration in Reinforcement Learning with Demonstrations},
  author={Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={6292--6299},
  year={2018},
  organization={IEEE},
  abstract={Leveraging demonstrations in Deep RL using auxiliary behavior cloning objective function, handling sub-optimal demonstrations by considering behavior cloning loss where demonstrator policy is better than the actor network policy. The proposed approach shows promise on even harder robotics task like Pushing, Sliding, Pick and place as compared to the tasks considered in previous work},
  doi={10.1109/ICRA.2018.8463162}
}

@article{Najar:2021:RLWithHumanAdvice,
  title={Reinforcement Learning With Human Advice: A Survey},
  author={Najar, Anis and Chetouani, Mohamed},
  journal={Frontiers in Robotics and AI},
  volume={8},
  year={2021},
  publisher={Frontiers Media SA},
  abstract={In this paper, we provide an overview of the existing methods for integrating human advice into a reinforcement learning process. We first propose a taxonomy of the different forms of advice that can be provided to a learning agent. We then describe the methods that can be used for interpreting advice when its meaning is not determined beforehand. Finally, we review different approaches for integrating advice into the learning process.},
  doi={10.3389/frobt.2021.584075}
}

@article{NeftciAverbeck:2019:RLBiologicalSystems,
  title={Reinforcement learning in artificial and biological systems},
  author={Neftci, Emre O and Averbeck, Bruno B},
  journal={Nature Machine Intelligence},
  volume={1},
  number={3},
  pages={133--143},
  year={2019},
  publisher={Nature Publishing Group},
  abstract={There is and has been a fruitful flow of concepts and ideas between studies of learning in biological and artificial systems. Much early work that led to the development of reinforcement learning (RL) algorithms for artificial systems was inspired by learning rules first developed in biology by Bush and Mosteller, and Rescorla and Wagner. More recently, temporal-difference RL, devel- oped for learning in artificial agents, has provided a foundational framework for interpreting the activity of dopamine neurons. In this Review, we describe state-of-the-art work on RL in biological and artificial agents. We focus on points of contact between these disciplines and identify areas where future research can benefit from information flow between these fields. Most work in biological systems has focused on simple learning problems, often embedded in dynamic environments where flexibility and ongoing learning are important, similar to real-world learning problems faced by biological systems. In contrast, most work in artificial agents has focused on learning a single complex problem in a static environment. Moving forward, work in each field will benefit from a flow of ideas that represent the strengths within each discipline.},
  doi={10.1038/s42256-019-0025-4}
}


@article{NieBrunskillWagner:2021:LearningPolicies,
  title={Learning When-to-Treat Policies},
  author={Nie, Xinkun and Brunskill, Emma and Wager, Stefan},
  journal={Journal of the American Statistical Association},
  volume={116},
  number={533},
  pages={392--409},
  year={2021},
  publisher={Taylor \& Francis},
  abstract={Many applied decision-making problems have a dynamic component: The policymaker needs not only to choose whom to treat, but also when to start which treatment. For example, a medical doctor may choose between postponing treatment (watchful waiting) and prescribing one of several available treatments during the many visits from a patient. We develop an “advantage doubly robust” estimator for learning such dynamic treatment rules using observational data under the assumption of sequential ignorability. We prove welfare regret bounds that generalize results for doubly robust learning in the single-step setting, and show promising empirical performance in several different contexts. Our approach is practical for policy optimization, and does not need any structural (e.g., Markovian) assumptions.},
  doi={10.1080/01621459.2020.1831925}
}

@inproceedings{ng:99,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
  booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
  publisher = {Morgan Kaufmann Publishers Inc.},
  volume={99},
  numpages = {10},
  series = {ICML '99},
  pages={278--287},
  year={1999},
  abstract ={This paper investigates conditions under which modifications to the reward function of a Markov decision process preserve the optimal policy. It is shown that, besides the positive linear transformation familiar from utility theory, one can add a reward for transitions between states that is expressible as the difference in value of an arbitrary potential function applied to those states. Furthermore, this is shown to be a necessary condition for invariance, in the sense that any other transformation may yield suboptimal policies unless further assumptions are made about the underlying MDP. These results shed light on the practice of reward shaping, a method used in reinforcement learning whereby additional training rewards are used to guide the learning agent. In particular, some well-known "bugs" in reward shaping procedures are shown to arise from non-potential-based rewards, and methods are given for constructing shaping potentials corresponding to distance-based and subgoal-based heuristics. We show that such potentials can lead to substantial reductions in learning time.}
}

@inproceedings{nguyen2019review,
  title={Review of Deep Reinforcement Learning for Robot Manipulation},
  author={Nguyen, Hai and La, Hung},
  booktitle={2019 Third IEEE International Conference on Robotic Computing (IRC)},
  pages={590--595},
  year={2019},
  publisher={IEEE},
  doi={10.1109/IRC.2019.00120}
}
%%% PPP %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{PapallasEtAl:2020:OnlineReplanningTrajectories,
  title={Online Replanning with Human-in-The-Loop for Non-Prehensile Manipulation in Clutter—A Trajectory Optimization based Approach},
  author={Papallas, Rafael and Cohn, Anthony G and Dogar, Mehmet R},
  journal={IEEE Robotics and Automation Letters},
  volume={5},
  number={4},
  pages={5377--5384},
  year={2020},
  publisher={IEEE},
  abstract={We are interested in the problem where a number of robots, in parallel, are trying to solve reaching through clutter problems in a simulated warehouse setting. In such a setting, we investigate the performance increase that can be achieved by using a human-in-the-loop providing guidance to robot planners. These manipulation problems are challenging for autonomous planners as they have to search for a solution in a high-dimensional space. In addition, physics simulators suffer from the uncertainty problem where a valid trajectory in simulation can be invalid when executing the trajectory in the real-world. To tackle these problems, we propose an online-replanning method with a human-in-the-loop. This system enables a robot to plan and execute a trajectory autonomously, but also to seek high-level suggestions from a human operator if required at any point during execution. This method aims to minimize the human effort required, thereby increasing the number of robots that can be guided in parallel by a single human operator. We performed experiments in simulation and on a real robot, using an experienced and a novice operator. Our results show a significant increase in performance when using our approach in a simulated warehouse scenario and six robots.},
  doi={10.1109/LRA.2020.3006826}
}

@book{Pearl:2009:Causality,
   year = {2009},
   author = {Pearl, Judea},
   title = {Causality: Models, Reasoning, and Inference (2nd Edition)},
   publisher = {Cambridge University Press},
   address = {Cambridge},
   abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 3,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.}
}

@article{Pearl:2018:CausalCounterfactualInference,
  title={Causal and counterfactual inference},
  author={Pearl, Judea},
  journal={The Handbook of Rationality},
  pages={1--41},
  year={2018},
  abstract={All accounts of rationality presuppose knowledge of how actions affect the state of the world and how the world would change had alternative actions been taken. The paper presents a framework called Structural Causal Model (SCM) which operationalizes this knowledge and explicates how it can be derived from both theories and data. In particular, we show how counterfactuals are computed and how they can be embedded in a calculus that solves critical problems in the empirical sciences}
}

@book{Pearl:2018:TheBookOfWhy,
  title={The book of why: the new science of cause and effect},
  author={Pearl, Judea and Mackenzie, Dana},
  year={2018},
  publisher={Basic books}
}

% TODO: is this an article or book?
@article{Pearl:2000:ModelsReasoningInference,
  title={Models, reasoning and inference},
  author={Pearl, Judea and others},
  journal={Cambridge, UK: CambridgeUniversityPress},
  volume={19},
  pages={2},
  year={2000}
}

% TODO: why is a youtube link included?
@inproceedings{PengEtAl:2016:AdaptingAgentSpeed,  
  author = {Peng, Bei and MacGlashan, James and Loftin, Robert and Littman, Michael L. and Roberts, David L. and Taylor, Matthew E.},  
  title = {{A Need for Speed: Adapting Agent Action Speed to Improve Task Learning from Non-Expert Humans}},  
  booktitle = {{Proceedings of the 2016 International Conference on Autonomous Agents and Multiagent Systems ( {AAMAS} )}},  
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  year = {2016},
  pages = {957–965},
  video = {[https://www.youtube.com/watch?v=AJQSGD_XPrk](https://www.youtube.com/watch?v=AJQSGD_XPrk)},  
  month_numeric = {5}  
}

@inproceedings{PenkovR19,
  author    = {Svetlin Penkov and
               Subramanian Ramamoorthy},
  title     = {Learning Programmatically Structured Representations with Perceptor Gradients},
  booktitle = {Proceedings of the 7th International Conference on Learning Representations},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=SJggZnRcFQ},
  timestamp = {Thu, 25 Jul 2019 14:25:40 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/PenkovR19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Pfeifer:2021:NetworkModuleDetection,
  title={Network Module Detection from Multi-Modal Node Features with a Greedy Decision Forest for Actionable Explainable AI},
  author={Pfeifer, Bastian and Saranti, Anna and Holzinger, Andreas},
  journal={arXiv preprint arXiv:2108.11674},
  year={2021},
  abstract={Network-based algorithms are used in most domains of research and industry in a wide variety of applications and are of great practical use. In this work, we demonstrate subnetwork detection based on multi-modal node features using a new Greedy Decision Forest for better interpretability. The latter will be a crucial factor in retaining experts and gaining their trust in such algorithms in the future. To demonstrate a concrete application example, we focus in this paper on bioinformatics and systems biology with a special focus on biomedicine. However, our methodological approach is applicable in many other domains as well. Systems biology serves as a very good example of a field in which statistical data-driven machine learning enables the analysis of large amounts of multi-modal biomedical data. This is important to reach the future goal of precision medicine, where the complexity of patients is modeled on a system level to best tailor medical decisions, health practices and therapies to the individual patient. Our glass-box approach could help to uncover disease-causing network modules from multi-omics data to better understand diseases such as cancer.}
}

@article{popova2018deep,
  title={Deep reinforcement learning for de novo drug design},
  author={Popova, Mariya and Isayev, Olexandr and Tropsha, Alexander},
  journal={Science advances},
  volume={4},
  number={7},
  pages={eaap7885},
  year={2018},
  publisher={American Association for the Advancement of Science},
  abstract = {We introduce an artificial intelligence approach to de novo design of molecules with desired physical or biological properties. We have devised and implemented a novel computational strategy for de novo design of molecules with desired properties termed ReLeaSE (Reinforcement Learning for Structural Evolution). On the basis of deep and reinforcement learning (RL) approaches, ReLeaSE integrates two deep neural networks—generative and predictive—that are trained separately but are used jointly to generate novel targeted chemical libraries. ReLeaSE uses simple representation of molecules by their simplified molecular-input line-entry system (SMILES) strings only. Generative models are trained with a stack-augmented memory network to produce chemically feasible SMILES strings, and predictive models are derived to forecast the desired properties of the de novo–generated compounds. In the first phase of the method, generative and predictive models are trained separately with a supervised learning algorithm. In the second phase, both models are trained jointly with the RL approach to bias the generation of new chemical structures toward those with the desired physical and/or biological properties. In the proof-of-concept study, we have used the ReLeaSE method to design chemical libraries with a bias toward structural complexity or toward compounds with maximal, minimal, or specific range of physical properties, such as melting point or hydrophobicity, or toward compounds with inhibitory activity against Janus protein kinase 2. The approach proposed herein can find a general use for generating targeted chemical libraries of novel compounds optimized for either a single desired property or multiple properties.},
  doi = {10.1126/sciadv.aap7885}
}

@article{PujolDustdar:2021:FogRobotics,
  title={Fog Robotics—Understanding the Research Challenges},
  author={Pujol, Victor Casamayor and Dustdar, Schahram},
  journal={IEEE Internet Computing},
  volume={25},
  number={5},
  pages={10--17},
  year={2021},
  publisher={IEEE},
  abstract={Fog robotics is an emerging topic that derives from cloud robotics, but similarly as fog computing, the applications require low latency connections in order to be useful in real life environments. This article presents a new perspective to the topic in which not only robotics takes advantage of the fog computing paradigm, but fog computing is able to leverage the robotics technology in order to enhance its features. This work highlights the benefits obtained by each technology when it is mixed with the other and sketches the relevant topics to research in order to make this partnership possible.},
  doi={10.1109/MIC.2021.3060963}
}

@inproceedings{PuiuttaVeith:2020:xAIRLSurvey,
  title={Explainable Reinforcement Learning: A Survey},
  author={Puiutta, Erika and Veith, Eric MSP},
  booktitle={International Cross-Domain Conference for Machine Learning and Knowledge Extraction},
  pages={77--95},
  year={2020},
  organization={Springer},
  abstract={Explainable Artificial Intelligence (XAI), i.e., the development of more transparent and interpretable AI models, has gained increased traction over the last few years. This is due to the fact that, in conjunction with their growth into powerful and ubiquitous tools, AI models exhibit one detrimental characteristic: a performance-transparency trade-off. This describes the fact that the more complex a model’s inner workings, the less clear it is how its predictions or decisions were achieved. But, especially considering Machine Learning (ML) methods like Reinforcement Learning (RL) where the system learns autonomously, the necessity to understand the underlying reasoning for their decisions becomes apparent. Since, to the best of our knowledge, there exists no single work offering an overview of Explainable Reinforcement Learning (XRL) methods, this survey attempts to address this gap. We give a short summary of the problem, a definition of important terms, and offer a classification and assessment of current XRL methods. We found that a) the majority of XRL methods function by mimicking and simplifying a complex model instead of designing an inherently simple one, and b) XRL (and XAI) methods often neglect to consider the human side of the equation, not taking into account research from related fields like psychology or philosophy. Thus, an interdisciplinary effort is needed to adapt the generated explanations to a (non-expert) human user in order to effectively progress in the field of XRL and XAI in general.},
  doi={doi.org/10.1007/978-3-030-57321-8_5}
}

@book{Pumperla:2019:DeepLearningGameOfGo,
  title={Deep learning and the game of Go},
  author={Pumperla, Max and Ferguson, Kevin},
  volume={231},
  year={2019},
  publisher={Manning Publications Company},
}

%%% RRR %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{Ribeiro:2016:WhyShouldITrustYou,
  title={"Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  publisher = {Association for Computing Machinery},
  series = {KDD '16},
  pages={1135--1144},
  year={2016},
  abstract={Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  doi = {10.1145/2939672.2939778}
}

@article{RodriguezEtAl:2021:DeepCovidxAI,
  title={DeepCOVID: An Operational Deep Learning-driven Framework for Explainable Real-time COVID-19 Forecasting},
  author={Rodriguez, Alexander and Tabassum, Anika and Cui, Jiaming and Xie, Jiajia and Ho, Javen and Agarwal, Pulak and Adhikari, Bijaya and Prakash, B Aditya},
  journal={medRxiv},
  pages={2020--09},
  year={2021},
  publisher={Cold Spring Harbor Laboratory Press},
  abstract={How do we forecast an emerging pandemic in real time in a purely data-driven manner? How to leverage rich heterogeneous data based on various signals such as mobility, testing, and/or disease exposure for forecasting? How to handle noisy data and generate uncertainties in the forecast? In this paper, we present DEEPCOVID, an operational deep learning framework designed for real-time COVID-19 forecasting. DEEP-COVID works well with sparse data and can handle noisy heterogeneous data signals by propagating the uncertainty from the data in a principled manner resulting in meaningful uncertainties in the forecast. The deployed framework also consists of modules for both real-time and retrospective exploratory analysis to enable interpretation of the forecasts. Results from real-time predictions (featured on the CDC website and FiveThirtyEight.com) since April 2020 indicates that our approach is competitive among the methods in the COVID-19 Forecast Hub, especially for short-term predictions.},
  doi={10.1101/2020.09.28.20203109}
}


@article{RoyEtAl:2021:RLRoboticsChallenges,
  title={From Machine Learning to Robotics: Challenges and Opportunities for Embodied Intelligence},
  author={Roy, Nicholas and Posner, Ingmar and Barfoot, Tim and Beaudoin, Philippe and Bengio, Yoshua and Bohg, Jeannette and Brock, Oliver and Depatie, Isabelle and Fox, Dieter and Koditschek, Dan and others},
  journal={arXiv preprint arXiv:2110.15245},
  publisher = {arXiv},
  year={2021},
  abstract={Machine learning has long since become a keystone technology, accelerating science and applications in a broad range of domains. Consequently, the notion of applying learning methods to a particular problem set has become an established and valuable modus operandi to advance a particular field. In this article we argue that such an approach does not straightforwardly extended to robotics — or to embodied intelligence more generally: systems which engage in a purposeful exchange of energy and information with a physical environment. In particular, the purview of embodied intelligent agents extends significantly beyond the typical considerations of main-stream machine learning approaches, which typically (i) do not consider operation under conditions significantly different from those encountered during training; (ii) do not consider the often substantial, long-lasting and potentially safety-critical nature of interactions during learning and deployment; (iii) do not require ready adaptation to novel tasks while at the same time (iv) effectively and efficiently curating and extending their models of the world through targeted and deliberate actions. In reality, therefore, these limitations result in learning-based systems which suffer from many of the same operational shortcomings as more traditional, engineering-based approaches when deployed on a robot outside a well defined, and often narrow operating envelope. Contrary to viewing embodied intelligence as another application domain for machine learning, here we argue that it is in fact a key driver for the advancement of machine learning technology. In this article our goal is to highlight challenges and opportunities that are specific to embodied intelligence and to propose research directions which may significantly advance the state-of-the-art in robot learning.}
}

@inproceedings{RuanEtAl:2019:Quizbot,
  title={QuizBot: A Dialogue-based Adaptive Learning System for Factual Knowledge},
  author={Ruan, Sherry and Jiang, Liwei and Xu, Justin and Tham, Bryce Joe-Kun and Qiu, Zhengneng and Zhu, Yeshuang and Murnane, Elizabeth L and Brunskill, Emma and Landay, James A},
  booktitle={Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages={1--13},
  year={2019},
  abstract={
  Advances in conversational AI have the potential to enable more engaging and effective ways to teach factual knowledge. To investigate this hypothesis, we created QuizBot, a dialogue-based agent that helps students learn factual knowledge in science, safety, and English vocabulary. We evaluated QuizBot with 76 students through two within-subject studies against a flashcard app, the traditional medium for learning factual knowledge. Though both systems used the same algorithm for sequencing materials, QuizBot led to students recognizing (and recalling) over 20\% more correct answers than when students used the flashcard app. Using a conversational agent is more time consuming to practice with, but in a second study, of their own volition, students spent 2.6x more time learning with QuizBot than with flashcards and reported preferring it strongly for casual learning. Our results in this second study showed QuizBot yielded improved learning gains over flashcards on recall. These results suggest that educational chatbot systems may have beneficial use, particularly for learning outside of traditional settings.},
  doi={10.1145/3290605.3300587}
}

%%% SSS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{SalviniEtAl:2021:ISO13482:2014,
  author = {Salvini, Pericle and Paez-Granados, Diego and Billard, Aude},
  title = {On the Safety of Mobile Robots Serving in Public Spaces: Identifying Gaps in EN ISO 13482:2014 and Calling for a New Standard},
  year = {2021},
  journal = {Transactions on Human-Robot Interaction},
  issue_date = {September 2021},
  publisher = {Association for Computing Machinery},
  volume = {10},
  number = {3},
  numpages = {27},
  abstract = {Since 2014, a specific standard has been dedicated for the safety certification of personal care robots, which operate in close proximity to humans. These robots serve as information providers, object transporters, personal mobility carriers, and security patrollers. In this article, we point out the shortcomings concerning EN ISO 13482:2014, which encompasses guidelines regarding the safety and design of personal care robots. In particular, we argue that the current standard is not suitable for guaranteeing people's safety when these robots operate in public spaces. Specifically, the standard lacks requirements to protect pedestrians and bystanders. The guideline implicitly assumes that private spaces, such as households and offices, present the same hazards as in public spaces. We highlight the existence of at least three properties pertaining to robots’ use in public spaces. These properties include (1) crowds, (2) social norms and proxemics rules, and (3) people's misbehaviours. We discuss how these properties impact robots’ safety. This article aims to raise stakeholders’ awareness on individuals’ safety when robots are deployed in public spaces. This could be achieved by integrating the gaps present in EN ISO 13482:2014 or by creating a new dedicated standard.},
  doi = {10.1145/3442678}
}

@inproceedings{Saranti:2019:LearningCompetencePGMs,
  title={Insights into Learning Competence Through Probabilistic Graphical Models},
  author={Saranti, Anna and Taraghi, Behnam and Ebner, Martin and Holzinger, Andreas},
  booktitle={International cross-domain conference for machine learning and knowledge extraction},
  editor={Andreas Holzinger and Peter Kieseberg and A Min Tjoa and Edgar Weippl},
  pages={250--271},
  year={2019},
  organization={Springer},
  publisher={Springer},
  abstract={One-digit multiplication problems is one of the major fields in learning mathematics at the level of primary school that has been studied over and over. However, the majority of related work is focusing on descriptive statistics on data from multiple surveys. The goal of our research is to gain insights into multiplication misconceptions by applying machine learning techniques. To reach this goal, we trained a probabilistic graphical model of the students’ misconceptions from data of an application for learning multiplication. The use of this model facilitates the exploration of insights into human learning competence and the personalization of tutoring according to individual learner’s knowledge states. The detection of all relevant causal factors of the erroneous students answers as well as their corresponding relative weight is a valuable insight for teachers. Furthermore, the similarity between different multiplication problems - according to the students behavior - is quantified and used for their grouping into clusters. Overall, the proposed model facilitates real-time learning insights that lead to more informed decisions.},
  doi={10.1007/978-3-030-29726-8_16}
}

@incollection{Saranti:2009,
  title={Quantum Harmonic Oscillator Sonification},
  author={Saranti, Anna and Eckel, Gerhard and Pirr{\'o}, David},
  booktitle={Auditory Display},
  editor={Sølvi Ystad and Mitsuko Aramaki and Richard Kronland-Martinet and Kristoffer Jensen},
  pages={184--201},
  year={2009},
  publisher={Springer},
  abstract={This work deals with the sonification of a quantum mechanical system and the processes that occur as a result of its quantum mechanical nature and interactions with other systems. The quantum harmonic oscillator is not only regarded as a system with sonifiable characteristics but also as a storage medium for quantum information. By representing sound information quantum mechanically and storing it in the system, every process that unfolds on this level is inherited and reflected by the sound. The main profit of this approach is that the sonification can be used as a first insight for two models: a quantum mechanical system model and a quantum computation model.},
  doi={10.1007/978-3-642-12439-6_10}
}


@article{SchnakeMontavon:2020:XAIgraphs,
   year = {2020},
   author = {Schnake, Thomas and Eberle, Oliver and Lederer, Jonas and Nakajima, Shinichi and Schütt, Kristof T. and Müller, Klaus-Robert and Montavon, Grégoire},
   title = {XAI for Graphs: Explaining Graph Neural Network Predictions by Identifying Relevant Walks},
   journal = {arXiv preprint arXiv:2006.03589v2},
   abstract = {Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph into the neural network structure, common explainable AI (XAI) approaches are not applicable. To a large extent, GNNs have remained black-boxes for the user so far. In this paper, we contribute by proposing a new XAI approach for GNNs. Our approach is derived from high-order Taylor expansions and is able to generate a decomposition of the GNN prediction as a collection of relevant walks on the input graph. We find that these high-order Taylor expansions can be equivalently (and more simply) computed using multiple backpropagation passes from the top layer of the GNN to the first layer. The explanation can then be further robustified and generalized by using layer-wise-relevance propagation (LRP) in place of the standard equations for gradient propagation. Our novel method which we denote as `GNN-LRP' is tested on scale-free graphs, sentence parsing trees, molecular graphs, and pixel lattices representing images. In each case, it performs stably and accurately, and delivers interesting and novel application insights.}
}

@incollection{Schneeberger:2020:legalAI,
   year = {2020},
   author = {Schneeberger, David and Stoeger, Karl and Holzinger, Andreas},
   title = {The European Legal Framework for Medical AI},
   booktitle = {International Cross-Domain Conference for Machine Learning and Knowledge Extraction, Springer LNCS 12279},
   publisher = {Springer},
   pages = {209--226},
   doi = {10.1007/978-3-030-57321-8_12}
}

@article{Scurto:2021:DesigningDeepRLHumanParameterExploration,
  title={Designing Deep Reinforcement Learning for Human Parameter Exploration},
  author={Scurto, Hugo and Kerrebroeck, Bavo Van and Caramiaux, Baptiste and Bevilacqua, Fr{\'e}d{\'e}ric},
  journal={ACM Transactions on Computer-Human Interaction (TOCHI)},
  volume={28},
  number={1},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA},
  abstract={Software tools for generating digital sound often present users with high-dimensional, parametric interfaces, that may not facilitate exploration of diverse sound designs. In this article, we propose to investigate artificial agents using deep reinforcement learning to explore parameter spaces in partnership with users for sound design. We describe a series of user-centred studies to probe the creative benefits of these agents and adapting their design to exploration. Preliminary studies observing users’ exploration strategies with parametric interfaces and testing different agent exploration behaviours led to the design of a fully-functioning prototype, called Co-Explorer, that we evaluated in a workshop with professional sound designers. We found that the Co-Explorer enables a novel creative workflow centred on human–machine partnership, which has been positively received by practitioners. We also highlight varied user exploration behaviours throughout partnering with our system. Finally, we frame design guidelines for enabling such co-exploration workflow in creative digital applications.},
  doi = {10.1145/3414472}
}

@book{ShneidermanEtAl:2016:GoldenRulesHCI,
  title={Designing the user interface: strategies for effective human-computer interaction},
  author={Shneiderman, Ben and Plaisant, Catherine and Cohen, Maxine S and Jacobs, Steven and Elmqvist, Niklas and Diakopoulos, Nicholas},
  year={2016},
  publisher={Pearson}
}

@article{ShteingartLoewenstein:2014:RLHumanBehavior,
  title = {Reinforcement learning and human behavior},
  journal = {Current Opinion in Neurobiology},
  volume = {25},
  pages = {93--98},
  year = {2014},
  note = {Theoretical and computational neuroscience},
  issn = {0959-4388},
  author = {Hanan Shteingart and Yonatan Loewenstein},
  abstract = {The dominant computational approach to model operant learning and its underlying neural activity is model-free reinforcement learning (RL). However, there is accumulating behavioral and neuronal-related evidence that human (and animal) operant learning is far more multifaceted. Theoretical advances in RL, such as hierarchical and model-based RL extend the explanatory power of RL to account for some of these findings. Nevertheless, some other aspects of human behavior remain inexplicable even in the simplest tasks. Here we review developments and remaining challenges in relating RL models to human operant learning. In particular, we emphasize that learning a model of the world is an essential step before or in parallel to learning the policy in RL and discuss alternative models that directly learn a policy without an explicit world model in terms of state-action pairs.},
  doi = {10.1016/j.conb.2013.12.004}
}

@article{ShuXiongSocher:2017:HierarchicalTaskExplanation,
  title={Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning},
  author={Shu, Tianmin and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1712.07294},
  year={2017},
  publisher = {arXiv},
  abstract={Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.}
}

@inproceedings{silva2020optimization,
  title={Optimization Methods for Interpretable Differentiable Decision Trees Applied to Reinforcement Learning},
  author={Silva, Andrew and Gombolay, Matthew and Killian, Taylor and Jimenez, Ivan and Son, Sung-Hyun},
  booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  editor = {Chiappa, Silvia and Calandra, Roberto},
  pages={1855--1865},
  year={2020},
  organization={PMLR},
  abstract = { Decision trees are ubiquitous in machine learning for their ease of use and interpretability. Yet, these models are not typically employed in reinforcement learning as they cannot be updated online via stochastic gradient descent. We overcome this limitation by allowing for a gradient update over the entire tree that improves sample complexity affords interpretable policy extraction. First, we include theoretical motivation on the need for policy-gradient learning by examining the properties of gradient descent over differentiable decision trees. Second, we demonstrate that our approach equals or outperforms a neural network on all domains and can learn discrete decision trees online with average rewards up to 7x higher than a batch-trained decision tree. Third, we conduct a user study to quantify the interpretability of a decision tree, rule list, and a neural network with statistically significant results (p &lt; 0.001).}
}

@article{Silver:2018:AlphaZero,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science},
  abstract={The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
  doi = {10.1126/science.aar6404}
}

@article{SongEtAl:2019:ExplainableGraphBasedRecommendations,
  author    = {Weiping Song and
               Zhijian Duan and
               Ziqing Yang and
               Hao Zhu and
               Ming Zhang and
               Jian Tang},
  title     = {Explainable Knowledge Graph-based Recommendation via Deep Reinforcement
               Learning},
  journal   = {arXiv preprint arXiv:1906.09506},
  publisher = {arXiv},
  year      = {2019},
  abstract = {This paper studies recommender systems with knowledge graphs, which can effectively address the problems of data sparsity and cold start. Recently, a variety of methods have been developed for this problem, which generally try to learn effective representations of users and items and then match items to users according to their representations. Though these methods have been shown quite effective, they lack good explanations, which are critical to recommender systems. In this paper, we take a different route and propose generating recommendations by finding meaningful paths from users to items. Specifically, we formulate the problem as a sequential decision process, where the target user is defined as the initial state, and the edges on the graphs are defined as actions. We shape the rewards according to existing state-of-the-art methods and then train a policy function with policy gradient methods. Experimental results on three real-world datasets show that our proposed method not only provides effective recommendations but also offers good explanations}
}


@article{SrinivasLaskinAbbeel:2020:ContrastiveUnsupervisedRL,
  title={CURL: Contrastive Unsupervised Representations for Reinforcement Learning},
  author={Srinivas, Aravind and Laskin, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2004.04136},
  publisher = {arXiv},
  year={2020},
  abstract={We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features}
}

@article{Stoeger:2021:MedicalAI,
  year = {2021},
  author = {Stoeger, Karl and Schneeberger, David and Holzinger, Andreas},
  title = {Medical Artificial Intelligence: The European Legal Perspective},
  journal = {Communications of the ACM},
  volume = {64},
  number = {11},
  pages = {34--36},
  abstract = {Although the European Commission proposed new legislation for the use of "high-risk artificial intelligence" earlier this year, the existing European fundamental rights framework already provides some clear guidance on the use of medical AI.},
  doi = {10.1145/3458652}
}

@inproceedings{SturmEtAl:2015:InteractiveHeatmap,
  year = {2015},
  author = {Sturm, Werner and Schaefer, Till and Schreck, Tobias and Holzinger, Andeas and Ullrich, Torsten},
  title = {Extending the Scaffold Hunter Visualization Toolkit with Interactive Heatmaps},
  booktitle = {EG UK Computer Graphics and Visual Computing CGVC 2015},
  editor = {Borgo, Rita and Turkay, Cagatay},
  publisher = {Euro Graphics (EG)},
  pages = {77--84},
  abstract = {In many application areas, large amounts of data arise, which are often hard to interpret or make use of by humans. Interactive visualization can help to overview and explore large amounts of data. An example is in the life sciences, where databases of chemical compounds need to be analyzed in terms of similarities of molecular properties. Scientists then need to explore this data in an efficient way. The Scaffold Hunter framework is an Open Source software system for interactive visualization of high dimensional data. In this paper, we present an extension of Scaffold Hunter with an interactive heatmap, which ties in tightly with a dendrogram visualization. We added specific interaction modalities and views tailored to the analysis of chemical compounds. Zooming capabilities allow to start from an overview of the data (showing all data elements at once) down to a detail-on-demand view which includes chemical structural views of molecules. We show how the interactive heatmap with clustered rows and columns can bring new insights into the data regarding various properties. The implementation is made available for researchers and practitioners to use. },
  keywords = {interactive visualization, interactive heatmaps, visusal analytics, chemical compounds, chemical molecules},
  doi = {10.2312/cgvc.20151247}
}

@inproceedings{suay:11,
  title={Effect of Human Guidance and State Space Size on Interactive Reinforcement Learning},
  author={Suay, Halit Bener and Chernova, Sonia},
  booktitle={2011 Ro-Man},
  pages={1--6},
  year={2011},
  organization={IEEE},
  abstract ={The Interactive Reinforcement Learning algorithm enables a human user to train a robot by providing rewards in response to past actions and anticipatory guidance to guide the selection of future actions. Past work with soft- ware agents has shown that incorporating user guidance into the policy learning process through Interactive Reinforcement Learning significantly improves the policy learning time by reducing the number of states the agent explores. We present the first study of Interactive Reinforcement Learning in real- world robotic systems. We report on four experiments that study the effects that teacher guidance and state space size have on policy learning performance. We discuss modifications made to apply Interactive Reinforcement Learning to a real- world system and show that guidance significantly reduces the learning rate, and that its positive effects increase with state space size.},
  doi={10.1109/ROMAN.2011.6005223}
}

@inproceedings{sun2019program,
  title={Program guided agent},
  author={Sun, Shao-Hua and Wu, Te-Lin and Lim, Joseph J},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{Sun:2021:TopologyPerturbationGNNs,
  title={Preserve, Promote, or Attack? GNN Explanation via Topology Perturbation},
  author={Sun, Yi and Valente, Abel and Liu, Sijia and Wang, Dakuo},
  journal={arXiv preprint arXiv:2103.13944},
  year={2021},
  abstract={Prior works on formalizing explanations of a graph neural network (GNN) focus on a single use case - to preserve the prediction results through identifying important edges and nodes. In this paper, we develop a multi-purpose interpretation framework by acquiring a mask that indicates topology perturbations of the input graphs. We pack the framework into an interactive visualization system (GNNViz) which can fulfill multiple purposes: Preserve,Promote, or Attack GNN's predictions. We illustrate our approach's novelty and effectiveness with three case studies: First, GNNViz can assist non expert users to easily explore the relationship between graph topology and GNN's decision (Preserve), or to manipulate the prediction (Promote or Attack) for an image classification task on MS-COCO; Second, on the Pokec social network dataset, our framework can uncover unfairness and demographic biases; Lastly, it compares with state-of-the-art GNN explainer baseline on a synthetic dataset.}
}

@book{SuttonBarto:2018:RLIntroduction,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

%%% TTT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{TabrezHayes:2019:xRLTextualExplanations,
  title={Improving Human-Robot Interaction Through Explainable Reinforcement Learning},
  author={Tabrez, Aaquib and Hayes, Bradley},
  booktitle={2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  pages={751--753},
  year={2019},
  organization={IEEE},
  doi={10.1109/HRI.2019.8673198},
  abstract={Gathering the most informative data from humans without overloading them remains an active research area in AI, and is closely coupled with the problems of determining how and when information should be communicated to others [12]. Current decision support systems (DSS) are still overly simple and static, and cannot adapt to changing environments we expect to deploy in modern systems [3], [4], [9], [11]. They are intrinsically limited in their ability to explain rationale versus merely listing their future behaviors, limiting a human's understanding of the system [2], [7]. Most probabilistic assessments of a task are conveyed after the task/skill is attempted rather than before [10], [14], [16]. This limits failure recovery and danger avoidance mechanisms. Existing work on predicting failures relies on sensors to accurately detect explicitly annotated and learned failure modes [13]. As such, important non-obvious pieces of information for assessing appropriate trust and/or course-of-action (COA) evaluation in collaborative scenarios can go overlooked, while irrelevant information may instead be provided that increases clutter and mental workload. Understanding how AI models arrive at specific decisions is a key principle of trust [8]. Therefore, it is critically important to develop new strategies for anticipating, communicating, and explaining justifications and rationale for AI driven behaviors via contextually appropriate semantics.}
}

@article{taylor2009transfer,
  title={Transfer Learning for Reinforcement Learning Domains: A Survey},
  author={Taylor, Matthew E and Stone, Peter},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={7},
  pages = {1633--1685},
  year={2009}
}

@inproceedings{taylor2011integrating,
  title={Integrating Reinforcement Learning with Human Demonstrations of Varying Ability},
  author={Taylor, Matthew E and Suay, Halit Bener and Chernova, Sonia},
  booktitle={The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  address = {Richland, SC},
  series = {AAMAS '11},
  pages={617--624},
  year={2011},
  organization={Citeseer},
  abstract = {This work introduces Human-Agent Transfer (HAT), an algorithm that combines transfer learning, learning from demonstration and reinforcement learning to achieve rapid learning and high performance in complex domains. Using experiments in a simulated robot soccer domain, we show that human demonstrations transferred into a baseline policy for an agent and refined using reinforcement learning significantly improve both learning time and policy performance. Our evaluation compares three algorithmic approaches to incorporating demonstration rule summaries into transfer learning, and studies the impact of demonstration quality and quantity, as well as the effect of combining demonstrations from multiple teachers. Our results show that all three transfer methods lead to statistically significant improvement in performance over learning without demonstration. The best performance was achieved by combining the best demonstrations from two teachers.}
}

@article{TaylorEtAl:2021:InteractiveRLHIPPOGym,
  title={Improving Reinforcement Learning with Human Assistance: An Argument for Human Subject Studies with HIPPO Gym},
  author={Taylor, Matthew E and Nissen, Nicholas and Wang, Yuan and Navidi, Neda},
  journal={arXiv preprint arXiv:2102.02639},
  publisher = {arXiv},
  year={2021},
  abstract={Reinforcement learning (RL) is a popular machine learning paradigm for game playing, robotics control, and other sequential decision tasks. However, RL agents often have long learning times with high data requirements because they begin by acting randomly. In order to better learn in complex tasks, this article argues that an external teacher can often significantly help the RL agent learn. OpenAI Gym is a common framework for RL research, including a large number of standard environments and agents, making RL research significantly more accessible. This article introduces our new open-source RL framework, the Human Input Parsing Platform for Openai Gym (HIPPO Gym), and the design decisions that went into its creation. The goal of this platform is to facilitate human-RL research, again lowering the bar so that more researchers can quickly investigate different ways that human teachers could assist RL agents, including learning from demonstrations, learning from feedback, or curriculum learning.}
}

@inproceedings{Thomaz:2006:RLWithHumanTeachers,
  title={Reinforcement Learning with Human Teachers: Evidence of Feedback and
Guidance with Implications for Learning Performance},
  author={Thomaz, Andrea Lockerd and Breazeal, Cynthia},
  booktitle={Aaai},
  volume={6},
  pages={1000--1005},
  year={2006},
  organization={Boston, MA},
  abstract={While reinforcement learning (RL) is not traditionally designed for interactive supervisory input from a human teacher, several works in both robot and software agents have adapted it for human input by letting a human trainer control the reward signal. In this work, we experimentally examine the assumption underlying these works, namely that the human-given reward is compatible with the traditional RL reward signal. We describe an experimental platform with a simulated RL robot and present an analysis of real-time human teaching behavior found in a study in which untrained subjects taught the robot to perform a new task. We report three main observations on how people administer feedback when teaching a robot a task through reinforcement learning: (a) they use the reward channel not only for feedback, but also for future-directed guidance; (b) they have a positive bias to their feedback -possibly using the signal as a motivational channel; and (c) they change their behavior as they develop a mental model of the robotic learner. In conclusion, we discuss future extensions to RL to accommodate these lessons.}
}

@article{TickleEtAl:1998:HistoryNN,
  author={Tickle, A.B. and Andrews, R. and Golea, M. and Diederich, J.},  journal={IEEE Transactions on Neural Networks},   
  title={The Truth Will Come to Light: Directions and Challenges in Extracting the Knowledge Embedded Within Trained Artificial Neural Networks},   year={1998},  
  volume={9}, 
  number={6},  
  pages={1057--1068},  
  abstract={To date, the preponderance of techniques for eliciting the knowledge embedded in trained artificial neural networks (ANN's) has focused primarily on extracting rule-based explanations from feedforward ANN's. The ADT taxonomy for categorizing such techniques was proposed in 1995 to provide a basis for the systematic comparison of the different approaches. This paper shows that not only is this taxonomy applicable to a cross section of current techniques for extracting rules from trained feedforward ANN's but also how the taxonomy can be adapted and extended to embrace a broader range of ANN types (e,g., recurrent neural networks) and explanation structures. In addition we identify some of the key research questions in extracting the knowledge embedded within ANN's including the need for the formulation of a consistent theoretical basis for what has been, until recently, a disparate collection of empirical results.},  
  doi={10.1109/72.728352},  
  ISSN={1941-0093},  
  month={Nov}
}

@article{TomarEtAl:2021:LearnPixelControlRepresentations,
  title={Learning Representations for Pixel-based Control: What Matters and Why?},
  author={Tomar, Manan and Mishra, Utkarsh A and Zhang, Amy and Taylor, Matthew E},
  journal={arXiv preprint arXiv:2111.07775},
  publisher = {arXiv},
  year={2021},
  abstract={Learning representations for pixel-based control has garnered significant attention recently in reinforcement learning. A wide range of methods have been proposed to enable efficient learning, leading to sample complexities similar to those in the full state setting. However, moving beyond carefully curated pixel data sets (centered crop, appropriate lighting, clear background, etc.) remains challenging. In this paper, we adopt a more difficult setting, incorporating background distractors, as a first step towards addressing this challenge. We present a simple baseline approach that can learn meaningful representations with no metric-based learning, no data augmentations, no world-model learning, and no contrastive learning. We then analyze when and why previously proposed methods are likely to fail or reduce to the same performance as the baseline in this harder setting and why we should think carefully about extending such methods beyond the well curated environments. Our results show that finer categorization of benchmarks on the basis of characteristics like density of reward, planning horizon of the problem, presence of task-irrelevant components, etc., is crucial in evaluating algorithms. Based on these observations, we propose different metrics to consider when evaluating an algorithm on benchmark tasks. We hope such a data-centric view can motivate researchers to rethink representation learning when investigating how to best apply RL to real-world tasks.}
}

@inproceedings{topin2021iterative,
  title={Iterative Bounding MDPs: Learning Interpretable Policies via Non-Interpretable Methods},
  author={Topin, Nicholay and Milani, Stephanie and Fang, Fei and Veloso, Manuela},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={9923--9931},
  year={2021}
}

@inproceedings{toro2019learning,
  title={Learning Reward Machines for Partially Observable Reinforcement Learning},
  author={Icarte, Rodrigo Toro and Waldie, Ethan and Klassen, Toryn and Valenzano, Rick and Castro, Margarita and McIlraith, Sheila},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  volume={32},
  year={2019}
}

@inproceedings{icarte2018using,
  title={Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning},
  author={Icarte, Rodrigo Toro and Klassen, Toryn and Valenzano, Richard and McIlraith, Sheila},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  editor = {Dy, Jennifer and Krause, Andreas},
  volume = {80},
  series = {Proceedings of Machine Learning Research},
  pages={2107--2116},
  year={2018},
  organization={PMLR},
  abstract = {In this paper we propose Reward Machines {—} a type of finite state machine that supports the specification of reward functions while exposing reward function structure to the learner and supporting decomposition. We then present Q-Learning for Reward Machines (QRM), an algorithm which appropriately decomposes the reward machine and uses off-policy q-learning to simultaneously learn subpolicies for the different components. QRM is guaranteed to converge to an optimal policy in the tabular case, in contrast to Hierarchical Reinforcement Learning methods which might converge to suboptimal policies. We demonstrate this behavior experimentally in two discrete domains. We also show how function approximation methods like neural networks can be incorporated into QRM, and that doing so can find better policies more quickly than hierarchical methods in a domain with a continuous state space.}
}

@inproceedings{torrey2013teaching,
  title={Teaching on a Budget: Agents Advising Agents in Reinforcement Learning},
  author={Torrey, Lisa and Taylor, Matthew},
  booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  address = {Richland, SC},
  pages={1053--1060},
  year={2013},
  abstract = {This paper introduces a teacher-student framework for reinforcement learning. In this framework, a teacher agent instructs a student agent by suggesting actions the student should take as it learns. However, the teacher may only give such advice a limited number of times. We present several novel algorithms that teachers can use to budget their advice effectively, and we evaluate them in two experimental domains: Mountain Car and Pac-Man. Our results show that the same amount of advice, given at different moments, can have different effects on student learning, and that teachers can significantly affect student learning even when students use different learning methods and state representations.},
  doi={10.5555/2484920.2485086}
}

%%% VVV %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{vecerik2017leveraging,
  title={Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards},
  author={Vecerik, Mel and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1707.08817},
  publisher = {arXiv},
  year={2017},
  abstract={Leveraging human demonstrations along with an off-policy RL algorithm for continuous actions (DDPG) for robotics task like peg insertion, clip insertion, cable insertion etc. A separate replay buffer for demonstrations is maintained, prioritized replay is used to sample both agent and human demonstrations along with some other techniques}
}

@InProceedings{VermaEtAl:2018:ProgrammaticallyInterpretableRL,
  title = 	 {Programmatically Interpretable Reinforcement Learning},
  author =       {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5045--5054},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v80/verma18a.html},
  abstract = 	 {We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural “oracle”. We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.}
}

@inproceedings{verma2019imitation,
  title={Imitation-Projected Programmatic Reinforcement Learning},
  author={Verma, Abhinav and Le, Hoang and Yue, Yisong and Chaudhuri, Swarat},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  volume={32},
  year={2019},
  publisher = {Curran Associates, Inc.}
}

@article{Vu:2020:PGMExplainer,
  title={PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks},
  author={Vu, Minh N and Thai, My T},
  journal={arXiv preprint arXiv:2010.05788},
  publisher = {arXiv},
  year={2020},
  abstract={In Graph Neural Networks (GNNs), the graph structure is incorporated into the learning of node representations. This complex structure makes explaining GNNs' predictions become much more challenging. In this paper, we propose PGM-Explainer, a Probabilistic Graphical Model (PGM) model-agnostic explainer for GNNs. Given a prediction to be explained, PGM-Explainer identifies crucial graph components and generates an explanation in form of a PGM approximating that prediction. Different from existing explainers for GNNs where the explanations are drawn from a set of linear functions of explained features, PGM-Explainer is able to demonstrate the dependencies of explained features in form of conditional probabilities. Our theoretical analysis shows that the PGM generated by PGM-Explainer includes the Markov-blanket of the target prediction, i.e. including all its statistical information. We also show that the explanation returned by PGM-Explainer contains the same set of independence statements in the perfect map. Our experiments on both synthetic and real-world datasets show that PGM-Explainer achieves better performance than existing explainers in many benchmark tasks.}
}

%%% WWW %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{Walker,
	abstract = {
      Many reinforcement learning domains are highly relational.Whiletraditional temporal-difference methods can be applied to these domains, they are limited in their capacity to exploit the relational nature of the domain. Our algorithm, AMBIL, constructs relational world models in the form of relational Markov decision processes (MDPs). AMBIL works backwards from collections of high-reward states, utilizing inductive logic programming to learn their preimage, logical definitions of the region of state space that leads to the high-reward states via some action. These learned preimages are chained together to form an MDP that abstractly represents the domain. AMBIL estimates the reward and transition probabilities of this MDP from past experience. Since our MDPs are small, AMBIL uses value-iteration to quickly estimate the Q-values of each action in the induced states and determine a policy. AMBIL is able to employ complex background knowledge and supports relational representations. Empirical evaluation on both synthetic domains and a sub-task of the RoboCup soccer domain shows significant performance gains compared to standard Q-learning.
   },
	affiliation = {University of Wisconsin; University of Minnesota},
	author = {Walker, Trevor and Torrey, Lisa and Shavlik, Jude and Maclin, Richard},
	copyright = {Springer-Verlag Berlin Heidelberg},
	doi = {10.1007/978-3-540-78469-2\_27},
	title = {Building Relational World Models for Reinforcement Learning},
}




@book{walsh2010efficient,
  title={Efficient learning of relational models for sequential decision making},
  author={Walsh, Thomas J},
  year={2010},
  publisher={Rutgers The State University of New Jersey-New Brunswick},
  doi={10.7282/T370814H}
}



@inproceedings{Wang:2022:SkillPreferences,
  title={Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback},
  author={Wang, Xiaofei and Lee, Kimin and Hakhamaneshi, Kourosh and Abbeel, Pieter and Laskin, Michael},
  booktitle={Conference on Robot Learning},
  pages={1259--1268},
  year={2022},
  organization={PMLR},
  abstract={A promising approach to solving challenging long-horizon tasks has been to extract behavior priors (skills) by fitting generative models to large offline datasets of demonstrations. However, such generative models inherit the biases of the underlying data and result in poor and unusable skills when trained on imperfect demonstration data. To better align skill extraction with human intent we present Skill Preferences (SkiP), an algorithm that learns a model over human preferences and uses it to extract human-aligned skills from offline data. After extracting human-preferred skills, SkiP also utilizes human feedback to solve downstream tasks with RL. We show that SkiP enables a simulated kitchen robot to solve complex multi-step manipulation tasks and substantially outperforms prior leading RL algorithms with human preferences as well as leading skill extraction algorithms without human preferences.}
}

@article{Warnell:2018:DeepTAMER, 
  title={Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces}, 
  volume={32}, 
  url={https://ojs.aaai.org/index.php/AAAI/article/view/11485}, 
  abstractNote={While recent advances in deep reinforcement learning have allowed autonomous learning agents to succeed at a variety of complex tasks, existing algorithms generally require a lot of training data. One way to increase the speed at which agents are able to learn to perform tasks is by leveraging the input of human trainers. Although such input can take many forms, real-time, scalar-valued feedback is especially useful in situations where it proves difficult or impossible for humans to provide expert demonstrations. Previous approaches have shown the usefulness of human input provided in this fashion (e.g., the TAMER framework), but they have thus far not considered high-dimensional state spaces or employed the use of deep learning. In this paper, we do both: we propose DeepTAMER, an extension of the TAMER framework that leverages the representational power of deep neural networks in order to learn complex tasks in just a short amount of time with a human trainer. We demonstrate Deep TAMER’s success by using it and just 15 minutes of human-provided feedback to train an agent that performs better than humans on the Atari game of Bowling - a task that has proven difficult for even state-of-the-art reinforcement learning methods.}, 
  number={1}, 
  journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
  author={Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter}, 
  year={2018}, 
  month={Apr.},
  doi = {10.1609/aaai.v32i1.11485}
}

@article{WellsBednarz:2021:xAIRLSurvey,
  title={Explainable AI and Reinforcement Learning — A Systematic Review of Current Approaches and Trends},
  author={Wells, Lindsay and Bednarz, Tomasz},
  journal={Frontiers in artificial intelligence},
  volume={4},
  pages={48},
  year={2021},
  publisher={Frontiers},
  abstract={Research into Explainable Artificial Intelligence (XAI) has been increasing in recent years as a response to the need for increased transparency and trust in AI. This is particularly important as AI is used in sensitive domains with societal, ethical, and safety implications. Work in XAI has primarily focused on Machine Learning (ML) for classification, decision, or action, with detailed systematic reviews already undertaken. This review looks to explore current approaches and limitations for XAI in the area of Reinforcement Learning (RL). From 520 search results, 25 studies (including 5 snowball sampled) are reviewed, highlighting visualization, query-based explanations, policy summarization, human-in-the-loop collaboration, and verification as trends in this area. Limitations in the studies are presented, particularly a lack of user studies, and the prevalence of toy-examples and difficulties providing understandable explanations. Areas for future study are identified, including immersive visualization, and symbolic representation.},
  doi={10.3389/frai.2021.550030}
}

@article{WuEtAl:2021:HITLMLSurvey,
  title={A Survey of Human-in-the-loop for Machine Learning},
  author={Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang},
  journal={arXiv preprint arXiv:2108.00941},
  publisher = {arXiv},
  year={2021},
  abstract={Human-in-the-loop aims to train an accurate prediction model with minimum cost by integrating human knowledge and experience. Humans can provide training data for machine learning applications and directly accomplish tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent human-in-the-loop. Using the above categorization, we summarize major approaches in the field; along with their technical strengths/ weaknesses, we have simple classification and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for human-in-the-loop and motivates interested readers to consider approaches for designing effective human-in-the-loop solutions.}
}

@article{WuEtAl:2021:HITLDRLAutonomousDriving,
  title={Human-in-the-Loop Deep Reinforcement Learning with Application to Autonomous Driving},
  author={Wu, Jingda and Huang, Zhiyu and Huang, Chao and Hu, Zhongxu and Hang, Peng and Xing, Yang and Lv, Chen},
  journal={arXiv preprint arXiv:2104.07246},
  publisher = {arXiv},
  year={2021},
  abstract={Due to the limited smartness and abilities of machine intelligence, currently autonomous vehicles are still unable to handle all kinds of situations and completely replace drivers. Because humans exhibit strong robustness and adaptability in complex driving scenarios, it is of great importance to introduce humans into the training loop of artificial intelligence, leveraging human intelligence to further advance machine learning algorithms. In this study, a real-time human-guidance-based deep reinforcement learning (Hug-DRL) method is developed for policy training of autonomous driving. Leveraging a newly designed control transfer mechanism between human and automation, human is able to intervene and correct the agent's unreasonable actions in real time when necessary during the model training process. Based on this human-in-the-loop guidance mechanism, an improved actor-critic architecture with modified policy and value networks is developed. The fast convergence of the proposed Hug-DRL allows real-time human guidance actions to be fused into the agent's training loop, further improving the efficiency and performance of deep reinforcement learning. The developed method is validated by human-in-the-loop experiments with 40 subjects and compared with other state-of-the-art learning approaches. The results suggest that the proposed method can effectively enhance the training efficiency and performance of the deep reinforcement learning algorithm under human guidance, without imposing specific requirements on participant expertise and experience.}
}

%%% XXX %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{XinEtAl:2018:HITLMLFeedbackLoop,
  title={Accelerating Human-in-the-loop Machine Learning: Challenges and Opportunities},
  author={Xin, Doris and Ma, Litian and Liu, Jialin and Macke, Stephen and Song, Shuchen and Parameswaran, Aditya},
  booktitle={Proceedings of the second workshop on data management for end-to-end machine learning},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  pages={1--4},
  year={2018},
  abstract={Development of machine learning (ML) workflows is a tedious process of iterative experimentation: developers repeatedly make changes to workflows until the desired accuracy is attained. We describe our vision for a "human-in-the-loop" ML system that accelerates this process: by intelligently tracking changes and intermediate results over time, such a system can enable rapid iteration, quick responsive feedback, introspection and debugging, and background execution and automation. We finally describe Helix, our preliminary attempt at such a system that has already led to speedups of upto 10x on typical iterative workflows against competing systems.},
  doi={10.1145/3209889.3209897}
}

@article{XiongEtAl:2020:Robustness,
  title={Robustness to Adversarial Attacks in Learning-Enabled Controllers},
  author={Xiong, Zikang and Eappen, Joe and Zhu, He and Jagannathan, Suresh},
  journal={arXiv preprint arXiv:2006.06861},
  publisher = {arXiv},
  year={2020},
  abstract={Learning-enabled controllers used in cyber-physical systems (CPS) are known to be susceptible to adversarial attacks. Such attacks manifest as perturbations to the states generated by the controller's environment in response to its actions. We consider state perturbations that encompass a wide variety of adversarial attacks and describe an attack scheme for discovering adversarial states. To be useful, these attacks need to be natural, yielding states in which the controller can be reasonably expected to generate a meaningful response. We consider shield-based defenses as a means to improve controller robustness in the face of such perturbations. Our defense strategy allows us to treat the controller and environment as black-boxes with unknown dynamics. We provide a two-stage approach to construct this defense and show its effectiveness through a range of experiments on realistic continuous control domains such as the navigation control-loop of an F16 aircraft and the motion control system of humanoid robots.}
}


%%% YYY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Yang:2021:TrailNearOptimalSuboptimal,
  title={TRAIL: Near-Optimal Imitation Learning with Suboptimal Data},
  author={Yang, Mengjiao and Levine, Sergey and Nachum, Ofir},
  journal={arXiv preprint arXiv:2110.14770},
  publisher = {arXiv},
  year={2021},
  abstract={The aim in imitation learning is to learn effective policies by utilizing near-optimal expert demonstrations. However, high-quality demonstrations from human experts can be expensive to obtain in large numbers. On the other hand, it is often much easier to obtain large quantities of suboptimal or task-agnostic trajectories, which are not useful for direct imitation, but can nevertheless provide insight into the dynamical structure of the environment, showing what could be done in the environment even if not what should be done. We ask the question, is it possible to utilize such suboptimal offline datasets to facilitate provably improved downstream imitation learning? In this work, we answer this question affirmatively and present training objectives that use offline datasets to learn a factored transition model whose structure enables the extraction of a latent action space. Our theoretical analysis shows that the learned latent action space can boost the sample-efficiency of downstream imitation learning, effectively reducing the need for large near-optimal expert datasets through the use of auxiliary non-expert data. To learn the latent action space in practice, we propose TRAIL (Transition-Reparametrized Actions for Imitation Learning), an algorithm that learns an energy-based transition model contrastively, and uses the transition model to reparametrize the action space for sample-efficient imitation learning. We evaluate the practicality of our objective through experiments on a set of navigation and locomotion tasks. Our results verify the benefits suggested by our theory and show that TRAIL is able to improve baseline imitation learning by up to 4x in performance.}
}

@article{YangEtAl:2021:AutoCurriculumMARS,
  title={Diverse Auto-Curriculum is Critical for Successful Real-World Multiagent Learning Systems},
  author={Yang, Yaodong and Luo, Jun and Wen, Ying and Slumbers, Oliver and Graves, Daniel and Ammar, Haitham Bou and Wang, Jun and Taylor, Matthew E},
  journal={arXiv preprint arXiv:2102.07659},
  publisher = {arXiv},
  year={2021},
  abstract={Multiagent reinforcement learning (MARL) has achieved a remarkable amount of success in solving various types of video games. A cornerstone of this success is the auto-curriculum framework, which shapes the learning process by continually creating new challenging tasks for agents to adapt to, thereby facilitating the acquisition of new skills. In order to extend MARL methods to real-world domains outside of video games, we envision in this blue sky paper that maintaining a diversity-aware auto-curriculum is critical for successful MARL applications. Specifically, we argue that behavioural diversity is a pivotal, yet under-explored, component for real-world multiagent learning systems, and that significant work remains in understanding how to design a diversity-aware auto-curriculum. We list four open challenges for auto-curriculum techniques, which we believe deserve more attention from this community. Towards validating our vision, we recommend modelling realistic interactive behaviours in autonomous driving as an important test bed, and recommend the SMARTS/ULTRA benchmark.}
}

% TODO: authors: others?
@inproceedings{yang2021efficient,
  title={Efficient Deep Reinforcement Learning via Adaptive Policy Transfer},
  author={Yang, Tianpei and Hao, Jianye and Meng, Zhaopeng and Zhang, Zongzhang and Hu, Yujing and Chen, Yingfeng and Fan, Changjie and Wang, Weixun and Liu, Wulong and Wang, Zhaodong and Peng, Jiajie},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={3094--3100},
  year={2021},
  doi = {10.24963/ijcai.2020/428}
}

%%% ZZZ %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{ZagalJavierVallejos:2004:RealityGap,
  title = {Back to reality: Crossing the reality gap in evolutionary robotics},
  journal = {IFAC Proceedings Volumes},
  volume = {37},
  number = {8},
  pages = {834--839},
  year = {2004},
  note = {IFAC/EURON Symposium on Intelligent Autonomous Vehicles, Lisbon, Portugal, 5-7 July 2004},
  issn = {1474-6670},
  url = {https://www.sciencedirect.com/science/article/pii/S1474667017320840},
  author = {Juan Cristóbal Zagal and Javier Ruiz-del-Solar and Paul Vallejos},
  keywords = {Evolutionary Robotics, Autonomous Systems, On-line Learning},
  abstract = {In this work a new method to evolutionary robotics is proposed, it combines into asingle framework, learning from reality and simulations. An illusory sub-system is incorporated as an integral part of an autonomous system. The adaptation of the illusory system results from minimizing differences of robot behavior evaluations in reality and in simulations. Behavior guides the illusory adaptation by sampling task-relevant instances of the world. Thus explicit calibration is not required. We remark two attributes of the presented methodology: (i) it is a promising approach for crossing the reality-gap among simulation and reality in evolutionary robotics, and (ii) it allows to generate automatically models and theories of the real robot environment expressed as simulations. We present validation experiments on locomotive behavior acquisition for legged robots.},
  doi = {10.1016/S1474-6670(17)32084-0}
}

@article{zanzotto2019human,
  title={Viewpoint: Human-in-the-loop Artificial Intelligence},
  author={Zanzotto, Fabio Massimo},
  journal={Journal of Artificial Intelligence Research},
  volume={64},
  pages={243--252},
  year={2019},
  abstract={Little by little, newspapers are revealing the bright future that Artificial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future may have a possible dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers may need financial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Many learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, many of these workers are shooting themselves in the feet. In this paper, we propose Human-in-the-loop Artificial Intelligence (HitAI) as a fairer paradigm for AI systems. Recognizing that any AI system has humans in the loop, HitAI will reward these aware and unaware knowledge producers with a different scheme: decisions of AI systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. As modern Merry Men, HitAI researchers should fight for a fairer Robin Hood Artificial Intelligence that gives back what it steals.},
  doi={10.1613/jair.1.11345 }
}

@article{zhang2020explainable,
  title={Explainable Recommendation: A Survey and New Perspectives},
  author={Zhang, Yongfeng and Chen, Xu},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={14},
  number={1},
  pages={1--101},
  year={2020},
  publisher={Now Publishers, Inc.},
  doi = {10.1561/1500000066}
}

@incollection{Zhang:2020:AlphaZero,
  title={AlphaZero},
  author={Zhang, Hongming and Yu, Tianyang},
  booktitle={Deep Reinforcement Learning},
  pages={391--415},
  year={2020},
  publisher={Springer},
  abstract={In this chapter, we introduce combinatorial games such as chess and Go and take Gomoku as an example to introduce the AlphaZero algorithm, a general algorithm that has achieved superhuman performance in many challenging games. This chapter is divided into three parts: the first part introduces the concept of combinatorial games, the second part introduces the family of algorithms known as Monte Carlo Tree Search, and the third part takes Gomoku as the game environment to demonstrate the details of the AlphaZero algorithm, which combines Monte Carlo Tree Search and deep reinforcement learning from self-play.},
  doi={10.1007/978-981-15-4095-0_15}
}

@inproceedings{Zhang:2020:human_out_loop,
  title={Can Humans Be out of the Loop?},
  author={Junzhe Zhang and Elias Bareinboim},
  booktitle={First Conference on Causal Learning and Reasoning (CLeaR 2022},
  year={2020},
  url={https://openreview.net/forum?id=P0f91v5fTK}
}

@inproceedings{zhang2019leveraging,
  title={Leveraging Human Guidance for Deep Reinforcement Learning Tasks},
  author={Zhang, Ruohan and Torabi, Faraz and Guan, Lin and Ballard, Dana H and Stone, Peter},
  booktitle={Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages = {6339--6346},
  year={2019},
  doi={10.24963/ijcai.2019/884}
}

@inproceedings{Zhang:2020:CausalImitationLearning,
  title={Causal Imitation Learning With Unobserved Confounders},
  author={Zhang, Junzhe and Kumor, Daniel and Bareinboim, Elias},
  booktitle = {Advances in Neural Information Processing Systems},
  volume={33},
  editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages={12263--12274},
  year={2020},
  abstract={One of the common ways children learn is by mimicking adults. Imitation learning focuses on learning policies with suitable performance from demonstrations generated by an expert, with an unspecified performance measure, and unobserved reward signal. Popular methods for imitation learning start by either directly mimicking the behavior policy of an expert (behavior cloning) or by learning a reward function that prioritizes observed expert trajectories (inverse reinforcement learning). However, these methods rely on the assumption that covariates used by the expert to determine her/his actions are fully observed. In this paper, we relax this assumption and study imitation learning when sensory inputs of the learner and the expert differ. First, we provide a non-parametric, graphical criterion that is complete (both necessary and sufficient) for determining the feasibility of imitation from the combinations of demonstration data and qualitative assumptions about the underlying environment, represented in the form of a causal model. We then show that when such a criterion does not hold, imitation could still be feasible by exploiting quantitative knowledge of the expert trajectories. Finally, we develop an efficient procedure for learning the imitating policy from experts' trajectories.}
}

@inproceedings{zhang2021kogun,
  title={KoGuN: Accelerating Deep Reinforcement Learning via Integrating Human Suboptimal Knowledge},
  author={Zhang, Peng and Hao, Jianye and Wang, Weixun and Tang, Hongyao and Ma, Yi and Duan, Yihai and Zheng, Yan},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={2291--2297},
  year={2021}
}


@InProceedings{pmlr-v80-zhang18k,
  title = 	 {Composable Planning with Attributes},
  author =       {Zhang, Amy and Sukhbaatar, Sainbayar and Lerer, Adam and Szlam, Arthur and Fergus, Rob},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5842--5851},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v80/zhang18k.html},
  abstract = 	 {The tasks that an agent will need to solve often are not known during training. However, if the agent knows which properties of the environment are important then, after learning how its actions affect those properties, it may be able to use this knowledge to solve complex tasks without training specifically for them. Towards this end, we consider a setup in which an environment is augmented with a set of user defined attributes that parameterize the features of interest. We propose a method that learns a policy for transitioning between “nearby” sets of attributes, and maintains a graph of possible transitions. Given a task at test time that can be expressed in terms of a target set of attributes, and a current state, our model infers the attributes of the current state and searches over paths through attribute space to get a high level plan, and then uses its low level policy to execute the plan. We show in 3D block stacking, grid-world games, and StarCraft that our model is able to generalize to longer, more complex tasks at test time by composing simpler learned policies.}
}


@inproceedings{Zhou:2019:CgcNet,
  title={CGC-Net: Cell Graph Convolutional Network for Grading of Colorectal Cancer Histology Images},
  author={Zhou, Yanning and Graham, Simon and Alemi Koohbanani, Navid and Shaban, Muhammad and Heng, Pheng-Ann and Rajpoot, Nasir},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},
  publisher={IEEE},
  pages={388--398},
  year={2019},
  abstract={Colorectal cancer (CRC) grading is typically carried out by assessing the degree of gland formation within histology images. To do this, it is important to consider the overall tissue micro-environment by assessing the cell-level information along with the morphology of the gland. However, current automated methods for CRC grading typically utilise small image patches and therefore fail to incorporate the entire tissue micro-architecture for grading purposes. To overcome the challenges of CRC grading, we present a novel cell-graph convolutional neural network (CGC-Net) that converts each large histology image into a graph, where each node is represented by a nucleus within the original image and cellular interactions are denoted as edges between these nodes according to node similarity. The CGC-Net utilises nuclear appearance features in addition to the spatial location of nodes to further boost the performance of the algorithm. To enable nodes to fuse multi-scale information, we introduce Adaptive GraphSage, which is a graph convolution technique that combines multi-level features in a data-driven way. Furthermore, to deal with redundancy in the graph, we propose a sampling technique that removes nodes in areas of dense nuclear activity. We show that modeling the image as a graph enables us to effectively consider a much larger image (around 16x larger) than traditional patch-based approaches and model the complex structure of the tissue micro-environment. We construct cell graphs with an average of over 3,000 nodes on a large CRC histology image dataset and report state-of-the-art results as compared to recent patch-based as well as contextual patch-based techniques, demonstrating the effectiveness of our method.},
  doi={10.1109/ICCVW.2019.00050}
}

@article{ZhouEtAl:2021:QualitySurvey,
   year = {2021},
   author = {Zhou, Jianlong and Gandomi, Amir H. and Chen, Fang and Holzinger, Andreas},
   title = {Evaluating the Quality of Machine Learning Explanations: A Survey on Methods and Metrics},
   journal = {Electronics},
   volume = {10},
   number = {5},
   pages = {593},
   abstract = {The most successful Machine Learning (ML) systems remain complex black boxes to end-users, and even experts are often unable to understand the rationale behind their decisions. The lack of transparency of such systems can have severe consequences or poor uses of limited valuable resources in medical diagnosis, financial decision-making, and in other high-stake domains. Therefore, the issue of ML explanation has experienced a surge in interest from the research community to application domains. While numerous explanation methods have been explored, there is a need for evaluations to quantify the quality of explanation methods to determine whether and to what extent the offered explainability achieves the defined objective, and compare available explanation methods and suggest the best explanation from the comparison for a specific task. This survey paper presents a comprehensive overview of methods proposed in the current literature for the evaluation of ML explanations. We identify properties of explainability from the review of definitions of explainability. The identified properties of explainability are used as objectives that evaluation metrics should achieve. The survey found that the quantitative metrics for both model-based and example-based explanations are primarily used to evaluate the parsimony/simplicity of interpretability, while the quantitative metrics for attribution-based explanations are primarily used to evaluate the soundness of fidelity of explainability. The survey also demonstrated that subjective measures, such as trust and confidence, have been embraced as the focal point for the human-centered evaluation of explainable systems. The paper concludes that the evaluation of ML explanations is a multidisciplinary research topic. It is also not possible to define an implementation of evaluation metrics, which can be applied to all explanation methods.},
   doi = {10.3390/electronics10050593}
}

@inproceedings{zhu2020object,
  title={Object-Oriented Dynamics Learning through Multi-Level Abstraction},
  author={Zhu, Guangxiang and Wang, Jianhao and Ren, Zhizhou and Lin, Zichuan and Zhang, Chongjie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={6989--6998},
  year={2020},
  doi={10.1609/aaai.v34i04.6183}
}

@inproceedings{ZiebartEtAl:2008:ImitationLearningNavigation,
  title={Maximum Entropy Inverse Reinforcement Learning},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA},
  abstract={Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Prob- lems. This approach reduces learning to the problem of re- covering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behav- ior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling real- world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.}
}