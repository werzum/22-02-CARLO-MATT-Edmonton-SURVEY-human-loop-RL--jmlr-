diff --git a/main.tex b/main.tex
index 17a4149..a352e4f 100644
--- a/main.tex
+++ b/main.tex
@@ -1,818 +1,689 @@
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-% 2022-01-22 ah - last revision 2022-05-29 15:15 CET ah
+% 2022-01-22 ah
 \documentclass[twoside,11pt]{article}
 % Any additional packages needed should be included after jmlr2e.
 % Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
 % and defines many common macros, such as 'proof' and 'example'.
 %
 % It also sets the bibliographystyle to plainnat; for more information on
 % natbib citation styles, see the natbib documentation, a copy of which
 % is archived at http://www.jmlr.org/format/natbib.pdf
 
 % Available options for package jmlr2e are:
 %
 %   - abbrvbib : use abbrvnat for the bibliography style
 %   - nohyperref : do not load the hyperref package
 %   - preprint : remove JMLR specific information from the template,
 %         useful for example for posting to preprint servers.
 %
 % Example of using the package with custom options:
 %
 % \usepackage[abbrvbib, preprint]{jmlr2e}
 
 \usepackage{jmlr2e}
 \usepackage{bm}
 \usepackage{verbatim}
 \usepackage{amsmath}
 \usepackage{array, multirow}
 \usepackage{graphicx}
 \usepackage[colorinlistoftodos]{todonotes}
 \usepackage{booktabs}
 
 \usepackage{makecell}%To keep spacing of text in tables
 \setcellgapes{4pt}%parameter for the spacing
 
 % Definitions of handy macros can go here
 
 \newcommand{\dataset}{{\cal D}}
 \newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
 \DeclareUnicodeCharacter{2212}{−}
 
 
 \usepackage{xcolor}
 \usepackage[normalem]{ulem}
 \newcommand{\addtxt}[1]{{\textcolor[rgb]{0.0,0.5,0.25}{{ #1}}}}
 \newcommand{\sd}[1]{\textcolor{red}{[#1 \textsc{--Srijita}]}}
 \newcommand{\MET}[1]{\textcolor{blue}{MET: #1}}
 \newcommand{\AH}[1]{\textcolor{green}{AH: #1}}
 \newcommand{\ytp}[1]{\textcolor{orange}{Tianpei: #1}}
 \newcommand{\MO}[1]{\textcolor{red}{Mohammad: #1}}
 
 
 % Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}
 
 \usepackage{lastpage}
 \jmlrheading{21}{2022}{1-\pageref{LastPage}}{3/22; Revised
 x/22}{X/22}{20-212}{Retzlaff, Saranti, Angerschmid, Mousavi, Das, Wayllace, Afshari, Taylor, Holzinger}
 
 % Short headings should be running head and authors last names
 
-\ShortHeadings{Human-in-the-loop Reinforcement Learning}{Retzlaff, Saranti, Angerschmid, Mousavi, Das, Wayllace, Afshari, Yang, Taylor, Holzinger}
+\ShortHeadings{Human-in-the-Loop Reinforcement Learning}{Retzlaff, Saranti, Angerschmid, Mousavi, Das, Wayllace, Afshari, Yang, Taylor, Holzinger}
 \firstpageno{1}
 
 % added to fix Todolist error 
 \setlength {\marginparwidth }{2cm}
 
 \begin{document}
 
-\title{Human-in-the-loop Reinforcement Learning: A Survey of Requirements, Challenges and Opportunities}
+\title{Human-in-the-Loop Reinforcement Learning: A Survey of Requirements, Challenges and Opportunities}
 
 \author{\name Carl Orge Retzlaff \email carl.retzlaff@human-centered.ai\\ 
 \addr Human-Centered AI Lab, University of Natural Resources and Life Sciences Vienna, Austria\\
 and DAI Lab, TU Berlin, Germany\\
 \AND
-\name Srijita Das\email srijita1@ualberta.ca \\
-\addr Department of Computing Science \\ 
-University of Alberta, Canada
+\name Anna Saranti \email anna.saranti@human-centered.ai \\
+\addr Human-Centered AI Lab, University of Natural Resources and Life Sciences Vienna, Austria\\
 \AND
-\name Christabel Wayllace\email wayllace@ualberta.ca \\
-\addr Department of Computing Science \\ University of Alberta, Canada\\
+\name Alessa Angerschmid \email alessa.angerschmid@human-centered.ai \\
+\addr Human-Centered AI Lab, University of Natural Resources and Life Sciences Vienna, Austria\\
 \AND
 \name Payam Mousavi \email payam.mousavi@amii.ca \\
 \addr Alberta Machine Intelligence Institute\\
 \AND
-\name Anna Saranti \email anna.saranti@human-centered.ai \\
-\addr Human-Centered AI Lab, University of Natural Resources and Life Sciences Vienna, Austria\\
+\name Srijita Das\email srijita1@ualberta.ca \\
+\addr Department of Computing Science \\ 
+University of Alberta, Canada
 \AND
-\name Alessa Angerschmid \email alessa.angerschmid@human-centered.ai \\
-\addr Human-Centered AI Lab, University of Natural Resources and Life Sciences Vienna, Austria\\
+\name Christabel Wayllace\email wayllace@ualberta.ca \\
+\addr Department of Computing Science \\ University of Alberta, Canada\\
 \AND
 \name Mohammad Afshari\email mafshari@ualberta.ca \\
 \addr Department of Computing Science \\ University of Alberta, Canada\\
 \AND
 \name Tianpei Yang\email tianpei.yang@ualberta.ca \\
 \addr Department of Computing Science \\ University of Alberta, Canada\\
 \AND
 \name Matthew E.~Taylor \email matthew.e.taylor@ualberta.ca \\
 \addr Department of Computing Science\\
 Alberta Machine Intelligence Institute\\
 University of Alberta, Canada
 \AND
 \name Andreas Holzinger \email andreas.holzinger@human-centered.ai \\
 \addr Human-Centered AI Lab, University of Natural Resources and Life Sciences Vienna, Austria \\
 and xAI Lab, Alberta Machine Intelligence Institute\\
 University of Alberta, Canada
 }
 
 
 \editor{N.N.}
 
 \maketitle
 
+
 \newpage
 
 \begin{abstract}%
-Artificial intelligence (AI) in general, and reinforcement learning (RL) in particular, holds the promise of agents learning autonomously to accomplish difficult task with superhuman performance. However, even when an agent is going to eventually perform its task autonomously, we argue that RL is fundamentally a human-in-the-loop paradigm. For instance, an RL agent learns to act in a Markov decision process (MDP), but it is a \emph{human} who both sets an agent to act in this setting and specifies the MDP. Without a human, the agent would have \textit{no concept} of what a reward is or how to define it.  
+Artificial intelligence in general, and reinforcement learning (RL) in particular, holds the promise of agents learning autonomously to accomplish difficult task with superhuman performance. However, even when an agent is going to eventually perform its task autonomously, we argue that RL is fundamentally a human-in-the-loop paradigm. For instance, an RL agent learns to act in a Markov decision process (MDP), but it is a \emph{human} who both sets an agent to act in this setting and specifies the MDP. Without a human, the agent would have no concept of what a reward is or how to define it! 
 
 This article argues that existing work in the literature has not adequately acknowledged this human-centrality as a key component to successful RL. Moreover, we show how significant improvements to existing explainability techniques are required to fully invest humans, whether lay people, subject matter experts, or machine learning experts, with the ability required to drive this human-agent interaction for Human-In-The-Loop (HITL) RL.
 
-To subsume the central insights from this article, Table \ref{table:Explanations_table} gives an overview of the different phases for the agent deployment, with their respective requirements, aspects of human involvement, explanation types and new challenges. 
+Our expectation is that readers of this article will understand (and ideally agree with) our argument that RL is fundamentally a human-agent interactive process. Furthermore, they will quickly understand what current state of the art explainability methods can be used in HITL RL, be able to identify low hanging fruit on explainability research in RL, and appreciate longer-term research goals required to bring this field to fruition. We propose the vision of a human-robot teamup that allows both humans and robots to realise their full potential and play to their respective strengths.
+
+To subsume the central insights from this article, figure \ref{fig:Explanations_table} gives an overview of the different phases for the agent deployment, with their respective requirements, aspects of human involvement, explanation types and new challenges. 
+
+\begin{figure}[h]
+    \centering
+    \includegraphics[width=\textwidth]{img/Explanations_table.png}
+    \caption{Overview of the different explanations contexts in the four different phases.}
+    \label{fig:Explanations_table}
+\end{figure}
 
-Our expectation is that readers of this article will understand (and ideally agree with) our argument that RL is fundamentally a human-agent interactive process. Furthermore, they will quickly understand what current state-of-the-art explainability methods can be used in HITL RL, be able to identify low hanging fruit on explainability research in RL, and appreciate longer-term research goals required to bring this field to fruition. We propose the vision of a human-robot teamup that allows both humans and robots to realize their full potential and play to their respective strengths.
 
 
 \end{abstract}
 
 
 \begin{keywords}
-Reinforcement Learning, Human-in-the-loop, Explainable AI, Embodied Intelligence, Explainability
+Reinforcement Learning, Human-in-the-loop, Explainable AI, Embodied Intelligence, Cyber-physical Systems
 \end{keywords}
 
 
 \section{Introduction}
 \label{sec:introduction}
 
 %RL = autonomous learning
 %But, really humans involved throughout
 %1) where humans interface with RL and 2) why explainability is critical
 %Selecting algorithm, parameters, designing MDP, etc.
 %Pretraining/incorporating knowledge in features, etc.
 %Interactive teaching (curriculum learning), designing when to change MDP/agent (return to previous step)
 %Decision to deploy: trust, safety
 %During deployment: re-training, stopping, returning to drawing board
 %
 %Point of this article
 %1) RL is human-in-the-loop
 %2) Explainability is critical
 %3) What we can do, what should do next (low hanging fruit), what's a long way off still
 
 
 Reinforcement learning~\citep{SuttonBarto:2018:RLIntroduction} (RL) is a very general framework where an agent can autonomously learn how to take actions in order to best maximize the discounted long term sum of rewards. RL agents have had many impressive successes, such as in board games, video games, robotics and natural language processing \citep{Li:2017:DRLSurvey}. One of the benefits of RL is that agents can learn to outperform humans, sometimes coming up with completely novel (and unanticipated) strategies.
 
 The RL paradigm seems to fit well with the long-standing goals of artificial intelligence, in that agents can go into an environment and autonomously learn how to solve difficult problems. However, we argue that this framing is naive, overlooking the significant human input and biases that are encoded into every RL problem. This article argues that: 
 \begin{center}
 \fbox{
     \parbox{0.95\textwidth}{
          \begin{enumerate}
             \item Reinforcement learning is fundamentally a human-in-the-loop paradigm and 
             \item Explainability is critical for the success of real-world \\reinforcement learning systems.
         \end{enumerate}
      }%
 }
 \end{center}
 
-First, we argue that RL is a human-in-the-loop (HITL) paradigm, and identify four stages where human involvement is critical towards the goal of deploying and using RL agents. We emphasize that those stages are of cyclic nature and only loosely ordered in the presented sequence, meaning that the individual phases can be repeated and reiterated during the entire process of deployment.
+First, we argue that RL is a human-in-the-loop paradigm, and identify four stages where human involvement is critical towards the goal of deploying and using RL agents. We emphasize that those stages are of cyclic nature and only loosely ordered in the presented sequence, meaning that the individual phases can be repeated and reiterated during the entire process of deployment.
 
 % The development phase is characterized by the software and systems experts laying the technical groundwork. Here, the focus lays on the RL model itself.
 % Then, the model is trained in an interactive fashion with an end user, under close supervision of the developers. This phase focuses on understanding the model perceptions and fundamentals, and evaluate the interaction with the end user.
 % The third phase is characterized by testing the behavior of the trained model, which requires tools that allow to thoroughly evaluate a large-scale model. We will later talk about this fundamental challenge of finding explainable AI methods that scale well enough to support this phase.
 % Finally, the model is deployed in the real world and interacts with the end user. The model now has to build trust with the end user by providing explanations for its behavior and still accommodate the efficient interaction expected for expert-level users. 
 
 \vspace{2mm}
 \emph{1: Initial Agent Development}
 
 The agent's environment is first selected by a human based on where they think an RL agent might be useful, defining the problem to be solved. For that, human machine learning and/or subject matter specialists construct a Markov decision process (MDP), defining the state space, action space, and reward function --- all three have a critical impact on the speed of learning, the agent's final performance, and what policy is learned. They furthermore set the agent's algorithm and hyperparameters, and decide whether to incorporate prior knowledge, such as by adding detailed features or transferring knowledge from an existing agent. It must also be decided whether the agent should pre-train on existing data (e.g., using offline RL).
 \vspace{2mm}
 
 \emph{2: During Agent Learning}
 
 Humans can decide to disallow the agent from selecting invalid actions or actions that are known to always be suboptimal. The agent's action selection can also be expanded with a bias by an expert to  enable faster learning rates. Furthermore, the developers can decide to include interactive paradigm with human demonstration, feedback, advice or other assistance.
 Eventually, the subject matter expert should decide if the learning process is ``working well,'' or if the MDP should be revised, either because the agent is learning too slowly, or because the policy being learned is not what was anticipated for the problem.
 \vspace{2mm}
 
 \emph{3: Agent Evaluation}
 
 The trained agent is tested by developers and subject matter experts.
 The developers need to ensure that no erroneous behavior or glitches have emerged during training, operating mostly on a syntactical level. The subject matter expert will need to understand and evaluate the learned policies for sensible micro- and macro-behavior, constituting an evaluation of the semantic behavior. The decision for moving forward with the deployment to market or engaging in another development-learning-evaluation cycle lies in the evaluation phase and is made by the developers and project owners. The subject matter expert must decide if the learned policy is ``good enough'' to deploy, if more training is needed, or if the problem definition needs to be changed. 
 \vspace{2mm}
 
 \emph{4: Deployment of Learned Agent}
 
 A human will typically need to argue why the agent is safe enough, or should be trusted enough, to be deployed in the real world. The deployment decision needs to be taken again by the end user, who determines the usage context and specific application of the agent in the field. Furthermore, the development team will need to determine if the agent should continually learn, if its policy should be frozen, or if it should retrain if the environment has changed ``enough.'' 
 \vspace{2mm}
 
 \begin{figure}[h]
     \centering
     \includegraphics[width=0.6\textwidth]{img/HITL_Deployment_Workflow.pdf}
     \caption{Overview of the four steps to deployment and their sequence.}
     \label{fig:Deployment_Workflow}
 \end{figure}
 
 Figure \ref{fig:Deployment_Workflow} gives an overview of the different steps for the agent deployment and their sequence.
 
 Second, in each of these four stages, we argue that explainability is a critical and underdeveloped technology. Before training, explainability can help show the impact of algorithm or hyperparameter selection, how prior knowledge biases the agent, and how pre-training changes the agent's learning process. During training and testing, explainability can show the impact biasing via an existing controller or human advice, how learning is progressing, or how the current policy functions. In the deployment phase after training, explainability can help explain the final policy, improve trust, help the person evaluate safety, and understand the policy's stability.
 
 Part of this article can be considered as a position paper. We want to convince the reader that ignoring humans and treating RL as a fundamentally autonomous learning paradigm is shortsighted --- we will highlight where and how explainability can play a critical role is this human-agent collaboration. 
 
-But this article can also be considered a survey. While discussing existing literature on explainability in RL, we will call out three types of challenges, and frame them as generations of technologies to emphasize future developments and the steps needed to take to achieve the larger goal of human-in-the-loop RL. Solved challenges are those where current explainability techniques can be used to help humans interact with learning agent, which reflect the current generation of technology. Short-term challenges are where we could easily adapt or extend existing explainability work to assist humans and belong to the second generation. Long-term challenges are those where significant research is required before a breakthrough can be achieved. This third generation of long-term challenges challenges are discussed extensively in Subsection \ref{sec:ThirdGeneration}.
-
-The goal in this article is therefore both to try to shift the discussion about RL in general with respect to human subject experts, machine learning experts, or lay people, and also to provide an entry point into this exciting area of contemporary research at the intersection of explainability on RL, with the goal of getting non-experts quickly up to speed on exciting research directions. Throughout, we focus on human-robot interaction and therefore touch strongly on topics concerning embodied intelligence. But where appropriate, we will also refer to and discuss topics with regard to the respective superset of human-agent interaction. We furthermore differentiate between domain experts and end-users. We consider a person domain expert which is experienced in their respective field and has sufficient authority to be integrated as professional in the development cycle of a domain-specific product. End-users on the other hand are the customers which buy and use a final product. While they have a background in the respective field and are motivated to learn about the product, they could not provide guidance on designing such a tool.
-
-The paper is structured as follows: After the introduction and motivation, we provide in Section \ref{sec:background} a thorough overview of background work for explainability and interactive learning and review fundamental and current challenges for embodied intelligence. Then, we explore the four stages for the deployment of HITL RL systems and analyze where to apply xAI in HITL RL Development (Section~\ref{sec:Developing}), Agent Learning (Section~\ref{sec:AgentLearning}), Evaluation (Section~\ref{sec:Evaluation}), and Deployment (Section~\ref{sec:Deployment}), each with specific requirements for the success of human involvement. Section~\ref{sec:discussion} discusses the general challenges observed and how future solutions can be shaped to overcome them. Finally, Section~\ref{sec:conclusion} outlines current problems and goals in the field and proposes further future work.
-
-	\begin{table*}[!htbp]
-	\tiny
-	\resizebox{1\linewidth}{!}{
-		\begin{tabular}{|c||l|l|l|l|}
-			\cline{1-5}
-			\multicolumn{1}{|c||}{ }  & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{ }\\
-			\multicolumn{1}{|c||}{\textsc{Phase}}  & \multicolumn{1}{c|}{\textsc{Explanation Requirements}} & \multicolumn{1}{c|}{\textsc{Human Involvement }} & \multicolumn{1}{c|}{\textsc{Explainability} } & \multicolumn{1}{c|}{\textsc{Challenges}}\\
-			\multicolumn{1}{|c||}{ }  & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{ }\\
-			\cline{1-5}
-			\multirow{11}{*}{\rotatebox{90}{\hspace{0em}\textsc{Development}}} 
-			&  &  & & \\
-			& $\cdot$Comparable between different model versions &$\cdot$Define Problem &\textbf{Techniques:} &	$\cdot$Creating Interactive, Thorough,\\
-			& $\cdot$Rapidly Computable and&$\cdot$Construct State Space, &$\cdot$Pre-Training&$ $ and Comparable Model\\
-			& $ $ Shallow OR &	$ $ Action Space,  	&	 $\cdot$Human-Readable Model 	&	$ $ Summaries	\\
-			& $\cdot$Exhaustive but Slower &	 $ $ Reward Function 	&	 $\cdot$Policies 	&	$\cdot$Use Compositional and	\\
-			&&	 $\cdot$Model Design 	&	 $\cdot$Causal Models	&$ $ Representational Language	\\
-			&&	 	&	 \textbf{Directionality:}	&$\cdot$Integrate Causal Learning	\\
-			&&	 	&	 $\cdot$Unidirectional (model to user)	& $\cdot$Use Policy Querying	\\
-			&&	 	&	 \textbf{Users:}	&	\\
-			&&	 	&	 $\cdot$RL Experts	&	\\
-			&  &  & & \\
-			\cline{1-5}
-			\multirow{10}{*}{\rotatebox{90}{\hspace{0em}\textsc{Agent}} \rotatebox{90}{\textsc{Learning}}} 
-			&  &  & & \\
-			& $\cdot$Interpretable Inputs &	 $\cdot$Evaluative Feedback 	&	 \textbf{Techniques:} 	&	$\cdot$Start with Imitation Learning and	\\
-			& $\cdot$Interactive&	$\cdot$Action-Advice 	&	 $\cdot$Perception Explanation 	&	$ $ Fine-Tune with Preference-Based 	\\
-			& $\cdot$Support Human as Teacher &	 $\cdot$Human Preference 	&	 $\cdot$Behavior Evaluation 	&	$ $ Learning	\\
-			& $\cdot$Understandable by&	 $\cdot$Demonstrations 	&	\textbf{Directionality:}   	&	$\cdot$Adapt xAI Approaches to HITL\\
-			& $ $ Domain Expert & 	&	 $\cdot$Bidirectional 	&$ $ Context	\\
-			&   & 	&	 \textbf{Users:}	&	$\cdot$Supporting Human as Teacher 	\\
-			&   & 	&	 $\cdot$Domain Experts	&	$ $ with Replanning and Corrections	\\
-			&   & 	&	 $\cdot$RL Experts	&	\\
-			&  &  & & \\
-			\cline{1-5}
-			\multirow{10}{*}{\rotatebox{90}{\hspace{0em}\textsc{Evaluation}}}
-			&  &  & & \\
-			& $\cdot$Explanations of  & $\cdot$Understand and Evaluate	&	 \textbf{Techniques:}	& $\cdot$Ensure Understandability for	\\
-			&  $ $ Learned Behavior & $ $ Learned Policies on Micro	&	 $\cdot$Policy Summarization	&$ $ Subject Expert	\\
-			& $\cdot$Scale to Large Models  & $ $ and Macro-Level	&	 $\cdot$Graph-Based Explanations	&	$\cdot$Thorough and Comparable \\
-			& $\cdot$Comparable to  & 	&	 \textbf{Directionality:}	& $ $ Explanations that Scale with	\\
-			& $ $ Untrained Model  & 	&	 $\cdot$Bidirectional	& $ $ Model Size and Complexity	\\
-			& $\cdot$Understandable by  & 	&	 \textbf{Users:}	& $\cdot$Dashboard for Inspection from 	\\
-			&  $ $  Domain Expert& 	&	 $\cdot$Domain Experts	&$ $ Different Viewpoints/Aspects	\\
-			&  $ $   & 	&	 $\cdot$RL Experts	& 	\\
-			&  &  & & \\
-			\cline{1-5}
-			\multirow{10}{*}{\rotatebox{90}{\hspace{0em}\textsc{Deployment}}}
-			&  &  & & \\
-			&  $\cdot$Rapid Computation and& $\cdot$Deployment and	&\textbf{Techniques:}	&	$\cdot$New Approaches Beyond Image\\
-			&  $ $  Low Cognitive Load& $ $ Usage Decision	&	 $\cdot$Combined Explainability	&$ $ and Driving-Based Explanations	\\
-			&  $\cdot$Understandable by End-User& $\cdot$Interaction with End-User	&	 $ $ Dashboard	&	$\cdot$Simple and Fast Explanations\\
-			&  & 	&	 $\cdot$Intent Communication	& $\cdot$Error and Uncertainty Handling	\\
-			&  & 	&	 $\cdot$Error Handling	& $\cdot$Communication of Agent Intent \\
-			&  & 	&	\textbf{Directionality:}	&$ $ Via Different Modalities	\\
-			&   & 	&	 $\cdot$Bidirectional	&	\\
-			&   & 	&\textbf{Users:}	&	\\
-			&   & 	&	 $\cdot$End-Users	&	\\
-			&  &  & & \\
-			\cline{1-5}
-		\end{tabular}
-	}
-	\caption{Overview of the different explanations contexts in the four different phases. \emph{Explanation Requirements} enumerates desirable properties of explanations at this stage, and \emph{Human Involvement} describes how the human is involved in it. The \emph{Explainability} column lists (1) example techniques currently used, (2) directionality of explanations, i.e., agent to human, human to agent, or both, and (3) the types of users interacting with the agent at this stage. The \emph{Third Generation Challenges} describes blue-sky propositions we envision for a comprehensive HITL RL experience.}
-	\label{table:Explanations_table}
-\end{table*}
-
-We created  table \ref{table:Explanations_table} with the intention to give the reader a comprehensive overview of the main insights of our paper. It gives a synopsis of the central aspects of this paper, and subsumes the requirements, challenges and human context for the four phases we discuss extensively in the following pages. 
-
-We want to show that RL greatly benefits from being thought of as human-centered process, and that explainability is required to enable this HITL RL approach. We aim to highlight how current xAI methods can be used to facilitate such HITL approaches, and identify low hanging fruits for further explainability research, ultimately enabling a more productive interaction of humans and RL agents.
+But this article can also be considered a survey. While discussing existing literature on explainability in RL, we will call out three types of challenges, and frame them as generations of technologies to emphasize future developments and the steps needed to take to achieve the larger goal of HITL RL. Solved challenges are those where current explainability techniques can be used to help humans interact with learning agent, which reflect the current generation of technology. Short-term challenges are where we could easily adapt or extend existing explainability work to assist humans and belong to the second generation. Long-term challenges are those where significant research is required before a breakthrough can be achieved. This third generation of long-term challenges challenges are discussed extensively in subsection \ref{sec:ThirdGeneration}.
+
+The goal in this article is therefore both to try to shift the discussion about RL in general with respect to human subject experts, machine learning experts, or lay people, and also to provide an entry point into this exciting area of contemporary research at the intersection of explainability on RL, with the goal of getting non-experts quickly up to speed on exciting research directions.
+
+This paper is structured as follows: After the introduction and motivation, we provide in section \ref{sec:background} a thorough overview on background work for Explainability, Interactive Learning, and review fundamental and current challenges for embodied intelligence. Then, we explore the four stages for the deployment of HITL RL systems and where xAI can be applied in HITL RL Development, Teaching, Evaluation and Deployment, each with specific requirements for the success of human involvement. Finally, we discuss the general challenges observed and how future solutions can be shaped to overcome them. In the conclusion, we outline current problems and goals in the field, and propose further future work.
 
 % ==================
 \section{Background}
 \label{sec:background}
 
-In the background section, we want to establish the technical backdrop for our discussion of the four phases of the deployment of HITL RL agents. We therefore detail the concept of explainability and essential approaches for facilitating insights into the workings of ML models. We then refer to interactive learning as  approach for integrating human-in-the-loop into RL by guiding the RL agent to leverage domain knowledge and rich human experience. Finally, we summarize current challenges for reinforcement learning in general and more specific to the HITL context.
-
 \subsection{Explainability}
 
-Explainable artificial intelligence (xAI) is a framework for helping human users understand the process and outputs of machine learning models. As ML models are deployed in a growing number of applications that affect human life (e.g., agriculture, forestry, health,etc.), the need for such xAI frameworks is ever more apparent. The xAI approaches are essential for many human-AI collaboration scenarios, where understanding and trusting the outputs of the model is a prerequisite for their use.  
+Explainable Artificial Intelligence (xAI) is a framework for helping human users understand the process and outputs of machine learning models. As ML models are deployed in a growing number of applications that affect human life (e.g. agriculture, forestry, health), the need for such xAI frameworks is ever more apparent. The xAI approaches are essential for many human-AI collaboration scenarios, where understanding and trusting the outputs of the model is a prerequisite for their use.  
 
-Explainability has grown from the 1990's with researchers aiming to extract rules from neural networks \citep{TickleEtAl:1998:HistoryNN}, towards an progressively expanding, dedicated research community, as seen at the example of the DARPA initiative \citep{GunningAha:2019:DARPA}. The xAI efforts have since led to a number of very successful xAI methods \citep{HolzWoj:2022:XAIOverview, ZhouEtAl:2021:QualitySurvey}. Explainability in this context is a technical term used to refer to the collection of methods that aim to highlight decision-relevant parts of machine representations and machine models.
+The growing research community in the domain of xAI since the inception of the DARPA initiative \citep{GunningAha:2019:DARPA} has since developed a number of very successful xAI methods \citep{ZhouEtAl:2021:QualitySurvey,HolzWoj:2022:XAIOverview}. Explainability in this context is a technical term used to refer to the collection of methods that aim to highlight decision-relevant parts of machine representations and machine models.
 
 The following examples show how xAI frameworks and human users can interact:
 \begin{enumerate}
 \item Explaining the role of data source in the final decision. Specifically, to identify which data samples were used for a specific action or decision. This is also important for assigning credit to (and potentially compensating) the individuals who produced the data~\citep{zanzotto2019human}. 
 \item Identifying and visualizing components of the model that contributed to its accuracy or to a particular prediction during training via a heatmap \citep{SturmEtAl:2015:InteractiveHeatmap}.
 \item Building trust in the human users specifically when safety is a major issue. For example, in AI applications in medicine, the human user cannot blindly trust the decision made by the AI agent without a reliable explanation. Transparency and accountability are essential ~\citep{Schneeberger:2020:legalAI, Stoeger:2021:MedicalAI}. Hence, xAI strives for high-level of model correctness, helps with generalization and attempts to minimize confident decisions made based on faulty/inappropriate data. Other applications such as autonomous vehicles or robotics are additional examples of cases where the decisions have a significant impact on safety and therefore trust and accountability are pertinent~\citep{araiza2019safe, WellsBednarz:2021:xAIRLSurvey}.
 \item Enabling humans to provide richer feedback through additional, optional counterfactual examples. The feedback, in the form of explanations provided by humans is used by the AI agents towards ultimately a two-way cooperation leading to ever more accurate, robust, and transparent models ~\citep{Karalus:2021:HITL-counterfactuals,PuiuttaVeith:2020:xAIRLSurvey}.   
 \end{enumerate}
 
 These examples show the diverse applications for xAI frameworks. However, objectively assessing the quality of xAI solutions is a key element for their successful deployment. \cite{milani2022survey} name four key metrics for evaluating xAI solutions: \emph{Fidelity}, the truthfulness of the explanation with regard to the model itself. \emph{Performance}, the default metric used to evaluate the success of the AI solution to be explained. \emph{Relevancy}, the relevance of the provided explanations to the task at hand, and \emph{Cognitive load}, the mental effort required to understand the provided explanations. \\
 
 % Moved the section on interpretability here so we could use the same categorization for explainability as well (Payam) - Are we OK with this?
 
-%I feel there is no flow between the previous and this paragraph - Chris
+Interpretability can be considered a subset of explainability, and is defined as the methods which passively make the model understandable, whereas explainability actively generates explanations for a model. We refer to \citep{GlanoisEtAl:2021:SurveyInterpretableRL} for a comprehensive survey of interpretable RL, but extend their categorization of interpretability approaches to include explainability models as well. The three major categories are: 1) interpretable/explainable inputs of RL models; 2) interpretable/explainable environment models for RL; and 3) interpretable/explainable decision-making processes of RL. 
 
-Interpretability can be considered a subset of explainability, and is defined as the methods which passively make the model understandable, whereas explainability actively generates explanations for a model. We refer to \cite{GlanoisEtAl:2021:SurveyInterpretableRL} for a comprehensive survey of interpretable RL, but extend their categorization of interpretability approaches to include explainability models as well. The three major categories are: 1) interpretable/explainable inputs of RL models, 2) interpretable/explainable transition and reward models for RL, and 3) interpretable/explainable decision-making processes of RL. 
-
-The first category focuses on the inputs to the RL model used to make decisions. It not only includes the agent's state, but also other structural information, such as the problem descriptions from human experts \citep{hasanbeig2021deepsynth}, and the relational \citep{battaglia2018relational,martinez2017relational} or hierarchical structure \citep{andreas2017modular,lyu2019sdrl} of the problem. Such information helps humans better understand the decisions made by RL models. 
+The first category focuses on the inputs to the RL model used to make decisions. It not only includes the agent's state, but also other structural information, such as the problem descriptions from human experts \citep{hasanbeig2021deepsynth}, and the relational \citep{martinez2017relational,battaglia2018relational} or hierarchical structure \citep{andreas2017modular,lyu2019sdrl} of the problem. Such information helps humans better understand the decisions made by RL models. 
 %Visualization
-A prominent approach and important building block for explaining complicated inputs is to visualize them as the model perceives them. This is often combined with showing relevance and importance for model decisions, which can help to evaluate whether the model is looking at the right input aspects, but also has its own caveat of possibly misleading the user. See \citet{EvansEtAl:2021:ExplainabilityParadox} for further discussion.
-In the broad approach of saliency maps, important image regions are highlighted. \citet{LiuEtAl:2018:LinearModelUTrees} show an example where continuous ``super-pixels'' with large feature influence are highlighted. \citet{Bach:2015:LayerWiseRelevancePropagation} develop the technique of ``Layer-Wise Relevance Propagation'', that iteratively changes the model inputs to find the relevance of individual (image) parts or features. \\
+A prominent approach and important building block for explaining complicated inputs is to visualize them as the model perceives them. This is often combined with showing relevance and importance for model decisions, which can help to evaluate whether the model is looking at the right input aspects, but also has its own caveat of possibly misleading the user (as further discussed in \cite{EvansEtAl:2021:ExplainabilityParadox}).
+In the broad approach of saliency maps, important image regions are highlighted. \citet{LiuEtAl:2018:LinearModelUTrees} show an example where continuous "super-pixels" with large feature influence are highlighted. \citet{Bach:2015:LayerWiseRelevancePropagation} develop the technique of ``Layer-Wise Relevance Propagation", that iteratively changes the model inputs to find the relevance of individual (image) parts or features. \\
 
-The second category exploits an interpretable/explainable model of the task or environment, e.g., a transition model \citep{martinez2016learning,zhu2020object} or a preference model \citep{icarte2018using,toro2019learning}. Such models help both the RL agent's reasoning about its decision-making and humans' understandings of the decision-making process. \\
+The second category exploits an interpretable/explainable model of the task or environment, e.g., a transition model \citep{martinez2016learning,zhu2020object} or a preference model \citep{toro2019learning,icarte2018using}. Such environment models help both the RL agent's reasoning about its decision-making and humans' understandings of the decision-making process. \\
 
-The third category is the interpretable/explainable decision-making of RL agents, referring to cases where policies can be learned in a fully understandable manner. Some of the previous work directly learns such interpretable policies in the form of decision trees \citep{likmeta2020combining,silva2020optimization,topin2021iterative}, formulas \citep{hein2018interpretable,hein2019generating}, fuzzy rules \citep{akrour2019towards,hein2017particle,zhang2021kogun}, logic rules \citep{jiang2019neural}, programs \citep{sun2019program,verma2019imitation} and so on. Other researchers first learn a non-interpretable policy, then transform this policy to an interpretable one through imitation learning or transfer learning \citep{bastani2018verifiable,VermaEtAl:2018:ProgrammaticallyInterpretableRL}.
+The third category is the interpretable/explainable decision-making of RL agents, referring to cases where policies can be learned in a fully understandable manner. Some of the previous work directly learns such interpretable policies in the form of decision trees \citep{likmeta2020combining,silva2020optimization,topin2021iterative}, formulas \citep{hein2018interpretable,hein2019generating}, fuzzy rules \citep{zhang2021kogun,akrour2019towards,hein2017particle}, logic rules \citep{jiang2019neural}, programs \citep{verma2019imitation,sun2019program} and so on. Others, first learn a non-interpretable policy, then transform this policy to an interpretable one through imitation learning or transfer learning \citep{VermaEtAl:2018:ProgrammaticallyInterpretableRL,bastani2018verifiable}.
 
 A subset of xAI methods go beyond explaining outputs of feed-forward models assuming \emph{independent and identically distributed} (i.i.d.), input data distributions to shed light on \emph{decisions} made by the models, often parameterized by and referred to as \emph{policies}. These models are therefore specific to RL. The following approaches are examples for xAI techniques specifically related to explaining policies (i.e., fit within the third category): 
 
 %Policy Summarization
 In policy summarization, the general aim is to make the underlying model and its policy tangible. This can be done via codifying its decision process as rules, as seen in the linear model u-trees by \citet{LiuEtAl:2018:LinearModelUTrees}. These networks aim to represent the Q-function and use it to make more transparent the feature influence and the rules learned by the network. 
-Another approach is to represent the learned model with generated code blocks, as presented by \citet{VermaEtAl:2018:ProgrammaticallyInterpretableRL}. Their `Programmatically-Interpretable RL'' first learns a neural policy network and then searches a programmatic policy that adequately codifies this network. This approach incurs a performance degradation during training, but results in human-readable policies and improves generalization.
+Another approach is to represent the learned model with generated code blocks, as presented by \citet{VermaEtAl:2018:ProgrammaticallyInterpretableRL}. Their `Programmatically-Interpretable RL" first learns a neural policy network and then searches a programmatic policy that adequately codifies this network. This approach incurs a performance degradation during training, but results in human-readable policies and improves generalization.
 A third approach is to represent the network policies via natural language. \citet{AlonsoEtAl:2018:xAINLBeerClassifier} show an example of justifying classifications with a textual explanation of the choice made by a decision tree. 
 
-The examples shown are sorted in descending order of complexity: the rule representation is the most technical, and code examples provide a middle ground between technical requirements and formal correctness. On the other hand, the representation with natural language is most intuitively understandable but also less exact.
-
+The examples shown are sorted in order of descending complexity - while the rule representation is the most technical, code examples provide a middle-ground between technical requirements and formal correctness. The representation with natural language on the other hand is most intuitively understandable, but also less exact.
 %Policy Querying
 
-In policy querying, the decision process leading to a given result is explained. This can be either general (``when do you do X'') or or specific to a given action/decision.
-An example for a specific explanation is the natural language explanation for a classification as presented by \citet{AlonsoEtAl:2018:xAINLBeerClassifier}, when they provide a textual explanation along with additional details that show how individual factors where quantified and evaluated.
-An example for general policy querying is presented by \citet{HayesShah:2017:AutonomousPolicyExplanation}, which generate a summary of a ``when do you do X?'' type question in natural language to explain the actions of an agent. \\
+In policy querying, the decision process leading to a given result is explained. This can be either general (``when do you do X") or or specific to a given action/decision.
+An example for a specific explanation is the natural language explanation for a classification as presented by \citep{AlonsoEtAl:2018:xAINLBeerClassifier}, when they provide a textual explanation along with additional details that show how individual factors where quantified and evaluated.
+An example for general policy querying is presented by \citep{HayesShah:2017:AutonomousPolicyExplanation}, which generate a summary of a ``when do you do X?" type question in natural language to explain the actions of an agent. \\
 
 
 % Causal Learning subsection
 
-A relatively new and fundamentally different approach to explainability is provided by causal models. Methods available within this framework, generally fall into the second and third categories as they aim to provide causal explanations for the task/environment or the policies respectively. Explainable AI methods that construct causal models, like probabilistic graphical models (PGM) \citep{Koller:2009:ProbabilisticGraphicalModelsBook, Saranti:2019:LearningCompetencePGMs} are slowly emerging in the international literature, 
+A relatively new and fundamentally different approach to explainability is provided by causal models. Methods available within this framework, generally fall into the second and third categories as they aim to provide causal explanations for the task/environment or the policies respectively. Explainable AI methods that construct causal models, like Probabilistic Graphical Models (PGM) \citep{Koller:2009:ProbabilisticGraphicalModelsBook, Saranti:2019:LearningCompetencePGMs} are slowly emerging in the international literature, 
 partly due to the recognition that conventional machine learning models (including neural networks) base their decisions on correlations between input data and the desired action  \citep{Lapuschkin:2019:UnmaskingCleverHans}. 
 
 % GNN
-Another causal learning approach uses graph neural networks (GNN) \citep{Vu:2020:PGMExplainer}, to generate explanations for a prediction via a PGM that identifies crucial graph elements (e.g., nodes, edges) causally responsible for that prediction. Current work is also incorporating the human-in-the-loop strategy \citep{HolzingerEtAl:2016:iMLExperiment, Holzinger:2019:HumanLoopAPIN} where expert domain knowledge is used to select actions. 
+Another causal learning approach uses Graph Neural Networks (GNN) \citep{Vu:2020:PGMExplainer}, to generate explanations for a prediction via a PGM that identifies crucial graph elements (e.g., nodes, edges) causally responsible for that prediction. Current work is also incorporating the human-in-the-loop strategy \citep{HolzingerEtAl:2016:iMLExperiment, Holzinger:2019:HumanLoopAPIN} where expert domain knowledge is used to select actions. 
 
 % Use opportunity chains as modality for good explanations
-\citet{MadumalEtAl:2020:CausalRLCFs} encode causal models using \emph{action influence graphs} to generate explanations using \emph{causal chains} resulting in better explanations as well as improved prediction performance. In their more recent work, \citet{Madumal:2020:DistalEF} propose that investigating interactions between RL agents and humans is the key to generating finer details in the explanations thus alleviating some of the shortcomings of the action influence models. An insight gained through their human studies was that to generate explanations, humans tend to refer to \emph{future} actions that were dependent on the current ones. This supports the idea that humans have a deep understanding of cause and effect chains of actions and events, often referred to as \emph{opportunity chains} in the cognitive psychology literature. Inspired by this, the authors create explanatory models that focus on these opportunity chains and the future actions, they call \emph{distal} action. 
+\citet{MadumalEtAl:2020:CausalRLCFs}, encode causal models using \emph{action influence graphs} to generate explanations using \emph{causal chains} resulting in better explanations as well as improved prediction performance. In their more recent work, \citet{Madumal:2020:DistalEF} propose that investigating interactions between RL agents and humans is the key to generating finer details in the explanations thus alleviating some of the shortcomings of the action influence models. An insight gained through their human studies was that to generate explanations, humans tend to refer to \emph{future} actions that were dependent on the current ones. This supports the idea that humans have a deep understanding of cause and effect chains of actions and events, often referred to as \emph{opportunity chains} in the cognitive psychology literature. Inspired by this, the authors create explanatory models that focus on these opportunity chains and the future actions, they call \emph{distal} action. 
 
 % connection to HITL
 \citet{LiangEtAl:2017:HITLReinforcementLearn} argue that the cognitive ability of the human operator paired with the computational power of a machine have the potential to handle complex tasks. Moreover, they state that it is essential for a productive interaction that the machine is able to react to the environment as well as the human operator. This was demonstrated by safety measurements showing that the machine needs to pay attention to the operator as well as the environment and possible bystanders. Furthermore, humans need to understand and interpret the actions of the machine correctly to improve the algorithm \citep{heuillet2021explainability}. Moreover, \citet{heuillet2021explainability} state that the underlying algorithm and its decisions need to be understandable for a multitude of different audiences with various goals.
 
 % Counterfactuals
 
 With the help of adequate personalized User Interfaces (UI) \citep{Sun:2021:TopologyPerturbationGNNs}, counterfactuals can also be created by following an action sequence driven by human expertise. This can also be injected in the form of the priors of the random variables and consists of a form of inductive bias.
 Beyond counterfactuals, ``opportunity chains'' contain a sequence of events with causal dependency, and contain the concept of enabling events down this sequence \citep{Madumal:2020:DistalEF}.
 
 % Not just use human explanations as prior, but use expertise to imitate
-Observational data from expert actions go beyond injecting a prior to an explainable model \citep{Zhang:2020:CausalImitationLearning}. Causal imitation learning learns a structural causal model (SCM) \citep{Pearl:2000:ModelsReasoningInference} from policies performed by humans, even if the actual reward is not specified and the environment is not perceived the same to the learner and the human expert demonstrator. But this can also be beneficial since researchers have shown that incorporating that knowledge maximizes the aggregated reward, even if it sacrifices some autonomy. 
+Observational data from expert actions go beyond injecting a prior to an explainable model \citep{Zhang:2020:CausalImitationLearning}. Causal imitation learning learns a Structural Causal Model (SCM) \citep{Pearl:2000:ModelsReasoningInference} from policies performed by humans, even if the actual reward is not specified and the environment is not perceived the same to the learner and the human expert demonstrator. But this can also be beneficial since researchers have shown that incorporating that knowledge maximizes the aggregated reward, even if it sacrifices some autonomy. 
 
-Dynamic SCMs are incorporated to formalize the partially-observable markov decision process (POMDPs) \citep{SuttonBarto:2018:RLIntroduction} as perceived by the agent, and take into account the human intervention and its implications. The so-called counterfactual agent does not blindly take the human's advice and execute it; it compares it with other possible actions and decides correspondingly. In cases where the reward and the transition functions are the same, the ``human-in-the-loop'' is beneficial, even if the human's instructions are far from perfect. The implications that this will have on the explainability of the learned models are a vital area of future research. Especially in the latter case, where the human needs to provide adequate instructions in selected states, the human understanding of the agent's current SCM, is vital.  
+Dynamic SCMs are incorporated to formalize the Partially-observable Markov Decision Process (POMDPs) \citep{SuttonBarto:2018:RLIntroduction} - as perceived by the agent - and take into account the human intervention and its implications. The so-called counterfactual agent does not blindly take the human's advice and execute it; it compares it with other possible actions and decides correspondingly. In cases where the reward and the transition functions are the same, the ``human-in-the-loop'' is beneficial, even if the human's instructions are far from perfect. The implications that this will have on the explainability of the learned models are a vital area of future research. Especially in the latter case, where the human needs to provide adequate instructions in selected states, the human understanding of the agent's current SCM, is vital.  
 
 \subsection{Interactive Learning}
-\label{sec:InteractiveLearning}
+\label{IRLBackground}
     
 The most fundamental ways of learning in nature is parents teaching their off-springs in an interactive fashion. Similar learning dynamics exist between a teacher and a student, where the teacher tries to guide the student with their experience and knowledge. Following the same motivation, interactive learning~\citep{Arzate:2020:SurveyInteractiveRL} in RL aims to involve human-in-the-loop to guide the RL agent by leveraging domain knowledge and rich human experience. In recent times,  RL has been successfully applied to solve many real-world problems, ranging from drug discovery~\citep{popova2018deep}, navigating super pressure balloons in the stratosphere~\citep{bellemare2020autonomous} to robot manipulation tasks~\citep{nguyen2019review}. While this is an exciting direction for research, recent deep RL systems still face a lot of bottlenecks, including sample inefficiency, sim-to-real transfer issues, generalization, exploration, etc., to name a few~\citep{ibarz2021train}. Interactive RL aims to solve some of these challenges by involving a human prior to training~\citep{Guo:2022:RLSurveyHumanPriorKnowledge}, during training~\citep{Knox:2008:TAMER} or in the deployment phase of the RL system~\citep{guo2021edge}. Interactions could either be teacher-initiated~\citep{torrey2013teaching}, student-initiated~\citep{da2020uncertainty,MandelEtAl:2017ActionsInHITL} or jointly initiated~\citep{amir2016interactive} by both the parties.
 
-In interactive learning, the human is characterized as a teacher and the teaching loop can contain different types of critique, advice modalities and guidance that can be fed back to the RL algorithm. There can be different modalities of human advice such as binary evaluative feedback [+1/-1] ~\citep{Knox:2008:TAMER}, action-advice~\citep{torrey2013teaching}, preference based feedback~\citep{Christiano:2017:DeepRLHumanPreferences,LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE}, and sub-goal specification~\citep{le2018hierarchical}. A comprehensive survey of various types of human guidance in Deep RL can be found in \citet{zhang2019leveraging}. \citet{Thomaz:2006:RLWithHumanTeachers} proposed one of the earliest work on Interactive RL which allowed human trainers to give binary feedback for the agent's behavior and specific objects associated with the task. \citet{Arzate:2020:SurveyInteractiveRL} showed that humans provided rewards for future actions rather than past actions executed by the agent  classify methods of interactive RL according to the way human feedback tailors an RL dimension. That method could modify the reward function, the agent's policy, the exploration process, or the value function.  
+In interactive learning, the human is characterized as a teacher and the teaching loop can contain different types of critique, advice modalities and guidance that can be fed back to the RL algorithm. There can be different modalities of human advice such as binary evaluative feedback [+1/-1] ~\citep{Knox:2008:TAMER}, action-advice~\citep{torrey2013teaching} , preference based feedback~\citep{Christiano:2017:DeepRLHumanPreferences,LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE}, and sub-goal specification~\citep{le2018hierarchical}. A comprehensive survey of various types of human guidance in Deep RL can be found in \citep{zhang2019leveraging}. \citet{Thomaz:2006:RLWithHumanTeachers} proposed one of the earliest work on Interactive RL which allowed human trainers to give binary feedback for the agent's behavior and specific objects associated with the task. They showed that humans provided rewards for future actions rather than past actions executed by the agent \citep{Arzate:2020:SurveyInteractiveRL} classify methods of interactive RL according to the way human feedback tailors an RL dimension. A method could modify the reward function, the agent's policy, the exploration process, or the value function.  
 
 A common approach to modify the reward function is called reward shaping. In reward shaping, the teacher provides useful information to shape the reward function so that favourable parts of the state-space are encouraged and unfavourable parts are penalized \citep{ng:99}. This is useful in sparse reward environments and facilitates the reward specification in complex domains. A well-known reward shaping framework is TAMER~\citep{Knox:2008:TAMER, knox:13} where the human provides evaluative reinforcement (positive or negative feedback) by observing the agent in action. This signal is used to build a model of the human's reward function. 
 
-Methods that consider modifying the agent's policy are called policy shaping \citep{cederborg2015policy,griffith2013policy,WuEtAl:2021:HITLDRLAutonomousDriving}. These methods augment an agent's policy directly using human knowledge. This technique does not require a well formulated reward function, but it assumes the trainer knows a near-optimal policy to guide the agent. \citet{macglashan2017interactive} proposed COACH framework and showed that human feedback is dependent on the agent’s current policy. 
+Methods that consider modifying the agent's policy are called policy shaping \citep{griffith2013policy,cederborg2015policy,WuEtAl:2021:HITLDRLAutonomousDriving}. These methods augment an agent's policy directly using human knowledge. This technique does not require a well formulated reward function, but it assumes the trainer knows a near-optimal policy to guide the agent. \citet{macglashan2017interactive} proposed COACH framework and showed that human feedback is dependent on the agent’s current policy. 
 
 Human advice can also be useful in guiding the agent in it’s exploration phase so that the high rewarding states or trajectories are identified by the agent in less number of environment interactions~\citep{amir2016interactive}. Action pruning is another way where human-in-the-loop RL can guide exploration and improve learning \citep{Abel:2017:AgentAgnosticHumanInTheLoopRL}.
 
-Lastly, human-advice-based value functions can be combined with agent value functions~\citep{jiang:21,kartoun:10, taylor2011integrating, WuEtAl:2021:HITLDRLAutonomousDriving} to effectively guide the agent. Demonstrations \citep{hester2018deep,vecerik2017leveraging,nair2018overcoming} from humans can also augment the value function by biasing it according to the  actions taken by the expert. These approaches have been particularly successful in complex robotics tasks like pushing, sliding etc which can easily be demonstrated by humans.  Demonstrations, by default, may contain human biases which can, in turn, be removed by experts \citep{Wang:2022:SkillPreferences}.
+Lastly, human-advice-based value functions can be combined with agent value functions~\citep{taylor2011integrating, kartoun:10,jiang:21, WuEtAl:2021:HITLDRLAutonomousDriving} to effectively guide the agent. Demonstrations \citep{hester2018deep,vecerik2017leveraging,nair2018overcoming} from humans can also augment the value function by biasing it according to the  actions taken by the expert. These approaches have been particularly successful in complex robotics tasks like pushing, sliding etc which can easily be demonstrated by humans.  Demonstrations, by default, may contain human biases which can, in turn, be removed by experts \citep{Wang:2022:SkillPreferences}.
 
 The first indicator for the need for human intervention is model performance, since a bad model is more likely to produce bad policies. However, good models can also lead to bad policies as well at deployment scenarios because of the sim-to-real gap. In many cases, human intervention is simulated from pseudo-agents since both the potential benefits and the need to know where the imperfections of the model are can be detected before deployment. The designer of the interactive framework must also take into account that the human interventions might not always be perfect or beneficial; the user might need special training and an informative user interface to effectively improve the RL algorithm. 
 %merged the UI paragraph from the Interactive learning section here
 
-The type of UI, hardware-delivered or natural interaction,  determines the degree of expertise required and can affect the quality of the feedback \citep{lin:20}. Keyboard keys, mouse clicks with sliders, and game controllers are examples of UIs in hardware-delivered interactions, and experts or knowledgeable trainers generally use these UIs. On the other hand, sound interfaces that use the techniques of audification and sonification \citep{Hermann:2011:Sonification,kartoun:10, Saranti:2009,Scurto:2021:DesigningDeepRLHumanParameterExploration}, cameras to capture facial expressions \citep{arakawa:18}, etc. are examples of UIs for natural interaction that non-expert users prefer. 
+The type of user interface (UI), hardware-delivered or natural interaction,  determines the degree of expertise required and can affect the quality of the feedback \citep{lin:20}. Keyboard keys, mouse clicks with sliders, and game controllers are examples of UIs in hardware-delivered interactions, and experts or knowledgeable trainers generally use these UIs. On the other hand, sound interfaces that use the techniques of audification and sonification \citep{Hermann:2011:Sonification, Saranti:2009:QuantumHarmonicOscSonification,kartoun:10,Scurto:2021:DesigningDeepRLHumanParameterExploration}, cameras to capture facial expressions \citep{arakawa:18}, etc. are examples of UIs for natural interaction that non-expert users prefer. 
 
-\subsection{Challenges for Reinforcement Learning and HITL approaches} 
-\label{sec:ChallengesRL}
+\subsection{Challenges for Reinforcement Learning and HITL approaches}
 
-To conclude the background section, we want to talk about underlying challenges for reinforcement learning in general and those more specific to HITL approaches in order to get a better understanding of fundamental and current challenges in the field.
+To conclude the background section, we want to talk about underlying challenges for Reinforcement Learning in general and those more specific to embodied intelligence in order to get a better understanding of fundamental and current challenges in the field.
 
 %%%Exploration Exploitation Tradeoff
 One fundamental challenge is the Exploitation Exploration trade-off. The trade-off is defined by the decision when to continue exploiting a current option, and when to explore further for new options. This challenge is most often illustrated at the example of the one-armed bandit. An agent is tasked with finding the best (ie. most rewarding) one-armed bandit slot machine in a casino, and has a limited amount of coins. When the agent is now sitting at machine A, how long does he take to evaluate whether this machine provides a higher payoff, and when does he cut his losses to move on to another automaton? A more in-depth description of this problem can be found in \citet{AudibertMunosSzepesv:2009:ExplorationExploitation}.
 
 %%%Reality Gap
-Another general challenge is the ``sim-to-real gap'', which describes the difficult task of translating the experience from a simulation to an applications reality \citep{ZagalJavierVallejos:2004:RealityGap}. It first implies that we are always under-modelling our system, which means that aspects of reality are missing which will present our agent with unforeseen challenges and sometime even prevents a policy learned in a simulation from being transferable to the real world. Secondly, real-world samples are very expensive (cost, complexity and time-wise), which makes modelling despite its challenges much more appealing  \citep{KoberBagnellPeters:2013:RLRoboticsSurvey}.
+Another general challenge is the reality gap (or ``sim-to-real gap"), which describes the difficult task of translating the experience from a simulation to an applications reality \citep{ZagalJavierVallejos:2004:RealityGap}. It first implies that we are always under-modelling our system, which means that aspects of reality are missing which will present our agent with unforeseen challenges and sometime even prevents a policy learned in a simulation from being transferable to the real world. Secondly, real-world samples are very expensive (cost, complexity and time-wise), which makes modelling despite its challenges much more appealing  \citep{KoberBagnellPeters:2013:RLRoboticsSurvey}.
 
 %%% Pixel based learning is hard and leads to brittle assumptions
-With vision being a central modality for agents employed in the real world, learning from pixels is essential. This is however a very hard problem, which often relies on brittle assumptions and in consequence often does not generalize \citep{TomarEtAl:2021:LearnPixelControlRepresentations}.
+With vision being a central modality for agents employed in the real world, learning from pixels is essential. This is however a very hard problem, which often relies on brittle assumptions and in consequence often does not generalize \citet{TomarEtAl:2021:LearnPixelControlRepresentations}.
 
 
-More specific to reinforcement learning for embodied intelligence, \citet{RoyEtAl:2021:RLRoboticsChallenges} provide a comprehensive overview of different challenges. To highlight five constraints particular to it:
+More specific to Reinforcement Learning for embodied intelligence, \citet{RoyEtAl:2021:RLRoboticsChallenges} provide a comprehensive overview of different challenges. To highlight five constraints particular to it:
 \begin{itemize}
     \item Interaction with real world entails safety risks for exploration and hard limits on resources like energy.
     \item Poor alignment of learned models and real world.
     \item Require stronger generalizations  and adaptation since specifications, goals and rewards might change.
     \item Observed data is plentiful but drawn from local distribution, requiring agent to learn a reasonable world model beyond  what is currently observed. 
     \item Agent morphology defines what can be learned from the environment and has to be considered when designing agents.
 \end{itemize}
 
-Partially overlapping with those constraints, \citet{ibarz2021train} conducted a survey of RL applications and some of the main challenges they identified were sample efficiency, sim-to-real-gap, exploration challenges, generalization challenges, goal and reward shaping and safety issues. Finally, there are challenges more specific for HITL approaches. 
+Partially overlapping with that, \citet{ibarz2021train} conducted a survey of RL applications and the challenges they encountered. Some of the main challenges they identified were sample efficiency, sim-to-real-gap, exploration challenges, generalization challenges, goal and reward shaping and safety issues. Finally, there are challenges more specific for HITL approaches. 
 
 %reward shaping is difficult
-To be able to characterize, describe and compare reward shaping approaches, one needs some xAI methods to uncover the rules by which the neural network decides for an action. The designer or RL systems is there to provide the basic framework parameters - the most fundamental of those being the reward scheme - but a deeper interference can eventually destroy the potential of the agent being able to figure out effective strategies on its own. The challenge is that the cost/reward function design needs domain knowledge, but does the human always (at each possible state) know what is a profitable behaviour? The main idea is to not shape the reward too much or make it too complex --- but those two are highly subjective and need to be tested in practice. So, apart from the exploration-exploitation balance, there is also a trade-off between human intervention and the agent's ability of invention (intervention-invention). The main research question is to what extend should RL imitate a good human player or expert, and how can RL with the human-in-the-loop can discover new action sequences, not yet thought by humans and how xAI can help bridge this divide. \citet{LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE} discuss this challenge for reinforcement learning and propose a HITL approach to deal with this.
+To be able to characterize, describe and compare reward shaping approaches, one needs some xAI methods to uncover the rules by which the neural network decides for an action. The designer or RL systems is there to provide the basic framework parameters - the most fundamental of those being the reward scheme - but a deeper interference can eventually destroy the potential of the agent being able to figure out effective strategies on its own. The challenge is that the cost/reward function design needs domain knowledge, but does the human always (at each possible state) know what is a profitable behaviour? The main idea is to not shape the reward too much or make it too complex - but those two are highly subjective and need to be tested in practice. So, apart from the exploration-exploitation balance, there is also a trade-off between human intervention and the agent's ability of invention (intervention-invention). The main research question is to what extend should RL imitate a good human player or expert, and how can RL with the human-in-the-loop can discover new action sequences, not yet thought by humans and how xAI can help bridge this divide. \citet{LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE} discuss this challenge for Reinforcement Learning and propose a HITL approach to deal with this.
 
 % when did training converge and when to include human
 Deep RL agents, after training has reached a good performance, will generalize best for good players. However, we know that explanations are not reliable and do not make sense when the neural network does not have an overall good performance or generalization capability, even more for misclassified examples or other underlying errors. Therefore it is difficult to think in what way one can use those explanations during the training process or how one could automate the process of finding the right time point (epoch) to allow the human to do so. Newer methods for RL, like HindsightExperienceReplay (HER) \citep{Andrychowicz:2017:HERHindsightExperienceReplay}, even consider episodes where the goal state is not reached as such to use this additional information. This is more likely in a neural network that is not trained to the extent of having a good enough performance. In such a scenario, adequate xAI methods could shed light on agents that follow RL-policies with several goals. 
 
 %limits of xAI?
-One other aspect is that most of the current xAI methods invented for deep neural networks that also have a practical implementation are not created with RL principles in mind. They are typically created with the intent of uncovering a simpler, interpretable model or to pinpoint the important elements of a potential input, driven by the mathematical principles of neural networks. For example, in the groundbreaking example of the convolutional neural network (CNN) that was used to process the Atari images \citep{Mnih:2013:PlayingAtariDeepRL} a state-of-the-art xAI method, namely layer-wise relevance propagation (LRP) could be used \citep{Bach:2015:LayerWiseRelevancePropagation,Alber:2019:Innvestigate}. However, this would only provide to the user a heatmap about what is positive and what is negatively relevant for the prediction, meaning that it would only characterize (in RL terms) one input state. Those heatmaps are not juxtaposed or combined with the possible actions from that state, or their expected reward as a whole ---the human would not know why the RL algorithm decided for the selected next action. To reconstruct the complete strategy of a model, its rules and the underlying purposes of all (or at least the representative) state-action pairs out of those heatmaps, would be a very cumbersome task. 
+One other aspect is that most of the current xAI methods invented for deep neural networks that also have a practical implementation are not created with RL principles in mind. They are typically created with the intent of uncovering a simpler, interpretable model or to pinpoint the important elements of a potential input, driven by the mathematical principles of neural networks. For example, in the groundbreaking example of the Convolutional Neural Network (CNN) that was used to process the Atari images \citep{Mnih:2013:PlayingAtariDeepRL} a state-of-the-art xAI method, namely Layer-wise Relevance Propagation (LRP) could be used \citep{Bach:2015:LayerWiseRelevancePropagation,Alber:2019:Innvestigate}. However, this would only provide to the user a heatmap about what is positive and what is negatively relevant for the prediction, meaning that it would only characterize (in RL terms) one input state. Those heatmaps are not juxtaposed or combined with the possible actions from that state, or their expected reward as a whole - the human would not know why the RL algorithm decided for the selected next action. To reconstruct the complete strategy of a model, its rules and the underlying purposes of all (or at least the representative) state-action pairs out of those heatmaps, would be a very cumbersome task. 
 
-We argue that the discussed challenges show that generalisable and performant RL is a fundamentally challenging problem, as exemplified by the different obstacles detailed in this Section. We claim that many of those challenges can be overcome with the application of HITL approaches. 
-% Similarly, \citet{MatthewsonPilarski:2022:DesigningAndEvaluatingHCIML} argue that human-centered thinking for RL approaches is essential for designing and deployment of RL systems, enabled by human-centered approaches like reward shaping and better feedback between agent and human \citep{Li:2019:HumanCenteredRLSurvey}. While \citet{MatthewsonPilarski:2022:DesigningAndEvaluatingHCIML} take the bird-eye perspective on all machine learning as interactive machine learning and formulate an overall guideline for human-centered design of ML systems, we focus specifically on the deployment process of interactive and HITL RL systems.
- \citet{MatthewsonPilarski:2022:DesigningAndEvaluatingHCIML} argue that human-centered interactive approaches are essential for designing and deployment of all machine learning systems. While \citet{MatthewsonPilarski:2022:DesigningAndEvaluatingHCIML} take the bird-eye perspective on all machine learning systems as interactive machine learning and formulate an overall guideline for human-centered design of ML systems, we focus specifically on the design, evaluation and deployment of interactive HITL RL systems and associated short-term and long-term challenges.
+We argue that the discussed challenges show that generalisable and performant RL is a fundamentally challenging problem, as exemplified by the different obstacles detailed in this section. We claim that many of those challenges can be overcome with the application of HITL approaches. Similarly, \citet{MatthewsonPilarski:2022:DesigningAndEvaluatingHCIML} argue that human-centered thinking for RL approaches is essential for designing and deployment of RL systems, enabled by human-centered approaches like reward shaping and better feedback between agent and human \citep{Li:2019:HumanCenteredRLSurvey}. 
 
 We furthermore argue that xAI approaches are fundamental for the success of HITL approaches, a sentiment shared by other researchers \citep{heuillet2021explainability,milani2022survey}. We propose that in each of the four deployment phases outlined, xAI is essential. Before training, explainability can help show the impact of algorithm or hyperparameter selection, how prior knowledge biases the agent, and how pre-training changes the agent's learning process. During training and testing, explainability can show the impact biasing via an existing controller or human advice, how learning is progressing, or how the current policy functions. In the deployment phase after training, explainability can help explain the final policy, improve trust, help the person evaluate safety, and understand the policy's stability. 
 
-In the following Sections, we want to show where xAI can be applied in the deployment of HITL RL agents, which solutions exist and how they might be adapted to allow for better HITL interaction.
+In the following sections, we want to show where xAI can be applied in the deployment of HITL RL agents, which solutions exist and how they might be adapted to allow for better HITL interaction.
 % =====================
 
-\section{Initial Agent Development}
+\section{Developing RL Models}
 \label{sec:Developing}
 
-The first step of a HITL RL model deployment process is developing the underlying model, along with problem formulation and pre-learning considerations. This entails making the model understandable to the software developers and AI experts, who want detailed insight into their model. 
+The first step of a HITL RL model deployment process is developing the underlying model, along with problem formulation and pre-learning considerations. This entails making the model understandable to the software developers and AI experts, which want detailed insight into their model. 
 % Motivation for XAI consideration in the Development process
-Integrating considerations for explainability into this development process of RL systems is strongly recommended, since it allows the AI experts to start off in the right direction, instead of having to make costly (post-hoc) changes later on in the RL life-cycle. Decisions on the groundwork made now have large implications for the overall RL life-cycle and are likely to be hard to change later on.
+Integrating considerations for explainability into this development process of RL systems is strongly recommendable, since it allows them to start off in the right direction, instead of having to make costly (post-hoc) changes later on in the RL lifecycle. Decisions on the groundwork made now have large implications for the overall RL lifecycle and are likely to be hard to change later on.
 
 % Benefits - solid groundwork, avoid common errors, generate baseline
-At this stage, working with an understandable model can help to ensure the model is based on solid assumptions and comes to consistent, and sound conclusions. The numerous error sources pertaining to training data, model initialization, and the initial learning process can be monitored and limited. Ensuring proper model function and architecture at this step also ensures a proper baseline for comparing the trained model against.
+At this stage, working with an understandable model can help to ensure the model is based on solid assumptions and comes to consistent, sound conclusions. The numerous error sources pertaining to training data, model initialization, and the initial learning process can be monitored and limited. Ensuring proper model function and architecture at this step also ensures a proper baseline for comparing the trained model against.
 
 \subsection{Requirements}
 % considerations
-%We propose primary considerations for the development stage. In the beginning, the generated explanations should be comparable to other versions (i.e., not newly ordered graphs, which would be hard to compare), in order to track the progress of development. 
-%Then, we identify two approaches for an explainable development process. First, we identify a broader and more superficial evaluation of general model behavior, which in turn allows for inspecting many different aspects of the model behavior. Here, explanations have to be rapidly computed, to enable a quick feedback loop during training. 
-%\citet{XinEtAl:2018:HITLMLFeedbackLoop} explores the implications of a quicker HITL feedback loop. The authors highlight aspects like introspection, the ability to rapidly analyze and compare the impact of changes to reuse intermediate results, and an easier end-to-end optimization by quicker feedback.
-
-%Second, deep inspection with a technique like DAGs of model behavior which provides a more elaborate view of the model, but requires more time to understand and interpret correctly, and can therefore be used to view fewer model snapshots. Both approaches should complement each other since a thorough assessment of model behavior will require both depth and breadth.
-
-We propose primary considerations for the development stage. In the beginning, the generated explanations should be comparable to other versions (i.e., not newly ordered graphs, which would be hard to compare) in order to track the progress of development. 
+We propose central considerations for the development stage. Firstly, the generated explanations should be comparable to other versions (ie. not newly ordered graphs, which would be hard to compare), in order to track the progress of development. 
+Then, we identify two approaches for an explainable development process. On the one hand, a broader and more superficial evaluation of general model behavior, which in turn allows for inspecting many different aspects of the model behavior. Here, explanations have to be rapidly computed, to enable a quick feedback loop during training. 
+\citet{XinEtAl:2018:HITLMLFeedbackLoop} explore the implications of a quicker HITL feedback loop. They highlight aspects like introspection, the ability to rapidly analyze and compare the impact of changes, to reuse intermediate results, and an easier end-to-end optimization by quicker feedback.
 
-Further, we identify two approaches for an explainable development process. We consider (1) a broader and more superficial evaluation of general model behavior, which allows for a high-level inspection of the model behavior, and (2) an in-depth assessment that provides a more detailed and complex model view.
-
-In the first case, explanations must be computed fast to enable a feedback loop during training. \citet{XinEtAl:2018:HITLMLFeedbackLoop} explore the implications of a quicker HITL feedback loop. The authors highlight aspects like introspection, the ability to rapidly analyze and compare the impact of changes to reuse intermediate results, and an easier end-to-end optimization by quicker feedback.
-
-In the second case, explanations require more time to compute, understand, and interpret correctly. Therefore, people can use these explanations to analyze a small number of model snapshots.
-Both approaches should complement each other since a thorough assessment of model behavior will require both depth and breadth.
+On the other hand, deep inspection with a technique like DAGs of model behavior provides a more elaborate view of the model, but requires more time to understand and interpret correctly, and can therefore be used to view fewer model snapshots. Both approaches should complement each other since a thorough assessment of model behavior will require both depth and breadth.
 
 % removed requirements:
-%We can also set some constraints for the xAI methods. Depth-oriented techniques can be more complex and detailed since it is used repeatedly by knowledgeable personnel, who can afford the required cognitive load. In other words, we can move more to the ``complexity'' side on the performance-complexity spectrum, since we care more about detailed insights than easiest understandability. Also, computational resources are the least constrained, and the scalability of explanations is also not essential since the model is not yet fully trained in this phase.
+We can also limit some constraints for the xAI methods. Depth-oriented techniques can be more complex and detailed since it is used repeatedly by knowledgeable personnel, which can afford the required cognitive load. In other words, we can move more to the "complexity" side on the performance-complexity spectrum, since we care more about detailed insights than easiest understandability. Also, computational resources are the least constrained, and the scalability of explanations is also not essential since the model is not yet fully trained in this phase.
 
-Complex and detailed explanations should serve knowledgeable personnel since they care more about detailed insights than easy understandability and can afford the required cognitive load. Additionally, at the development stage, computational resources are the least constrained, and the scalability of explanations is not essential either since the model is not yet fully trained in this phase.
-
-We identify different approaches that can help during this phase of model deployment. First, pre-training can help with the groundwork of intelligent behavior and enable sensible debugging. Second, interpretability approaches should be considered at this step, since inherent understandability implemented at this step will also benefit all succeeding phases. Finally, we argue which types of explainability approaches are suitable for this phase.
+We identify different approaches that can help during this phase of model deployment. First, pre-training can help with the groundwork of intelligent behavior and enable sensible debugging. Secondly, interpretability approaches should be considered at this step, since inherent understandability implemented at this step will also benefit all succeeding phases. Finally, we argue which types of explainability approaches are suitable for this phase.
 
 \subsection{Pre-Training}
 Pre-Training models are beneficial in the RL training workflow since they prepare the groundwork for a more productive Human-Robot interaction. Pre-training for RL benefits HITL in several ways. It helps to 
-develop useful priors, diverse behaviors, generalized policies/feedback, and efficient initial feedback by the human. See~\citet{daniel2016hierarchical,eysenbach2018diversity,florensa2017stochastic,hazan2019provably,LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE} and the references therein.
+develop useful priors, diverse behaviors, generalized policies/feedback, and efficient initial feedback by the human. See~\citep{LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE,hazan2019provably,daniel2016hierarchical,florensa2017stochastic,eysenbach2018diversity} and the references therein.
 
 % Preference-based Learning
-An example of this is preference-based learning, in which a robot gives two possibilities (such as movement policies) to a human. The human then chooses one which simplifies the difficult reward-selection process. It is advantageous for this approach if the robot already exhibits two ''meaningful'' movement policies, rather than the normal frenetic behavior found in newly instantiated models.
+An example of this is preference-based learning, in which a robot gives two possibilities (such as movement policies) to a human. The human then chooses one which simplifies the difficult reward-selection process. It is advantageous for this approach if the robot already exhibits two "meaningful" movement policies, rather than the normal frenetic behavior found in newly instantiated models.
 More generally, judging the consistency of an already trained model is easier because it has progressed past the initial noise of random initialization and hopefully shows meaningful relations. \citet{LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE} gives an example for the application of pre-training and HITL feedback by applying their PEBBLE framework, and show how pre-training can be used to increase the sample/request efficiency.
 % lifelong learning
 Generally, approaches like transfer learning and lifelong learning for RL agents seem promising, since they also alleviate the problem of the noisy warm-up phase of RL \citep{taylor2009transfer,yang2021efficient}. \citet{AzarLazaricBrunskill:2013:LifelongLearning} propose a method to facilitate lifelong learning across tasks and more robust learning from historical data.
 
 \subsection{Interpretability}
 \label{subsec:interpretability}
 
 % Combination with Pretraining allows to judge  ------------------------------------------------------
-A central approach to imbuing agents with explainability (in the broader sense) is to make them interpretable ---that is, providing a inherently understandable AI solution. Interpretable models in combination with a pre-trained or otherwise initialized system facilitate the judgment of the model in a fleshed-out and its decision making.
+A central approach to imbuing agents with explainability (in the broader sense) is to make them interpretable - that is, providing a inherently understandable AI solution. Interpretable models in combination with a pre-trained or otherwise initialized system facilitate the judgment of the model in a fleshed-out and its decision making.
 
 %s Structural biases+ core knowledge+ innate reasoning = transparent models and decisions -------------
 \citet{RoyEtAl:2021:RLRoboticsChallenges} enumerate several approaches that would translate to more interpretable models. For one, embedding core knowledge into models (like physical constraints) could provide agents with innate reasoning capabilities, which would then facilitate checking this reasoning \citep{HaSchmidhuber:2018:CoreKnowledgeWorldModels}. A second aspect is the use of compositional language, which could facilitate a high-level understanding of the concepts the model learned. \citet{Koditschek:2021:RoboticsCompositionalLanguage} suggests that the use of model composition and with it compositional language are key elements for embodied intelligence.
 % Representative languages ----------------------------------------------------------------------------
-Additionally, a representative language could allow abstractive reasoning and with that a rigorous generalization. This can be inspired by graph neural networks, natural language, and attention mechanisms in combination with sys1/sys2 separation \citep{RoyEtAl:2021:RLRoboticsChallenges}, and further the inherent understandability of the learned model.
+Additionally, a representative language could allow abstractive reasoning and with that a rigorous generalization. This can be inspired by Graph Neural Networks, Natural Language, and Attention Mechanisms in Combination with Sys1/Sys2 separation \citep{RoyEtAl:2021:RLRoboticsChallenges}, and further the inherent understandability of the learned model.
 % Benefit of this at the example of adversarial images -----------------------------------------------
-The advantage of a combination of innate reasoning in combination with understandable language could allow an intuitive understanding of the model. The advantage of this can be best seen in comparison with the large challenge of adversarial attacks, for example on image recognition approaches. Here, a central problem lies in the fact that the learned (and often highly performing) models focus on very different aspects than we do, and ``understand'' images on a fundamentally different level \citep{ChakrabortyEtAl:2021:SurveyAdversarialAttacks}. This of course in turn prevents humans from understanding the model and its decision-making process without the help of other tools.
+The advantage of a combination of innate reasoning in combination with understandable language could allow an intuitive understanding of the model. The advantage of this can be best seen in comparison with the large challenge of adversarial attacks, for example on image recognition approaches. Here, a central problem lies in the fact that the learned (and often highly performing) models focus on very different aspects than we do, and "understand" images on a fundamentally different level \citep{ChakrabortyEtAl:2021:SurveyAdversarialAttacks}. This of course in turn prevents humans from understanding the model and its decision-making process without the help of other tools.
 
 % Opportunities - think about new ways of interacting ------------------------------------------------
-A final aspect of transparency is to think about the opportunities embodied intelligence presents us with. \citet{RoyEtAl:2021:RLRoboticsChallenges} encourage us to think about other forms of sensors, sensor-fusion, and new components to enable new forms of interaction and application areas and ways if learning (refer to \ref{sec:ChallengesRL} for a perspective on the respective challenges). Another direction involves human teaching \citep{kulick2013active} or programs \citep{PenkovR19,sun2019program} to guide the agent to learn symbolic structure or representations of the task, which greatly reduces the task complexity. In \citet{kulick2013active} for example, a human teaches a robot symbols to abstract the geometric information of objects, where the robot learns to maximize the information gain about the symbol to be learned in an active learning manner. \citet{PenkovR19} learn symbolic representations by mapping the perceived symbols to output actions following a beforehand program given by humans. In this way, this paper learns transferable symbolic representations which improve its generalization abilities.
+A final aspect of transparency is to think about the opportunities embodied intelligence presents us with. \citet{RoyEtAl:2021:RLRoboticsChallenges} encourage us to think about other forms of sensors, sensor-fusion, and new components to enable new forms of interaction and application areas and ways if learning. Another direction involves human teaching \citep{kulick2013active} or programs \citep{PenkovR19,sun2019program} to guide the agent to learn symbolic structure or representations of the task, which greatly reduces the task complexity. For example, in \citep{kulick2013active}, a human teaches a robot symbols to abstract the geometric information of objects, where the robot learns to maximize the information gain about the symbol to be learned in an active learning manner. \citet{PenkovR19} learn symbolic representations by mapping the perceived symbols to output actions following a beforehand program given by humans. In this way, this paper learns transferable symbolic representations which improve its generalization abilities.
 
 \subsection{Explainability}
-Some explanations for developing RL models during the development phase are necessary. A lot of contextual information should be taken into consideration when defining what constitutes a ``good” explanation for an RL model, e.g., the background knowledge and levels of expertise of the addressee of this explanation, their needs, and expectations. The explanations types are various, like visual \citep{DBLP:journals/corr/abs-1912-12191,DBLP:journals/corr/abs-1912-05743}, textual \citep{fukuchi2017autonomous,HayesShah:2017:AutonomousPolicyExplanation}, causal \citep{MadumalEtAl:2020:CausalRLCFs,Madumal:2020:DistalEF}, or decision tree explanations \citep{bastani2018verifiable}. The approach by \citet{LiuEtAl:2018:LinearModelUTrees} codifies the decision process as rules to make the feature influence and rules learned by the network more transparent. Another approach is to represent the learned DRL model with generated code blocks, as presented by \citet{VermaEtAl:2018:ProgrammaticallyInterpretableRL}. Here, a policy network is codified by learning a neural policy network and searching it for the optimal policy. This results in human-readable policies and improves generalization, but also incurs a performance hit during training. A third approach is to represent the network policies via natural language, such as \citet{AlonsoEtAl:2018:xAINLBeerClassifier}, which show an example of justifying classifications with a textual explanation of the choice made by a decision tree.
+Some explanations for developing RL models during the development phase are necessary. A lot of contextual information should be taken into consideration when defining what constitutes a ``good” explanation for an RL model, e.g., the background knowledge and levels of expertise of the addressee of this explanation, their needs, and expectations. The explanations types are various, like visual \citep{DBLP:journals/corr/abs-1912-12191,DBLP:journals/corr/abs-1912-05743}, textual \citep{HayesShah:2017:AutonomousPolicyExplanation,fukuchi2017autonomous}, causal \citep{MadumalEtAl:2020:CausalRLCFs,Madumal:2020:DistalEF}, or decision tree explanations \citep{bastani2018verifiable}. One approach \citet{LiuEtAl:2018:LinearModelUTrees} codifies the decision process as rules to make the feature influence and rules learned by the network more transparent. Another approach is to represent the learned DRL model with generated code blocks, as presented by \citet{VermaEtAl:2018:ProgrammaticallyInterpretableRL}. Here, a policy network is codified by learning a neural policy network and searching it for the optimal policy. This results in human-readable policies and improves generalization, but also incurs a performance hit during training. A third approach is to represent the network policies via natural language, such as \citet{AlonsoEtAl:2018:xAINLBeerClassifier} which show an example of justifying classifications with a textual explanation of the choice made by a decision tree.
 
 %Policy Querying
 Another approach is policy querying.
-Furthermore, the subset of policy querying approaches that allow looking into questions like ``when do you do X'' can be used for this phase.
-An example of this is given by \citet{HayesShah:2017:AutonomousPolicyExplanation}, which generates a summary of a ``when do you do X?'' type question in natural language to explain the actions of an agent. An important addition to this approach is the use of counterfactuals (see \citet{EvansEtAl:2021:ExplainabilityParadox} for a more thorough assessment of the importance of counterfactuals). \citet{MadumalEtAl:2020:CausalRLCFs} learn a structural causal model for RL agents, which is in turn used to generate explanations of taken actions (see \ref{sec:InteractiveLearning}. This approach also allows to respond to queries of counterfactuals, that is: ``why did you not do Y?'', which is shown by the authors to produce satisfactory explanations and with that increase user trust.
+Furthermore, the subset of policy querying approaches that allow looking into questions like ``when do you do X" can be used for this phase.
+An example of this is given by \citet{HayesShah:2017:AutonomousPolicyExplanation}, which generates a summary of a ``when do you do X?" type question in natural language to explain the actions of an agent. An important addition to this approach is the use of counterfactuals (see \citet{EvansEtAl:2021:ExplainabilityParadox} for a more thorough assessment of the importance of counterfactuals). \citet{MadumalEtAl:2020:CausalRLCFs} learn a structural causal model for RL agents, which is in turn used to generate explanations of taken actions. This approach also allows to respond to queries of counterfactuals, that is: ``why did you not do Y?", which is shown by the authors to produce satisfactory explanations and with that increase user trust.
 
 %Causal Approaches
 Finally, approaches in causal learning can help with understanding the underlying model and going beyond interpretability toward explainability.
-An example of such xAI methods is probabilistic graphical models (PGM), which help construct causal models and are often applied to graph neural networks (GNN) \citep{Saranti:2019:LearningCompetencePGMs}. grpah neural networks are especially suitable for explainability methods in this stage since they allow easier and more direct visualization of critical components. \citet{Vu:2020:PGMExplainer} for example supports the informed creation of a causal model by identifying essential graph components and then generating PGMs approximating that prediction. This can help identify cause and effect in neural networks and determine cause and effect relations.
-
-\subsection{Further considerations}
-
-
-
-
-%\subsection{Conclusion}
-
-\noindent To \textbf{summarize} the development phase, we refer to Table \ref{table:Explanations_table}. The model explanations should be comparable against each other, allowing the user to understand the differences between tested architectures. Explanations should also be either fast to compute but shallow, or with extensive coverage but can then afford to be slower (both to compute and understand). Humans are involved at this step for defining the problem, overall model design as well as specification of state space, action space and reward function and evaluation metrics of the agent. Possible (xAI) techniques for HITL are pre-training as a way to strengthen overall performance, human-readable model policies and causal models for explainability. 
-
-In the next section, we talk about the subsequent step of training the agent in an interactive fashion.
+An example of such xAI methods is Probabilistic Graphical Models (PGM), which help construct causal models and are often applied to Graph Neural Networks (GNN) \citep{Saranti:2019:LearningCompetencePGMs}. Graph Neural Networks are especially suitable for explainability methods in this stage since they allow easier and more direct visualization of critical components. \citet{Vu:2020:PGMExplainer} for example supports the informed creation of a causal model by identifying essential graph components and then generating PGMs approximating that prediction. This can help identify cause and effect in neural networks and determine cause and effect relations.
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \section{Agent Learning}
 \label{sec:AgentLearning}
 
 After the underlying model has been developed, it is trained by and with a human teacher.
 % Steps for Developing Agent Learning
-%Finding and determining the appropriate role for the human in the HITL interaction should be the first step in developing an optimal mechanism for interaction. We propose that during this teaching process, the focus should be on providing interpretable inputs to the teacher/user, so that they can gain an understanding of how the agent perceives the world.
-Finding and determining the appropriate role for the HITL interaction should be the first step in developing an optimal mechanism for interaction. We propose that the focus should be on providing interpretable information or clear explanations to the teacher/user during this teaching process. Hence, they understand how the agent perceives the world and provide better feedback.
+Finding and determining the appropriate role for the human in the HITL interaction should be the first step in developing an optimal mechanism for interaction. We propose that during this teaching process, the focus should be on providing interpretable inputs to the teacher/user, so that they can gain an understanding of how the agent perceives the world.
+
 \subsection{Requirements}
 % Special Requirements
 % Use xAI inputs since they are fast and easy
 For one, this step is the first where a novice user interacts with the RL agent, which imposes certain requirements for a reduced complexity of the explanation. This also supports the interactivity of training, which necessitates rapid explanations for the sake of fluency. For that, interpretable inputs satisfy this requirement, since they give rapid introspection into what the agent perceives.
 
 % Lifted Requirements
 % Is shallow, but thats okay since we take care of this in other stages
-The drawbacks of a more shallow introspection into the model is alleviated by the testing in other stages, since during development and the explicit evaluation phase extra care is taken for a thorough analysis. Fundamental errors of the model should be taken care of during the development phase, while hidden biases introduced during training are in focus during the evaluation phase.
+The drawbacks of a more shallow introspection into the model is alleviated by the testing in other stages, since during development and the explicit testing phase extra care is taken for a thorough analysis. Fundamental errors of the model should be taken care of during the development phase, while hidden biases introduced during training are in focus during the testing phase.
 
 We furthermore emphasize that even suboptimal explanations by human teachers are better than none. Current literature focuses on agents using human advice during the learning phase, leveraging humans' \emph{a priori} knowledge. Yet, even though the human decisions could be less accurate, \citet{Zhang:2020:human_out_loop} demonstrate that agents will learn sub-optimal policies if they ignore human advice. 
 
 \subsection{Interactive Learning}
-As stated in Subsection~\ref{sec:InteractiveLearning}, interactive RL uses human feedback to reduce sample efficiency, sim-to-real transfer problems, generalization, etc. Like any interaction, interactive RL requires a level of agent-human understanding, and one effective way to improve communication involves explaining one's and others' behavior~\citep{de:17}. Therefore, some recent approaches augment interactive RL using explainability techniques. For instance, researchers found that most people training an AI agent assume that their behavior reveals their knowledge \citep{habibian:21}. Hence, some approaches account for this human belief by making the robot behavior interpretable. A learning robot may query the trainer to learn the true reward function. The robot then selects different behaviors and asks people about their preferences. \citet{habibian:21} study the influence of robots' questions on how their trainers perceive them. In their approach, the robot chooses informative questions that simultaneously reveal its learning. Compared to other approaches that do not account for human perception, \citet{habibian:21} found out that people prefer revealing and informative questions since they find these questions easier to answer, more revealing about the robot's learning, and better focused on uncertain aspects of the task. A follow-up study determined that accounting what the human learns from the robot's learning helps decide which features are still unclear to the robot and whether the robot is ready to be deployed. 
+As stated in Subsection~\ref{IRLBackground}, interactive RL uses human feedback to reduce sample efficiency, sim-to-real transfer problems, generalization, etc. Like any interaction, interactive RL requires a level of agent-human understanding, and one effective way to improve communication involves explaining one's and others' behavior~\citep{de:17}. Therefore, some recent approaches augment interactive RL using explainability techniques. For instance, researchers found that most people training an AI agent assume that their behavior reveals their knowledge \citep{habibian:21}. Hence, some approaches account for this human belief by making the robot behavior interpretable. A learning robot may query the trainer to learn the true reward function. The robot then selects different behaviors and asks people about their preferences. \citet{habibian:21} study the influence of robots' questions on how their trainers perceive them. In their approach, the robot chooses informative questions that simultaneously reveal its learning. Compared to other approaches that do not account for human perception, \citet{habibian:21} found out that people prefer revealing+informative questions since they find these questions easier to answer, more revealing about the robot's learning, and better focused on uncertain aspects of the task. A follow-up study determined that accounting what the human learns from the robot's learning helps decide which features are still unclear to the robot and whether the robot is ready to be deployed. 
 
-One way to enhance cooperation between two parties is to understand how a partner behaves to predict future behavior. \citet{fukuchi2017autonomous} proposed a method to explain an agent's future behavior to its trainer while using the same expressions used by the trainer. The agent selects the phrases by assuming that a higher reward means the agent correctly followed the advice. A later work applies the approach to agents that change policies dynamically, i.e., agents under training~\citep{fukuchi2017application}. While explanations usually assume a human explainee, interactive RL may require giving explanations to the agent. One common way of providing feedback is to evaluate agent actions as positive or negative \citep{arakawa:18,Knox:2008:TAMER,knox:13,macglashan2017interactive}. However, this limited feedback could improve if the trainer explains why specific actions are wrong. \citet{guan2020explanation} augment the binary evaluative feedback with visual explanations using saliency maps from humans. In addition to improving the agent's sample efficiency, the approach also reduces the human input required.  Likewise, \citet{Karalus:2021:HITL-counterfactuals} enhance evaluative feedback but this time using counterfactual explanations. In case of negative feedback, the humans can communicate to the agent that the feedback would have been positive \emph{if} action $a$ was performed in a different state $s'$. The authors limit the counterfactual feedback only to the negative reward cases where they think they will benefit most. The counterfactual feedback based on both actions and states yielded significant improvements in convergence speed. 
+One way to enhance cooperation between two parties is to understand how a partner behaves to predict future behavior. \citet{fukuchi2017autonomous} proposed a method to explain an agent's future behavior to its trainer while using the same expressions used by the trainer. The agent selects the phrases by assuming that a higher reward means the agent correctly followed the advice. A later work applies the approach to agents that change policies dynamically, i.e., agents under training~\citep{fukuchi2017application}. While explanations usually assume a human explainee, interactive RL may require giving explanations to the agent. One common way of providing feedback is to evaluate agent actions as positive or negative \citep{Knox:2008:TAMER,knox:13,arakawa:18,macglashan2017interactive}. However, this limited feedback could improve if the trainer explains why specific actions are wrong. \citet{guan2020explanation} augment the binary evaluative feedback with visual explanations using saliency maps from humans. In addition to improving the agent's sample efficiency, the approach also reduces the human input required.  Likewise, Karalus \emph{et al.}  enhance evaluative feedback but this time using counterfactual explanations. In case of negative feedback, the humans can communicate to the agent that the feedback would have been positive \emph{if} action $a$ was performed in a different state $s'$. The authors limit the counterfactual feedback only to the negative reward cases where they think they will benefit most. The counterfactual feedback based on both actions and states yielded significant improvements in convergence speed. 
 
-Another example for the application of counterfactuals is provided by \citet{Pearl:2009:Causality}, which use dynamical structural causal models (DSCM) as their framework to explicitly model the differences in capabilities of the agent and the human operator as the world states evolve. In this framework, the agent views the human feedback as the intended action and adjusts it (using counterfactual reasoning), if the action is sub-optimal. A trade-off between autonomy and optimality is demonstrated, meaning that fully autonomous agents are likely to be sub-optimal and could only achieve optimality if they receive critical feedback from their human operators. The counterfactual approach proposed by the authors improves on standard methods even when human advice is imperfect.
+Another example for the application of counterfactuals is provided by \citep{Pearl:2009:Causality}, which use Dynamical Structural Causal Models (DSCM) as their framework to explicitly model the differences in capabilities of the agent and the human operator as the world states evolve. In this framework, the agent views the human feedback as the intended action and adjusts it (using counterfactual reasoning), if the action is sub-optimal. A trade-off between autonomy and optimality is demonstrated, meaning that fully autonomous agents are likely to be sub-optimal and could only achieve optimality if they receive critical feedback from their human operators. The counterfactual approach proposed by the authors improves on standard methods even when human advice is imperfect.
 
 \subsection{Further Considerations}
 % Different roles for 
 \citet{WuEtAl:2021:HITLMLSurvey} propose that a human can take different roles for interaction with RL agents, such as a Supervisor, Controller, Assistant, Collaborateur or Impactfactor. This encourages it to take into account how the collaboration is framed, and what it entails, for developing an efficient teaching approaches. A learning agent will possibly interact with the designer, trainer, and final user. Therefore, it is important to consider expert to na\"ive collaborators. For instance, tools for visual explanations using typical data visualization techniques such as bars maybe useful for people with some scientific background; however, they add mental load to others~\citep{anderson:20}. 
  
+
 %Genneration 2 ideas
 While giving feedback speeds up agent learning, it can also cause fatigue or boredom to human trainers \citep{akalin:21}, which reduces the advice quality and frequency, resulting in a diminished cumulative reward. Additionally, \citet{macglashan2017interactive} showed that human feedback is dependent on the agent’s current policy.  Transparency might also arise during the training of an agent via human reward, causing the teacher to give incorrect feedback~\citep{knox:13}. These refer to situations where the agent took an unexpected action to satisfy some safety constraints, the human trainer being oblivious of such issues. Hence, explaining the agent’s policy to the trainer will help them to give better and useful feedback and they would be able to reason out about the agent’s unexpected choices. Explanation of the agent’s policy could also enable the human to provide targeted demonstration or advice in the regions where the agent policy is most uncertain or unstable.  Letting the trainer know how much of the advice provided was used compared to environment exploration could help the trainer decide when and where to provide feedback. Moreover, two-way communication where trainers explain the reason for their feedback and agents demonstrate where they applied the advice could undoubtedly provide a better experience or motivation to the trainer while improving the agent’s learning. Further, human trainers tend to give more positive feedback, and the learning agent should be able to inherently accommodate this feedback bias. Making the agent's assumptions transparent to the trainer can improve the overall process.RL agents who learn from human demonstration, imitation, or by querying the trainer preferences can inherit biased human behavior. xAI in that case can also shed light on the biases of the model before deployment.
 
 % sweet spot for ideal interaction
-\citet{WuEtAl:2021:HITLMLSurvey} state that the ideal interaction for HITL would be fluent, performant and reliable. For systems geared at performance, the interaction is usually framed as collaboration, while a focus on reliability favors the role of supervisor for the human. For fluency however, new roles of interaction are proposed and discussed, which would also require new kinds of interfaces. An agent could furthermore integrate implicit, empathic feedback from a human in the form of gestures, vocalizations and facial expressions as shown by \citet{CuiEtAl:2020:EMPATHICFrameworkHumanFeedback}, which enables a more intuitive interaction and richer feedback from human to learning agent. To leverage such feedback effectively, appropriate user interfaces should be developed and the underlying model should be able to process multi-modal data such as speech, image etc. Another form of implicitly improving feedback is the approach presented by \citet{PengEtAl:2016:AdaptingAgentSpeed}, which makes the agent move slower if it is uncertain, enabling the human to provide feedback where it is most useful with an intuitive cue. Identifying and leveraging useful implicit feedbacks from humans would facilitate agent learning by moving beyond what the human explicitly mentions as a teacher.
-
-Ultimately, we propose that Agent Learning approaches should consider different approaches into their HITL framework, rather than forcing one specific technique. This should allow to better determine the mentioned sweetspot of interaction. \citet{WuEtAl:2021:HITLMLSurvey} show how their HIPPOGym Environment facilitates Human Teacher approaches, which can be in the form of preference-based learning (with positive/negative feedbacK) or imitation learning (giving demonstrations to agent).\\
+\citet{WuEtAl:2021:HITLMLSurvey} state that the ideal interaction for HITL would be fluent, performant and reliable. For systems geared at performance, the interaction is usually framed as collaboration, while a focus on reliability favors the role of supervisor for the human. For fluency however, new roles of interaction are proposed and discussed, which would also require new kinds of interfaces.
 
-%\subsection{Conclusion}
+Ultimately, we propose that Agent Learning approaches should consider different approaches into their HITL framework, rather than forcing one specific technique. This should allow to better determine the mentioned sweetspot of interaction. \citet{WuEtAl:2021:HITLMLSurvey} show how their HIPPOGym Environment facilitates Human Teacher approaches, which can be in the form of preference-based learning (with positive/negative feedbacK) or imitation learning (giving demonstrations to agent). The successive application of first imitation-learning to build fundamental behavior, followed by preference-based learning to finetune the actions seems most sensible at the given time. This could then be enhanced by enabling querying approaches to get most value out of teaching sessions in different environments.
 
-% The successive application of first imitation-learning to build fundamental behavior, followed by preference-based learning to finetune the actions seems most sensible at the given time. This could then be enhanced by enabling querying approaches to get most value out of teaching sessions in different environments. 
+% % Different roles for 
+% \citet{WuEtAl:2021:HITLMLSurvey} propose that a human can take different roles for interaction with RL agents, such as a Supervisor, Controller, Assistant, Collaborateur or Impactfactor. This encourages it to take into account how the collaboration is framed, and what it entails, for developing an efficient teaching approaches.
+% % sweet spot for ideal interaction
+% \citet{WuEtAl:2021:HITLMLSurvey} state that the ideal interaction for HITL would be fluent, performant and reliable. For systems geared at performance, the interaction is usually framed as collaboration, while a focus on reliability favors the role of supervisor for the human. For fluency however, new roles of interaction are proposed and discussed, which would also require new kinds of interfaces.
 
-
-\noindent To \textbf{summarize} the agent learning phase as per Table \ref{table:Explanations_table}, explanations should provide the user with interpretable inputs and allow a rich interaction to better understand model behavior. This also requires explanations to be in the language of the domain expert, facilitating a productive ``Human as Teacher'' interaction. The human is involved at this stage to provide evaluative feedback, give advice and preferences to the agent, and provide demonstrations for complicated tasks. Main explainability techniques are those explaining the agent perceptions and evaluating behavior, for example with counterfactuals. Explainability is interactive at this stage, and RL experts as well as domain experts are involved.
-
-In the next section, we talk about the subsequent step of thoroughly evaluating the learned policies and the emerging agent behavior. 
+% Ultimately, we propose that Agent Learning approaches should consider different approaches into their HITL framework, rather than forcing one specific technique. This should allow to better determine the mentioned sweetspot of interaction. \citet{WuEtAl:2021:HITLMLSurvey} show how their HIPPOGym Environment facilitates Human Teacher approaches, which can be in the form of preference-based learning (with positive/negative feedbacK) or imitation learning (giving demonstrations to agent). The successive application of first imitation-learning to build fundamental behavior, followed by preference-based learning to finetune the actions seems most sensible at the given time. This could then be enhanced by enabling querying approaches to get most value out of teaching sessions in different environments.
 
 \section{Model Evaluation}
 \label{sec:Evaluation}
 
-In this section, we detail the requirements of the agent evaluation step, and how techniques like policy summarization, graph-based explanation and other approaches can be used to facilitate this inspection.
 %Safety is especially important with HITL robot interaction
-For success of human-robot-teamwork, safety is essential. The robot must meet innate expectations of human to be predictable and safe, and communicate its intentions \citep{EderHarperLeonards:2014:HITLRoboticsSafetyAssurance}. To ensure this, a trained system has to be tested extensively. It is important to make a distinction between errors in the underlying model, and errors learned during training. In the Model Deployment phase described in Section \ref{sec:Deployment}, underlying errors to the model architecture should be discovered and fixed. Therefore, the phase of Model Evaluation can focus on discovering errors acquired (or becoming apparent) during training, and ultimately ensure a safe decision-making process.
+For success of human-robot-teamwork, safety is essential. The robot must meet innate expectations of human to be predictable and safe, and communicate its intentions \citep{EderHarperLeonards:2014:HITLRoboticsSafetyAssurance}. To ensure this, a trained system has to be tested extensively. It is important to make a distinction between errors in the underlying model, and errors learned during training. In the section of "Model Development", the underlying errors have to be discovered and fixed. Therefore, this section can focus on discovering errors acquired (or becoming apparent) during training, and ultimately ensure a safe decision-making process.
 
 % Shortcut Learning
 This type of acquired errors becomes apparent in various forms. One is shortcut learning, where a model finds undesired shortcuts in the training data instead of learning the desired concept. This often in turn prevents a generalization, since just the shortcut has been learned, which is often not present in the application context. Examples for this are enumerated by \citet{GeirhosEtAl:2020:ShortcutLearningDNN}, like an algorithm that rather learns the hospital token embedded in an image than the targeted signs of pneumonia on X-ray images.
 % Adversarial Attacks
 Another symptom of errors acquired during training are adversarial attacks, which show that the model did not learn the desired concept, but rather invisible patterns in the image \citep{GoodfellowShlensSzegedy:2014:AdversarialExamples}. There are several approaches to reduce the attack surface for adversarial attacks with optimizations in the training process, but we will focus on the underlying issue of models failing to learn concepts.
 
 % Focus on decision making process
 Both issues show why it is important to examine the behavior of the trained model. We propose that at this stage of the reinforcement learning pipeline, the focus should be on the decision making process of the model, as this reflects the learned behavior. This process can be made tangible with approaches like policy summarization, graph-based explanations and causal models.
 
 % Overview
     % 1. Topics: How to ensure safe decisions and prevent a deterioration of properly pre-trained models
     % 2. Focus: Interpretable decision-making (policies and value-functions)
     % 3. xAI: Policy Summarization (as text, code,...), Graph-based explanations (trees, DAGs), Causal Models
 
 \subsection{Requirements}
 % Considerations and Requirements
 Of course, this part of the interaction process also requires special considerations. One is that the xAI approaches have to work with large models and complex decision-making processes. This, for example, makes the use of text- or rule-based approaches more challenging, since they might, for example, be useful when producing one page of output, while parsing and understanding many pages of model policy explanations will become prohibitive. In a similar vein, visual approaches like trees or DAGs in general should not exceed a certain size to still be useful. 
 %size and complexity
-This is supported by \citet{WellsBednarz:2021:xAIRLSurvey}, which find that the authors of several xAI approaches identify the scaling of their approaches as a major challenge, which also shows why many xAI approaches are only applied to toy examples. This applies, for example, to \citet{TabrezHayes:2019:xRLTextualExplanations}, where textual explanations are used to provide insight into which constraint will be violated, and \citet{HayesShah:2017:AutonomousPolicyExplanation}, which generates textual rules for providing insights into controller policies.
+This is supported by \citet{WellsBednarz:2021:xAIRLSurvey}, which finds that the authors of several xAI approaches identify the scaling of their approaches as a major challenge, which also shows why many xAI approaches are only applied to toy examples. This applies, for example, to \citet{TabrezHayes:2019:xRLTextualExplanations}, where textual explanations are used to provide insight into which constraint will be violated, and \citet{HayesShah:2017:AutonomousPolicyExplanation}, which generates textual rules for providing insights into controller policies.
 %must be expert-readable
 Another consideration is that at this stage, the model should provide explanations which are understandable to the domain experts. It will often be the case that only the domain expert, instead of the developer, can judge whether a learned policy is consistent, which makes it a requirement that the domain expert can evaluate it.
 %must be comparable
 A final consideration is that the provided explanations should be able to highlight differences in the learned behavior with regard to a newly initialized or only pre-trained model, in order to gain an understanding of what the agent actually learned during the different steps of the training process.
 
 %Lifted constraints
 Other requirements can, however, be reduced. The rapid computation of the xAI method is no longer required since the focus now lies on a thorough evaluation, which can take longer to assess.
 
 \subsection{Policy Summarization}
 % Policy Summarization
 Policy summarization approaches focus on showing and explaining the model policies to the user. We referred to examples that codify the model decision process as rules \citep{LiuEtAl:2018:LinearModelUTrees}, as code blocks \citep{VermaEtAl:2018:ProgrammaticallyInterpretableRL} or via natural language \citep{AlonsoEtAl:2018:xAINLBeerClassifier}. 
 This is ideally suited for assessing a trained model and checking its policies for unexpected and undesired behavior. Depending on whether and which domain experts are included in the process, different summarization approaches are advisable. Summarizing model policies as code blocks can be intuitive for computer science and adjacent fields, but are likely inadvisable for domain experts with a non-technical background. Here, special care should be given to assessing how a model could best be summarized to be intuitively understandable for the explanation target since the additional cognitive load for understanding the explanation modality should be kept to a minimum.
 A second aspect is that the scale of the model should be considered. Ten blocks of model policy code can easily be evaluated, but hundred blocks of code will be very difficult to understand and thoroughly inspect. Here, the second approach of graph-based explanations can be a useful addition.
 
 \subsection{Graph-based explanations}
 
 Graphs-based explanations can be very helpful by providing a quick and intuitive overview of model behavior. \citet{Holzinger:2016:iML} recommends the use of Graph-based explanations for HITL systems, since they can be used to compare an experts domain knowledge intuitively with the learned model behavior. \citet{SongEtAl:2019:ExplainableGraphBasedRecommendations} show how graph-based explanations can be applied in recommender systems, a field where knowledge-graphs are often used. With their presented Ekar system, the user is shown a meaningful path within that graph on how a recommendation was formed, which helps provide effective recommendations and good explanations.
 The use of graph-based explanations can however become overwhelming for the user if the model behavior or explained decision becomes too complex. Approaches like PGExplainer by \citep{Vu:2020:PGMExplainer} focus the explanation graph on relevant parts of the decision graph and with that achieve better explanatory value.
 
 \subsection{Further Considerations}
 % Other methods
-It is also important to note that the approaches for testing are an essential part of the training loop, but should not be the only component for ensuring a safe operation. An example of how safety can be ensured is presented by \citet{XiongEtAl:2020:Robustness}, which propose using shield-based defenses, where agents learn to stay in predefined, safe boundaries during training and application and with that increase robustness.
+It is also important to note that the approaches for testing are an essential part of the training loop, but should not be the only component for ensuring a safe operation. An example of how safety can be ensured is presented by \citep{XiongEtAl:2020:Robustness}, which propose using shield-based defenses, where agents learn to stay in predefined, safe boundaries during training and application and with that increase robustness.
 
 Furthermore, approaches that estimate the model uncertainty in different scenarios can be useful. \citet{LuetjensEverettHow:2018:RLModelUncertainty} present a collision avoidance policy to provide computationally tractable and parallelizable uncertainty estimations in navigation tasks. This could be used to ensure that the model is on the one hand sufficiently confident in the test scenarios employed by the developers and discover possible blind spots. The second application could be to discover these spots and test how the model behaves when encountering them.
 
 We suggest that a combination of the methods described can help significantly by ensuring that the model has only learned desired behavior. The use of graph-based explanations is recommended as complementary to the policy summarization approach, since the summarizing provides a broad overview of model policy, which can then be further inspected by querying specific explanations. The rapidity and intuitiveness of graph-based explanations can then be a major factor to inspect the learned model policies together with domain experts.
 
-%\subsection{Conclusion}
-
-\noindent To \textbf{summarize} the model evaluation phase as per Table \ref{table:Explanations_table}, explanations at this stage should be able to scale to large trained models and be comparable to previous versions of the agent. They have to be understandable by the domain experts to allow a exhaustive comparison of the learned model behavior. Humans are involved to understand and evaluate these policies and the resulting behavior at micro-level (sensible individual decision) and on macro-level (cohesive overall behavior). Useful explainability techniques are policy-summarization and graph-based explanations, involving domain experts and RL experts in bidirectional fashon.
-
-In the next section, we discuss the final step of deploying the agent to a real-world context.
-
 \section{Agent Deployment}
 \label{sec:Deployment}
 
 % Tradeoff for explainability
-In this Section, we focus on approaches that facilitate an efficient interaction of the trained agent and its human user. This frequent and repeated interaction requires to find the delicate balance between appropriately showing explanations and not hindering the task at hand --- this tradeoff is also highlighted by \citet{AndersonBischof:2013:PerformanceGestureGuides}, which state that while initially guides can be helpful, they can also be detrimental for long-time performance and learning.
+In this section, we should focus on approaches that facilitate an efficient interaction of the trained agent and its human user. This frequent and repeated interaction requires to find the delicate balance between appropriately showing explanations and not hindering the task at hand.
 
 % ensure performance gain
-We propose that the usage of HITL agents can lead to significant performance gains, along the whole development, learning, evaluation and deployment pipeline. In this Section, we focus on approaches ensuring that those benefits actually reach the end user, focusing on issues like mental overload and distrust.
+We propose that the usage of HITL agents can lead to significant performance gains, along the whole development, learning, evaluation and deployment pipeline. In this section, we focus on approaches ensuring that those benefits actually reach the end user, focusing on issues like mental overload and distrust.
 
-\subsection{Requirements}
+\subsection{Considerations and Requirements}
 
-The xAI systems used by end-users can draw on the vast fundus of research on HCI usability. Therefore, we derive considerations and requirements from the famous ``golden rules of interface design'' \citep{ShneidermanEtAl:2016:GoldenRulesHCI}.
+The xAI systems used by end-users can draw on the vast fundus of research on HCI usability. Therefore, we derive considerations and requirements from the famous "golden rules of interface design" \citep{ShneidermanEtAl:2016:GoldenRulesHCI}.
 
 % requirements: have to be fast, both computable and understandable
-Corresponding to the goal of reducing memory load, explanations have to be easily and rapidly understandable. We aim to facilitate difficult tasks, and should avoid to further complicate the human-robot interaction with overly complex explanations. Since operators are likely to work with rapidly changing perspectives and environments, explanations should be computed in real-time to ensure they are corresponding to the current situation. Think for example of the usefulness of an autonomous car, were all explanations are provided with a lag of several seconds --- all actions which could require intervention will already have happened in such a case. 
+Corresponding to the goal of reducing memory load, explanations have to be easily and rapidly understandable. We aim to facilitate difficult tasks, and should avoid to further complicate the human-robot interaction with overly complex explanations. Since operators are likely to work with rapidly changing perspectives and environments, explanations should be computed in real-time to ensure they are corresponding to the current situation. Think for example of the usefulness of an autonomous car, were all explanations are provided with a lag of several seconds - all actions which could require intervention will already have happened in such a case. 
 
 % second branch - on demand, which can use standard xAI for decision - has to be hideable
-With regard to the rule of allowing experienced users to take shortcuts, explanations should be provided on demand or able to be deactivated if desired, as further discussed by \cite{AndersonBischof:2013:PerformanceGestureGuides}. This option is essential for preventing information fatigue and enabling a natural and efficient human-robot teamup. 
+With regard to the rule of allowing experienced users to take shortcuts, explanations should be provided on demand or able to be deactivated if desired. This option is essential for preventing information fatigue and enabling a natural and efficient human-robot teamup. 
 
 % third - predictability, errors and uncertainty
 Our third consideration draws on the ideas of simple error handling and giving the user the feeling of being in control. We propose that a HITL model should provide some means to show whether it is uncertain about a given situation or decision, for example in the form of a warning light as seen in cars. Such a mechanism would give the user a notification that something is wrong or uncertain, and allow the user to then investigate what causes this.  Along similar lines, we propose some kind of startup check sequence, again based on the warning light startup sequence of a car, where users can ensure that the system is in order and correctly understands the situational context.
 
 Opposing to the other steps of the HITL deployment, we do not propose that major requirements can be lifted at this stage. We rather suggest that this step is the most demanding of the four enumerated, since it combines constraints on computational and cognitive capacities.
 
 \subsection{Explainability}
 
 % fast approaches
 Several researchers provide examples of how real-time explanations for different use-cases could look like. \citet{RodriguezEtAl:2021:DeepCovidxAI} provide feature-based explanations for COVID-19 case predictions, while \citet{Kulkarni:2021:EducationAIDashboard} developed a classroom dashboard that gives an overview of students performance with dendograms and text-based explanations. The majority of those real-time explainability systems are data/software-based, while for the area of explanations for robotic systems, there are much fewer examples.
 Most autonomous driving-systems provide explanations in form of bounding-boxes and labels for recognized objects, which is a valid option for explaining model perception. 
-The next step is decision explanations. Here, \citet{Ben-YounesEtAl:2022:DrivingBehaviorEx} present a method where object saliency is combined with a textual explanation for an action. For example, the observed traffic light is highlighted, in combination with the textual explanation of a ``stop'' action. Such an approach is already helpful and quick to evaluate by the end user, but could for example be even further refined when using known symbols and signs instead of text along with regional highlighting 
+The next step is decision explanations. Here, \citet{Ben-YounesEtAl:2022:DrivingBehaviorEx} present a method where object saliency is combined with a textual explanation for an action. For example, the observed traffic light is highlighted, in combination with the textual explanation of a "stop" action. Such an approach is already helpful and quick to evaluate by the end user, but could for example be even further refined when using known symbols and signs instead of text along with regional highlighting 
 
 % Show intent
-A major component for trusting an agent is the predictability of the agents actions. Therefore, we suggest that xAI approaches used in the real-world focus on making the agents decision and planning transparent for the user by showing intended actions. Strictly, this does not even fall in the category of ``explainability'', since actions do not have to be explained, just announced. This strongly simplifies the requirements for such an indication, though of course HCI principles still have to be taken into account to avoid incurring too much mental load. \citet{Caltagarione:2017:DrivingPathGeneration} for example show a predicted trajectory for autonomous driving applications, which could easily be translated to other movement-based domains. 
+A major component for trusting an agent is the predictability of the agents actions. Therefore, we suggest that xAI approaches used in the real world focus on making the agents decision and planning transparent for the user by showing intended actions. Strictly, this does not even fall in the category of "explainability", since actions do not have to be explained, just announced. This strongly simplifies the requirements for such an indication, though of course HCI principles still have to be taken into account to avoid incurring too much mental load. \citet{Caltagarione:2017:DrivingPathGeneration} for example show a predicted trajectory for autonomous driving applications, which could easily be translated to other movement-based domains. 
 An open challenge is how those predictions can be communicated in other contexts than autonomous cars and with other modalities. Here, items like smartwatches, headphones or just visual indicators could provide familiar and flexible interfaces.
 
 \subsection{Error Handling}
 % Use warning lights
-We furthermore suggest that the use of ``warning light'' alerts could be beneficial, which recognize when the agent is unsure about a decision and notify the user. This could on the one hand increase the general robustness of the agents decision, and also foster human trust in the agents decision, since the user can now estimate better if the ``agent knows what it is talking about''.
+We furthermore suggest that the use of "warning light" alerts could be beneficial, which recognize when the agent is unsure about a decision and notify the user. This could on the one hand increase the general robustness of the agents decision, and also foster human trust in the agents decision, since the user can now estimate better if the "agent knows what it is talking about".
 
 % Estimate uncertainty
 Such a warning light could be based on uncertainty estimation, and becomes activated when it rises over a given threshold. \citet{JainEtAl:2021:EpistemicUncertaintyPrediction} give an example of epistemic uncertainty can be estimated to a certain degree. The introduction of such an approach could help the user with focusing on the given task and interaction with the robot, and still being in control and able to intervene when required.
 
 % Operator corrects errors
-Such an intervention approach is demonstrated by \citet{WuEtAl:2021:HITLDRLAutonomousDriving}. They allow the HITL operator to intervene when the agent makes erroneous decisions, and furthermore allow the model to learn from those interventions. 
+Such an intervention approach is demonstrated by \citep{WuEtAl:2021:HITLDRLAutonomousDriving}. They allow the HITL operator to intervene when the agent makes erroneous decisions, and furthermore allow the model to learn from those interventions. 
 % Startup
-The startup sequence approach could complement this error handling concept. \citet{LiuGuoMahmud:2021:HITLErrorDetectionFramework} for example propose an error detection framework, where the HITL operator is presented with a list of most relevant, explainable features, to detect unusual or nonsensical behavior. This could be evaluated during startup with a quick glance, and provide considerable trust benefits.\\
-
-
-%\subsection{Conclusion}
-
-\noindent To \textbf{summarize} the agent deployment phase as per Table \ref{table:Explanations_table}, the main requirement is for explanations to be understandable by the end-user. Additionally, they will be used in the field and therefore should not incur a significant overhead, either with  too much cognitive load or long and costly computation times. The human is involved as end-user deploying the agent, defining the usage context and specific agent task. Explainability can be provided in the form of a combined explainability dashboard, communicating the agent intents and actions and ways of handling uncertainty and errors in bi-directional fashion.
+The startup sequence approach could complement this error handling concept. \citet{LiuGuoMahmud:2021:HITLErrorDetectionFramework} for example propose an error detection framework, where the HITL operator is presented with a list of most relevant, explainable features, to detect unusual or nonsensical behavior. This could be evaluated during startup with a quick glance, and provide considerable trust benefits.
 
-In the next section, we first open up the discussion on propositions for general research directions which emerged in this paper, and then focus on how research could more specifically be applied to complement the requirements identified in the four steps for HITL RL deployment.
 
 \section{Discussion}
-\label{sec:discussion}
-We first discuss some general challenges in HITL RL approaches, and then treat the presented opportunities and requirements in a three-stage approach. At stage one, we discuss what is possible with current methods. These approaches are mostly discussed in the background section. Stage two contains a discussion on how current methods should be adapted to better suit the HITL requirements, applied to the Sections discussing the four steps required for deployment of HITL systems. The third stage approaches focus on those requiring further research to develop new methods and technologies.
 
-\subsection{Open Research Directions}
+We first want to discuss some general challenges in HITL RL approaches, and then treat the presented opportunities and requirements in a three-stage approach. At stage one, we discuss what is possible with current methods. These approaches can be mainly found in the background section. Stage two contains a discussion on how current methods should be adapted to better suit the HITL requirements, which is applied to the sections discussing the four steps required for deployment of HITL systems. The third generation is discussed in the Discussion section and is concerned with future generation approaches that will require new research, methods and technology.
 
-We refer back to Section \ref{challenges-for-RL} to emphasize that we consider RL a challenging problem setting that could greatly benefit from HITL approaches. We further emphasize that there is no one-size-fits-all solution for explainabiliy, and that the requirements for suitable HITL xAI differ between each phase. We do not propose a strict separation of explainability techniques in different phases, but rather recognize the suitability of certain types of xAI for each phase depending on the nature of the human involvement, the aspect of the agent in focus (e.g., model architecture, its decision making process, or its perception of the world), and the task to be tackled (e.g., developing the underlying model, adding knowledge to the agent, inspecting the learned behavior and utilizing the agent in a real-world scenario). The types of xAI approaches we recommend are informed by the explanation requirements listed in Table ~\ref{table:Explanations_table} as well as the nature of human involvement, the types of users, and the directionality of the interaction between the human and the agent.
+\subsection{Challenges}
+
+We refer back to the section "challenges for reinforcement learning" to emphasize that we consider Reinforcement Learning a challenging problem, which greatly benefits from HITL approaches.
 
-We envision that the depicted HITL RL approaches could in the future enable a human-robot team-up, greatly enhancing the productivity of a human-robot team. \citet{KhatibEtAl:1999:RihEnvironment} stated that HITL contributes experience, domain knowledge and is able to ensure the correct execution of tasks. The robot on the other hand can increase the human's capabilities in terms of force, speed and precision. Moreover, the robot should reduce human exposure to harmful and hazardous conditions. According to \citet{DeSaintsEtAl:2008:phri}, only trustworthy robots are able to work in such a team. Due to the close working environment and interaction with the human, it is of utmost importance that the robot is reliable and safe.
+We envision that the depicted HITL RL approaches could in the future enable a human-robot team-up, which greatly increases human's productivity in a human-robot team. \citet{KhatibEtAl:1999:RihEnvironment} stated that the human-in-the-loop contributes experience, domain knowledge and is able to control the correct execution of tasks. The robot on the other hand can increase the human's capabilities in terms of force, speed and precision. Moreover, the robot should reduce human exposure to harmful and hazardous conditions. According to \citep{DeSaintsEtAl:2008:phri}, only trustworthy robots are able to work in such a team. Due to the close working environment and interaction with the human, it is of utmost importance that the robot is reliable and safe.
 
 % Teamup requirements
 
-The human-robot team-up comes with challenges beyond explainability which was the focus of this work. When the agent is deployed, the trust requirement is essential, or else the agent will not be used. Humans display a tendency to anthropomorphize robots, thereby overestimating their cognitive capabilities. \citet{DeSaintsEtAl:2008:phri} argue that a user's mental model might result in a fake robot \emph{dependability}, due to certain aspects or the posture of a robot. This further exacerbates the problem of safety in human-robot collaboration. The human-robot team-up relies on the predictability of the robot's actions, testability, explainability of the policies, as well as performance increases. The performance increase should be quantifiable and clearly demonstrated. We also advise a separation between the critical deployment phase, where the agent is evaluated based on its task performance and the initial training phase, focused on developing trust.
+The human-robot teamup comes with challenges beyond the explainability as described in this paper. When the agent is deployed, the trust requirement is essential, or else the agent will not be used.  Due to the human's anthropomorphic nature, the users estimation of the robots cognitive capabilities will often be overestimated. \citet{DeSaintsEtAl:2008:phri} argue that a user's mental model might result in a fake robot 'dependability', due to certain aspects or the posture of a robot. When interacting with a robot, a human might have a wrong idea of its awareness, since it looks like a living creature. This further strengthens the problem of safety in a human-robot collaboration. The required predictability of the robots actions, testability, explainable policies, but also a performance increase to be actually useful. This performance increase should be measurable and be communicated. It is also recommendable to think about the phases of deployment, where the initial phase for building trust should be differentiated from the performance phase, where the agent is evaluated with regard to its contribution to task performance. 
 
 % xAI takeaways
 
-Most work in xAI is heavily biased by what researchers assumed to be good explanations for a given task or domain \citep{Miller:2019:xAISocialSciencesInsights}, not taking into account the preferences and expertise of the human end-users. To be effective, the models and approaches will need to adapt their language and modalities based on operators' feedback. This requires a more human-centered development \citep{PuiuttaVeith:2020:xAIRLSurvey}. To ensure that explainability methods align with users' expectations, we call for a comprehensive set of guidelines and requirements for developing xAI systems. 
-
-%\sd{A para about explanations mapped to phases -- TIANPEI}
-%\sd{Small summary of challenges with info from the table -- SRIJITA}
+As for explainability, most work in xAI is based on what researcher deems a good explanation, which is heavily biased \citep{Miller:2019:xAISocialSciencesInsights}. Most models and approaches lack understanding of the human end user, and need to adapt their language and modalities accordingly. This requires to use human-centered development \citep{PuiuttaVeith:2020:xAIRLSurvey}. To ensure that explainability methods orient themselves at the end user and take their respective step (as proposed here) into account, a comprehensive guideline with factors to consider when developing xAI systems could be created to help with this process.
 
-
-\subsection{Challenges}
+\subsection{Third Generation Challenges}
 \label{sec:ThirdGeneration}
 
-In this Section, we first sum up the specific challenges encountered in each phase. Then, we discuss the blue-sky approaches we envision for improving HITL RL with regard to those challenges.
-
-As for current challenges in the development phase, we find that there exist many xAI approaches intended for this setting. We however find a lack of interactivity and comparability of the explanations, which hinders thorough introspection. We furthermore find that current explainability approaches often lack the causality and intuitive understandability required for a thorough introspection of a newly developed model.
-
-For the agent learning phase, we find that many approaches support the \emph{Human-as-Teacher} paradigm itself, but identify a lack of explainability approaches which facilitate this interaction. More generally, xAI approaches in this phase need to evolve to support the interactivity in the HITL setting, that are currently underdeveloped.
-
-In the model evaluation phase, we identify a major challenge in the scalability of xAI approaches, that are often suited to smaller models, but fail to provide explainability for large trained models and their learned policies. We also find that few approaches allow to identify variations between different versions of a trained model, which we consider a central feature for this phase in order to conduct an exhaustive evaluation of the learned policy.
-
-For the final step of agent deployment, there are very few suitable xAI approaches. The application of current approaches is most often hindered by the failure to speak the user`s language, limited available computation, or too much complexity to be usable in a real-time context. Additionally, many HITL RL approaches fail to gain (and deserve) the user`s trust, in addition to failing to communicate uncertainty when warranted.
-
-We therefore picture the following third generation of approaches, which consist of blue-sky propositions that are currently out of reach but could lead to large improvements in HITL RL systems. Refer to Table \ref{table:Explanations_table} for an overview of the propositions discussed hereafter.
+The third generation of approaches consists of blue sky propositions, which are yet out of reach but could lead to large improvements in HITL RL systems.
 
 % Developing - Interactive, thorough and comparable model summaries
 
-We suggest to use compositional and representational language for explainability to enhance the intuitive understandability of models as mentioned by \citet{RoyEtAl:2021:RLRoboticsChallenges}.
-Furthermore, representing the model as hierarchical, graphical, or topological structure \citep{lyu2019sdrl,battaglia2018relational} is more understandable to humans rather than a traditional neural network model. Such structural models are not powerful as neural network models since their expression and computation ability are limited, so an optimal approach would be to integrate them with the model itself without losing explainability.
-We furthermore think that causal learning approaches should be adapted and integrated much more deeply into HITL approaches, since in addition to generating better explanations, they could also yield improved prediction performance as exemplified by \citet{MadumalEtAl:2020:CausalRLCFs}. Thirdly, policy querying approaches like those presented by \citet{HayesShah:2017:AutonomousPolicyExplanation} could be adapted to allow specific inquiries into model structure, and be expanded with counterfactual structures.
-We ultimately envision interactive, thorough, and comparable model summaries. Individual components of such a solution can already be found, but the simplicity of a comprehensive solution could greatly benefit such a process.
+We suggest to use compositional and representational language for explainability to enhance the intuitive understandability of models as mentioned by \citet{RoyEtAl:2021:RLRoboticsChallenges}. We furthermore think that causal learning approaches should be adapted and integrated much more deeply into HITL approaches, since they could not only help with generating better explanations, but also better prediction performance as exemplified by \citet{MadumalEtAl:2020:CausalRLCFs}. Thirdly, policy querying approaches like those presented by \citet{HayesShah:2017:AutonomousPolicyExplanation} could be adapted to allow specific inquiries into model structure, and be expanded with counterfactual structures.
+We ultimately envision interactive, thorough and comparable model summaries. Individual components of such a solution can already be found, but the simplicity of a comprehensive solution could greatly benefit such a process.
 
 
 % Teaching - Efficient training in the field, with replanning and corrections
 
-For the agent learning step, the various approaches enumerated in the first generation should be adapted further for HITL and RL contexts. An important adaptation is the development of a suite of tools to facilitate the systematic deployment and comparison of the different approaches to discover sweet-spot mixes. An example of an effective combination could be to build fundamental behavior via imitation learning, followed by fine-turning actions by preference-based learning and finally identifying and solving weak spots using querying approaches. We propose that we should strive for a solution that allows efficient HITL training in the field along with subject matter experts, enabling users to rapidly bootstrap agent behavior and support this process further with replanning and corrections. A combination of such approaches could be very efficient and fast in bringing up robust agents suited for real-world applications.
+For the agent learning step, the various approaches enumerated in the first generation should be adapted further for RL/HITL contexts. An important adaption is also the provision of a suite of tools to facilitate using different approaches and consequently find the individual sweet-spot mix of different approaches. Adapting the tools to work in a succession that builds fundamental behavior with imitation learning, followed by fine-tuning actions by preference-based learning finally identifying and solving weak spots with querying approaches is only one possible combination.
+We propose the goal of a solution that allows efficient HITL training in the field and with subject matter experts, enabling users to rapidly bootstrap agent behavior and support this process further with replanning and corrections. A combination of such approaches could be very efficient and fast in bringing up robust agents suited for real-world applications.
 
 % Testing - Thorough model decision explanations, with detailed diffs
 
-We highlighted that many explainability approaches that suffice in the development and learning phase need adaptation to the evaluation stage due to model size and complexity. Approaches like code block summarization as provided by \citet{VermaEtAl:2018:ProgrammaticallyInterpretableRL} could be extended by focusing only on relevant parts of the explanations as illustrated by \citet{Vu:2020:PGMExplainer}. Furthermore, expert-readability needs to be ensured to allow subject matter experts to help with the testing and evaluating whether the learned policies are sensible.
-Furthermore, post-hoc explanations like visual \citep{DBLP:journals/corr/abs-1912-12191,DBLP:journals/corr/abs-1912-05743} or textual \citep{fukuchi2017autonomous,HayesShah:2017:AutonomousPolicyExplanation} explanations would increase explainability.
-
-We recommend integrating various tools to help evaluate and scrutinize a model from many different viewpoints. We propose that thorough model decision explanations are essential for this process. Furthermore, the focus should be on providing explanations in such a way that they are understandable to the subject matter experts, allowing to not only debug superficial model behavior, but also check the learned routines for semantically sensible behavior. This cannot be done by the software developers alone, if it is to generate explanations in a user-specific language.
+We highlighted that many explainability approaches which suffice in the developing and learning phase need adaptation to the testing stage due to model size and complexity. Approaches like code block summarization as provided by \citet{VermaEtAl:2018:ProgrammaticallyInterpretableRL} could be extended by focusing only on relevant parts of the explanation as illustrated by \citet{Vu:2020:PGMExplainer}. Furthermore, expert-readability needs to be ensured to allow subject matter experts to help with the testing and evaluating whether the learned policies are sensible.
+We recommend focusing on a tool suite that integrates different tools which help scrutinize a model from many different viewpoints. On a underlying matter, we propose that thorough model decision explanations are essential for this process. Furthermore, the focus should be on providing explanations in such a way that they are understandable to the subject matter expert, allowing to not only debug superficial model behavior, but also check the learned routines for semantically sensible behavior. This can not be done by the software developers themselves, and affords explanations in a user-specific language.
 
 % Using - simple and fast highlight-symbol explanations, explaining abstract actions via different modalities, motor warning light and startup sequence
 
-With regard to explainability in the deployment of HITL approaches, we propose that the currently available approaches look beyond the use-cases of autonomous driving focused on visual aspects, and consider other modalities, for example the context of credit or policy computations, which requires explaining textual facts. Also, modalities beyond graphic dashboards should be considered to ensure that the full potential of HITL approaches is reached. Auditory and tactic perspectives should be explored, while visual perspectives should be explored also in different form-factors such as smartwatches, LED indicators and image projections.
-We emphasize the need for simple and fast explanations, very much unlike current approaches. We furthermore recommend to consider explaining agent actions via different modalities, such as visual indicators, but also haptic or auditory signals, aspects which are largely unexplored as of now. Finally, we envision a suite of tools equipped with active warning lights that indicate when the agent encounters difficult situations, allowing the user to trust the agent when it is within its generalization capabilities, and inform the user if that is not the case. Ultimately, a startup-sequence with different checks would allow the user to ensure that the agent is properly initialized and trustworthy.
-
-In the next section, we summarize the contents and central insights of this paper, and conclude with the visions we have for the future of HITL RL.
+With regard to explainability in the deployment of HITL approaches, we propose that the currently available approaches look beyond the scope of autonomous driving and consider use cases such as explaining textual computations (as seen in credit or policy computations). Also, modalities beside graphic dashboards should be considered to ensure that the full potential of HITL approaches is reached. Auditory and tactic perspectives should be explored, while visual perspectives should be explored also in other form-factors such as smartwatches, LED indicators and image projections.
+For one, we emphasize the need for simple and fast explanations, very much unlike the most common approaches seen today. We furthermore recommend to think about explaining agent actions via different modalities, such as visual indicators, but also haptic or auditory signals, aspects which are largely unexplored as of now. Finally, we envision a suite of tools that allows active warning lights when the agent encounters difficult situations, allowing the user to trust the agent when it is within its generalization capabilities, and communicate it if not. Ultimately, a startup-sequence with different checks would allow the user to ensure that the agent is properly initialized and help greatly with building trust.
 
 \section{Conclusion and Future Outlook}
-\label{sec:conclusion}
-In summary, we emphasise that RL is a fundamentally difficult problem setting and could benefit greatly from human-in-the-loop interactions. A human expert can contribute conceptual understanding gained through many years of experience to the task at hand, thereby significantly improving robustness as well as explainability \citep{Holzinger:2021:TrustAI}. Numerous approaches have demonstrated that RL benefits from human-centred approaches \citep{Li:2019:HumanCenteredRLSurvey,MatthewsonPilarski:2022:DesigningAndEvaluatingHCIML}.
 
-In addition, we further argue that HITL RL in particular, benefit greatly from xAI approaches. These are, after all, fundamentally human approaches, which in turn ensure successful interactions, acceptance, and trust as well as conceptual knowledge about the agents' limitations \citep{heuillet2021explainability,milani2022survey}.
+In summary, we emphasise that reinforcement learning is a fundamentally difficult problem for machine learning. Therefore, in the future, it can also benefit greatly from the advantages of a human-in-the-loop. A human expert can contribute conceptual understanding and often many years of experience to the task at hand, thus contributing not only to robustness but also to explainability \citep{Holzinger:2021:TrustAI}. Numerous approaches have confirmed that RL benefits from human-centred approaches \citep{MatthewsonPilarski:2022:DesigningAndEvaluatingHCIML,Li:2019:HumanCenteredRLSurvey}.
 
-Specifically, we identify the following phases for deploying HITL RL solutions: 1) RL model development, 2) agent learning, 3) model evaluation, and 4) agent deployment. In our work, we discuss how xAI can support each of these phases and what are some considerations for successful deployment. Thus, the HITL combination also increases productivity and enables human-robot collaboration.
+In addition, we further argue that in turn RL approaches, HITL in particular, benefit greatly from xAI approaches. These are, after all, fundamentally human approaches, which in turn ensure successful interaction, acceptance, trust however also conceptual knowledge about the agents' limitations \citep{heuillet2021explainability,milani2022survey}.
 
-Ultimately, we argue that combining RL and HITL with xAI approaches should be a main focus to increase productivity in real-world applications of RL, for example in agriculture or forestry \citep{Holzinger:2022:DigitalTrans}, and discuss different blue-sky approaches to current xAI methods. In the deployment phase, interactive, thorough and coherent model summaries can enable an agile and transparent workflow. During the agent's interactive learning, xAI approaches can enable more efficient training in the field through interactive replanning and corrections. In the evaluation phase, comprehensive explanations of model decisions can provide detailed insights into the trained model and lead to informed decisions about whether to proceed with the deployment or start another development cycle. In the deployment phase, simple and quick explanations of actions and different explanations for error handling could significantly increase user confidence in the agent and lead to more efficient collaboration.
+Specifically, we identify the following phases for deploying HITL RL solutions: 1) RL model development, 2) agent learning, 3) model evaluation, and 4) agent deployment. In our work, we discuss how xAI can support each of these phases and what needs to be considered for successful deployment. Thus, the HITL combination also increases productivity and enables human-robot collaboration.
 
-Last but not least, we propose a vision of an interactive human-robot team-up that enables new use cases for RL applications and allows both humans and robots to realize their full potential and respective strengths. Such a team-up requires strong interactive collaboration and trust between both parties, that can only be achieved through comprehensive explainability and deep and intuitive understanding of the mental model generated by the agent.
+Ultimately, we argue that combining RL and HITL with xAI approaches should be a main focus to increase productivity in real-world applications of RL, for example in agriculture or forestry \citep{Holzinger:2022:DigitalTrans}, and discuss different blue-sky approaches to current xAI methods. In the deployment phase, interactive, thorough and coherent model summaries can enable an agile and transparent workflow. During the agent's interactive learning, xAI approaches can enable more efficient training in the field through interactive replanning and corrections. In the testing phase, comprehensive explanations of model decisions can provide detailed insights into the trained model and lead to informed decisions about whether to proceed with the deployment or start another development cycle. In the deployment phase, simple and quick explanations of actions and different explanations for error handling could significantly increase user confidence in the agent and lead to more efficient collaboration.
 
+Last but not least, we propose a vision of an interactive human-robot teamup that enables new use cases for RL applications and allows both humans and robots to realise their full potential and respective strengths. Such a teamup requires strong interactive collaboration and trust between both parties, which can only be achieved through comprehensive explainability and deep and intuitive understanding of the mental model generated by the agent.
 
-\begin{comment}
 
-\section{Author Contributions}
-The following examples show how xAI frameworks and human users can interact:
-\begin{enumerate}
-\item Carl Orge Retzlaff --- Main Author
-\end{enumerate}
+\begin{comment}
 
 \section*{Abbreviations}
 
 \begin{itemize}
 
 \item AI = Artificial Intelligence
 \item CAM = Class Activation Mapping
 \item BP = Bongard Problem
 \item c-EB = contrastive Excitation Backpropagation
 \item CG = Counterfactual Graph
 \item CNN = Convolutional Neural Network
 \item CRF = Conditional Random Fields
 \item CT = Computational Tomography
 \item DF = Decision Forest
 \item DGNN = Dynamic  Graph  Neural  Network
 \item EB = Excitation Backpropagation
 \item GAN = Generative Adversarial Network
 \item GB = Guided Backpropagation
 \item GCNN = Graph Convolutional (Neural) Network
 \item GloVe = Global Vectors for Word Representation
 \item GNN = Graph Neural Network
 \item Grad-CAM = gradient-weighted Class Activation Mapping
 \item GraphSAGE = Graph Sampling \& Aggregation
 \item GRL = Graph Representation Learning
 \item HER = Hindsight Experience Replay
 \item ICG = Interaction \& Correspondence Graph
 \item LIME = Local Interpretable Model-Agnostic Explanations
 \item LSTM = Long Short-Term Memory
 \item LRP = Layer Wise Relevance Propagation
 \item MM = Multi-Modal
 \item MRI = Magnetic Resonance Imaging
 \item NAM = Node Attribution Method
 \item NIV = Node Importance Visualization
 \item OCT = Optical Coherence Tomography
 \item OGB = Open Graph Benchmark
 \item PGN = Pointer Graph Network
 \item PGM = Probabilistic Graphical Models
 \item PET = Positron Emission Tomography
 \item RL = Reinforcement Learning
 \item RW = Random Walks
 \item ReLU = Rectified Linear Unit
 \item SA = Sensitivity Analysis
 \item UI = User Interface
 \item xAI = explainable Artificial Intelligence
 \item XGNN = Explanations of Graph Neural Networks
 
 \end{itemize}
 
 \end{comment}
 
 \acks{Parts of this work have been funded by the Austrian Science Fund (FWF), Project: P-32554 ``explainable Artificial Intelligence''.}
 \newpage
 
 %\appendix
 %\section*{Appendix A.}
 %\label{}
 
 % Note: in this sample, the section number is hard-coded in. Following
 % proper LaTeX conventions, it should properly be coded as a reference:
 
 %In this appendix we prove the following theorem from
 %Section~\ref{sec:textree-generalization}:
 
 
 {
 %\bibliographystyle{IEEEtran}
 \bibliography{references}
 }
 
 
 \end{document}
\ No newline at end of file
diff --git a/references.bib b/references.bib
index bdc2624..05d6e58 100644
--- a/references.bib
+++ b/references.bib
@@ -1,2349 +1,2308 @@
 % Sort all references alphabetically, so you can easily search and organize
 % Please keep all references complete and include the doi where available (arxiv do not have it)
 % 04.03.2022
 
 %%% AAA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @inproceedings{Abbeel:2004:InverseRL,
-  title={Apprenticeship Learning via Inverse Reinforcement Learning},
+  title={Apprenticeship learning via inverse reinforcement learning},
   author={Abbeel, Pieter and Ng, Andrew Y.},
   booktitle={Proceedings of the twenty-first international conference on Machine learning},
   publisher = {Association for Computing Machinery},
   pages={1},
   year={2004},
   abstract={We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
   doi={10.1145/1015330.1015430}
 }
 
 @article{Abel:2017:AgentAgnosticHumanInTheLoopRL,
-  title={Agent-Agnostic Human-in-the-Loop Reinforcement Learning},
+  title={Agent-agnostic human-in-the-loop reinforcement learning},
   author={Abel, David and Salvatier, John and Stuhlm{\"u}ller, Andreas and Evans, Owain},
   journal={arXiv preprint arXiv:1701.04079},
   year={2017},
   abstract={Providing Reinforcement Learning agents with expert advice can dramatically improve various aspects of learning. Prior work has developed teaching protocols that enable agents to learn efficiently in complex environments; many of these methods tailor the teacher's guidance to agents with a particular representation or underlying learning scheme, offering effective but specialized teaching procedures. In this work, we explore protocol programs, an agent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is to incorporate the beneficial properties of a human teacher into Reinforcement Learning without making strong assumptions about the inner workings of the agent. We show how to represent existing approaches such as action pruning, reward shaping, and training in simulation as special cases of our schema and conduct preliminary experiments on simple domains.},
   doi = {10.48550/ARXIV.1701.04079}
 }
 
 @article{akalin:21,
-  title={Reinforcement Learning Approaches in Social Robotics},
+  title={Reinforcement learning approaches in social robotics},
   author={Akalin, Neziha and Loutfi, Amy},
   journal={Sensors},
   volume={21},
   number={4},
   pages={1292},
   year={2021},
   publisher={Multidisciplinary Digital Publishing Institute},
   abstract={This survey analyzes RL applied to social robotics. A relevant outcome is the identification of challenges in interactive learning observed in this specific domain. Human-centered explainability can learn from social robotics and help reduce some of those challenges. For instance, human teachers tend to reduce the feedback frequency as the training progresses, resulting in a diminished cumulative reward. Reducing the feedback frequency could be caused by fatigue, boredom, or the belief that the robot "remembers" and "understands" all previous feedback, requiring less advice later. Transparency issues may also arise during the training of a physical robot via human reward, causing the teacher to give incorrect feedback. Moreover, ambiguous robot behavior might affect the willingness of a human to interact again. Inexperienced users usually take more time training. Transparency or explainability could reduce confusion and help guide human trainers. Further, human trainers tend to give more positive feedback, and the learning agent should be aware of this bias. Making the agent's assumptions transparent to the trainer can improve the process.},
   doi = {10.3390/s21041292}
 }
 
 % TODO: please add doi if possible
 @inproceedings{akrour2019towards,
   title={Towards Reinforcement Learning of Human Readable Policies},
   author={Akrour, Riad and Tateo, Davide and Peters, Jan},
   booktitle={Workshop on Deep Continuous-Discrete Machine Learning},
   year={2019}
 }
 
 @article{Alber:2019:Innvestigate,
-  title={iNNvestigate Neural Networks!},
+  title={iNNvestigate neural networks!},
   author={Alber, Maximilian and Lapuschkin, Sebastian and Seegerer, Philipp and H{\"a}gele, Miriam and Sch{\"u}tt, Kristof T and Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert and D{\"a}hne, Sven and Kindermans, Pieter-Jan},
   journal={Journal of Machine Learning Research},
   volume={20},
   number={93},
   pages={1--8},
   year={2019},
   abstract={In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and predictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this shortcoming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-the-box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.}
 }
 
 @article{alharin2020reinforcement,
-  title={Reinforcement Learning Interpretation Methods: A Survey},
+  title={Reinforcement learning interpretation methods: A survey},
   author={Alharin, Alnour and Doan, Thanh-Nam and Sartipi, Mina},
   journal={IEEE Access},
   volume={8},
   pages={171058--171077},
   year={2020},
   publisher={IEEE},
   doi={10.1109/ACCESS.2020.3023394}
 }
 
 @inproceedings{AlonsoEtAl:2018:xAINLBeerClassifier,
   title={Explainable AI Beer Style Classifier.},
   author={Alonso, Jos{\'e} Maria and Ramos-Soto, Alejandro and Castiello, Ciro and Mencar, Corrado},
   url={https://ceur-ws.org/Vol-2151/Paper_S1.pdf},
   year={2018},
   booktitle={SICSA ReaLX},
   abstract={This paper describes how to build an eXplainable Artificial Intelligence (XAI) classifier for a real use case related to beer style classification. It combines an opaque machine learning algorithm (Random Forest) with an interpretable machine learning algorithm (Decision Tree). The result is a XAI classifier which provides users with a good interpretability-accuracy trade-off but also with explanation capabilities. First, the opaque algorithm acts as an “oracle” which finds out the most plausible output. Then, we generate a textual explanation of the given output which emerges as an automatic interpretation of the inference process carried out by the related decision tree, if the outputs from both classifiers coincide. We apply a Natural Language Generation Approach to generate the textual explanations}
 }
 
 @inproceedings{amir2016interactive,
   author = {Amir, Ofra and Kamar, Ece and Kolobov, Andrey and Grosz, Barbara},
   title = {Interactive Teaching Strategies for Agent Training},
   booktitle = {In Proceedings of IJCAI 2016},
   year = {2016},
   month = {May},
   abstract = {Agents learning how to act in new environments can benefit from input from more experienced agents or humans. This paper studies interactive teaching strategies for identifying when a student can benefit from teacher-advice in a reinforcement learning framework. In student-teacher learning, a teacher agent can advise the student on which action to take. Prior work has considered heuristics for the teacher to choose advising opportunities. While these approaches effectively accelerate agent training, they assume that the teacher constantly monitors the student. This assumption may not be satisfied with human teachers, as people incur cognitive costs of monitoring and might not always pay attention. We propose strategies for a teacher and a student to jointly identify advising opportunities so that the teacher is not required to constantly monitor the student. Experimental results show that these approaches reduce the amount of attention required from the teacher compared to teacher-initiated strategies, while maintaining similar learning gains. The empirical evaluation also investigates the effect of the information communicated to the teacher and the quality of the student’s initial policy on teaching outcomes.}
 }
 
-@inproceedings{AndersonBischof:2013:PerformanceGestureGuides,
-author = {Anderson, Fraser and Bischof, Walter F.},
-title = {Learning and Performance with Gesture Guides},
-year = {2013},
-isbn = {9781450318990},
-publisher = {Association for Computing Machinery},
-address = {New York, NY, USA},
-url = {https://doi.org/10.1145/2470654.2466143},
-doi = {10.1145/2470654.2466143},
-abstract = {Gesture-based interfaces are becoming more prevalent and complex, requiring non-trivial learning of gesture sets. Many methods for learning gestures have been proposed, but they are often evaluated with short-term recall tests that measure user performance, rather than learning. We evaluated four types of gesture guides using a retention and transfer paradigm common in motor learning experiments and found results different from those typically reported with recall tests. The results indicate that many guide systems with higher levels of guidance exhibit high performance benefits while the guide is being used, but are ultimately detrimental to user learning. We propose an adaptive guide that does not suffer from these drawbacks, and that enables a smooth transition from novice to expert. The results contrasting learning and performance can be explained by the guidance hypothesis. They have important implications for the design and evaluation of future gesture learning systems.},
-booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
-pages = {1109–1118},
-numpages = {10},
-keywords = {learning, guides, evaluation, gestures},
-location = {Paris, France},
-series = {CHI '13}
-}
-
 @article{anderson:20,
-  title={Mental Models of Mere Mortals with Explanations of Reinforcement Learning},
+  title={Mental models of mere mortals with explanations of reinforcement learning},
   author={Anderson, Andrew and Dodge, Jonathan and Sadarangani, Amrita and Juozapaitis, Zoe and Newman, Evan and Irvine, Jed and Chattopadhyay, Souti and Olson, Matthew and Fern, Alan and Burnett, Margaret},
   journal={ACM Transactions on Interactive Intelligent Systems (TiiS)},
   volume={10},
   number={2},
   pages={1--37},
   year={2020},
   publisher={ACM New York, NY, USA}, 
   abstract={How should reinforcement learning (RL) agents explain themselves to humans not trained in AI? To gain insights into this question, we conducted a 124-participant, four-treatment experiment to compare participants’ mental models of an RL agent in the context of a simple Real-Time Strategy (RTS) game. The four treatments isolated two types of explanations vs. neither vs. both together. The two types of explanations were as follows: (1) saliency maps (an “Input Intelligibility Type” that explains the AI’s focus of attention) and (2) reward-decomposition bars (an “Output Intelligibility Type” that explains the AI’s predictions of future types of rewards). Our results show that a combined explanation that included saliency and reward bars was needed to achieve a statistically significant difference in participants’ mental model scores over the no-explanation treatment. However, this combined explanation was far from a panacea: It exacted dis- proportionately high cognitive loads from the participants who received the combined explanation. Further, in some situations, participants who saw both explanations predicted the agent’s next action worse than all other treatments’ participants.},
   doi = {10.1145/3366485}
 }
 
 @inproceedings{andreas2017modular,
-  title={Modular Multitask Reinforcement Learning with Policy Sketches},
+  title={Modular multitask reinforcement learning with policy sketches},
   author={Andreas, Jacob and Klein, Dan and Levine, Sergey},
   booktitle = {Proceedings of the 34th International Conference on Machine Learning},
   pages={166--175},
   year={2017},
   editor = {Precup, Doina and Teh, Yee Whye},
   volume = {70},
   series = {Proceedings of Machine Learning Research},
   organization={PMLR},
   abstract = {We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them—specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor–critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.}
 }
 
 @article{Andrychowicz:2017:HERHindsightExperienceReplay,
-  title={Hindsight Experience Replay},
+  title={Hindsight experience replay},
   author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
   journal={arXiv preprint arXiv:1707.01495},
   year={2017},
   abstract={Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
   doi = {10.48550/ARXIV.1707.01495}
 }
 
-@incollection{araiza2019safe,
-   year = {2019},
-   author = {Araiza-Illan, Dejanira and Eder, Kerstin},
-   title = {Safe and Trustworthy Human-Robot Interaction},
-   booktitle = {Humanoid Robotics: A Reference},
-   editor = {Goswami, Ambarish and Vadakkepat, Prahlad},
-   publisher = {Springer},
-   address = {Dortrecht},
-   pages = {2397--2419},
-   abstract = {To be genuinely useful, robotic assistants must be both smart and powerful. This makes them potentially dangerous, and we must consider safety and trustworthiness as primary design goals for human-assistive robots. This chapter is focused on techniques that can be used to gain confidence in the safety of code used to control robots that directly interact with humans. We include formal methods and simulation-based testing techniques as well as experimental evaluation to determine how much users actually trust robots when interacting with them in a practical setting. The complexity of verifying and validating the behavior of robots in human-robot interactions requires combining different techniques, and we discuss the benefits of doing so. As robots are being equipped with increasingly sophisticated reasoning capabilities to operate fully autonomously in open environments, it becomes more and more important that verification methods are being developed to match that level of artificial intelligence.},
-   doi = {10.1007/978-94-007-6046-2_131}
+@article{araiza2019safe,
+  title={Safe and trustworthy human-robot interaction},
+  author={Araiza-Illan, D and Eder, K},
+  journal={Humanoid Robotics: A Reference, eds A. Goswami and P. Vadakkepat (Dordrecht: Springer Netherlands)},
+  pages={2397--2419},
+  year={2019},
+  publisher={Springer, Dordrecht},
+  doi={10.1007/978-94-007-6046-2_131}
 }
 
 @article{arakawa:18,
-  title={DQN-Tamer: Human-in-the-Loop Reinforcement Learning with Intractable Feedback},
+  title={Dqn-tamer: Human-in-the-loop reinforcement learning with intractable feedback},
   author={Arakawa, Riku and Kobayashi, Sosuke and Unno, Yuya and Tsuboi, Yuta and Maeda, Shin-ichi},
   journal={arXiv preprint arXiv:1810.11748},
   year={2018},
   abstract={Exploration has been one of the greatest chal- lenges in reinforcement learning (RL), which is a large obstacle in the application of RL to robotics. Even with state-of-the-art RL algorithms, building a well-learned agent often requires too many trials, mainly due to the difficulty of matching its actions with rewards in the distant future. A remedy for this is to train an agent with real-time feedback from a human observer who immediately gives rewards for some actions. This study tackles a series of challenges for introducing such a human- in-the-loop RL scheme. The first contribution of this work is our experiments with a precisely modeled human observer: BINARY, DELAY, STOCHASTICITY, UNSUSTAINABILITY, and NATURAL REACTION. We also propose an RL method called DQN-TAMER, which efficiently uses both human feedback and distant rewards. We find that DQN-TAMER agents outperform their baselines in Maze and Taxi simulated environments. Furthermore, we demonstrate a real-world human-in-the-loop RL application where a camera automatically recognizes a user’s facial expressions as feedback to the agent while the agent explores a maze.},
   doi = {10.48550/ARXIV.1810.11748}
 }
-% check from here on
+
 @article{Arras:2017:ExplainingRNNsPerturbationAnalysis,
   title={Explaining recurrent neural network predictions in sentiment analysis},
   author={Arras, Leila and Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
   journal={arXiv preprint arXiv:1706.07206},
   year={2017},
   abstract={Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.},
   doi = {10.48550/ARXIV.1706.07206}
 }
 
 @inproceedings{Arzate:2020:SurveyInteractiveRL,
   title={A survey on interactive reinforcement learning: Design principles and open challenges},
   author={Arzate Cruz, Christian and Igarashi, Takeo},
   booktitle={Proceedings of the 2020 ACM Designing Interactive Systems Conference},
   publisher = {Association for Computing Machinery},
   pages={1195--1209},
   year={2020},
   abstract={Interactive reinforcement learning (RL) has been successfully used in various applications in different fields, which has also motivated HCI researchers to contribute in this area. In this paper, we survey interactive RL to empower human-computer interaction (HCI) researchers with the technical background in RL needed to design new interaction techniques and propose new applications. We elucidate the roles played by HCI researchers in interactive RL, identifying ideas and promising research directions. Furthermore, we propose generic design principles that will provide researchers with a guide to effectively implement interactive RL applications.},
   doi={10.1145/3357236.3395525}
 }
 
 @article{AudibertMunosSzepesv:2009:ExplorationExploitation,
   title={Exploration--exploitation tradeoff using variance estimates in multi-armed bandits},
   author={Audibert, Jean-Yves and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
   journal={Theoretical Computer Science},
   volume={410},
   number={19},
   pages={1876--1902},
   year={2009},
   publisher={Elsevier},
   doi={10.1016/j.tcs.2009.01.016},
   abstract={Algorithms based on upper confidence bounds for balancing exploration and exploitation are gaining popularity since they are easy to implement, efficient and effective. This paper considers a variant of the basic algorithm for the stochastic, multi-armed bandit problem that takes into account the empirical variance of the different arms. In earlier experimental works, such algorithms were found to outperform the competing algorithms. We provide the first analysis of the expected regret for such algorithms. As expected, our results show that the algorithm that uses the variance estimates has a major advantage over its alternatives that do not use such estimates provided that the variances of the payoffs of the suboptimal arms are low. We also prove that the regret concentrates only at a polynomial rate. This holds for all the upper confidence bound based algorithms and for all bandit problems except those special ones where with probability one the payoff obtained by pulling the optimal arm is larger than the expected payoff for the second best arm. Hence, although upper confidence bound bandit algorithms achieve logarithmic expected regret rates, they might not be suitable for a risk-averse decision maker. We illustrate some of the results by computer simulations.}
 }
 
 % TODO: maybe reformat and add doi if possible
 @article{DBLP:journals/corr/abs-1912-05743,
   author    = {Akanksha Atrey and
                Kaleigh Clary and
                David D. Jensen},
   title     = {Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps
                for Deep {RL}},
   journal   = {CoRR},
   volume    = {abs/1912.05743},
   year      = {2019},
   url       = {http://arxiv.org/abs/1912.05743},
   eprinttype = {arXiv},
   eprint    = {1912.05743},
   timestamp = {Thu, 02 Jan 2020 18:08:18 +0100},
   biburl    = {https://dblp.org/rec/journals/corr/abs-1912-05743.bib},
   bibsource = {dblp computer science bibliography, https://dblp.org}
 }
 
 @article{AzarLazaricBrunskill:2013:LifelongLearning,
   title={Sequential transfer in multi-armed bandit with finite set of models},
   author={Azar, Mohammad Gheshlaghi and Lazaric, Alessandro and Brunskill, Emma},
   journal={arXiv:1307.6887},
   year={2013},
   abstract={Learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents. Although results in supervised and reinforcement learning show that transfer may significantly improve the learning performance, most of the literature on transfer is focused on batch learning tasks. In this paper we study the problem of sequential transfer in online learning, notably in the multi-armed bandit framework, where the objective is to minimize the total regret over a sequence of tasks by transferring knowledge from prior tasks. We introduce a novel bandit algorithm based on a method-of-moments approach for estimating the possible tasks and derive regret bounds for it.},
   doi = {10.48550/ARXIV.1307.6887}
 }
 
 %%% BBB %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @article{Bach:2015:LayerWiseRelevancePropagation,
   title={On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
   author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
   journal={PloS one},
   volume={10},
   number={7},
   pages={e0130140},
   year={2015},
   publisher={Public Library of Science San Francisco, CA USA},
   abstract={Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
   doi={10.1371/journal.pone.0130140}
 }
 
 @inproceedings{bastani2018verifiable,
   title={Verifiable reinforcement learning via policy extraction},
   author={Bastani, Osbert and Pu, Yewen and Solar-Lezama, Armando},
   booktitle = {Advances in Neural Information Processing Systems},
   editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
   volume={31},
   publisher = {Curran Associates, Inc.},
   year={2018}
 }
 
 
 
 @inproceedings{Bapst:2019:AgentsPhysicalGNN,
   title={Structured agents for physical construction},
   author={Bapst, Victor and Sanchez-Gonzalez, Alvaro and Doersch, Carl and Stachenfeld, Kimberly and Kohli, Pushmeet and Battaglia, Peter and Hamrick, Jessica},
   booktitle={International Conference on Machine Learning},
   pages={464--474},
   year={2019},
   organization={PMLR},
   abstract={Physical construction—the ability to compose objects, subject to physical dynamics, to serve some function—is fundamental to human intelligence. We introduce a suite of challenging physical construction tasks inspired by how children play with blocks, such as matching a target configuration, stacking blocks to connect objects together, and creating shelter-like structures over target objects. We examine how a range of deep reinforcement learning agents fare on these challenges, and introduce several new approaches which provide superior performance. Our results show that agents which use structured representations (e.g., objects and scene graphs) and structured policies (e.g., object-centric actions) outperform those which use less structured representations, and generalize better beyond their training when asked to reason about larger scenes. Model-based agents which use Monte-Carlo Tree Search also outperform strictly model-free agents in our most challenging construction problems. We conclude that approaches which combine structured representations and reasoning with powerful learning are a key path toward agents that possess rich intuitive physics, scene understanding, and planning.}
 }
 
 % TODO: authors: others? who?; please add doi if possible
 @article{battaglia2018relational, 
   title={Relational inductive biases, deep learning, and graph networks}, author={Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others}, 
   journal={arXiv:1806.01261}, 
   year={2018} 
 }
 
 @article{bellemare2020autonomous,
   title={Autonomous navigation of stratospheric balloons using reinforcement learning},
   author={Bellemare, Marc G and Candido, Salvatore and Castro, Pablo Samuel and Gong, Jun and Machado, Marlos C and Moitra, Subhodeep and Ponda, Sameera S and Wang, Ziyu},
   journal={Nature},
   volume={588},
   number={7836},
   pages={77--82},
   year={2020},
   publisher={Nature Publishing Group},
   doi={10.1038/s41586-020-2939-8}
 }
 
 @article{Ben-YounesEtAl:2022:DrivingBehaviorEx,
   title={Driving behavior explanation with multi-level fusion},
   author={Ben-Younes, H{\'e}di and Zablocki, {\'E}loi and P{\'e}rez, Patrick and Cord, Matthieu},
   journal={Pattern Recognition},
   volume={123},
   pages={108421},
   year={2022},
   publisher={Elsevier},
   abstract = {In this era of active development of autonomous vehicles, it becomes crucial to provide driving systems with the capacity to explain their decisions. In this work, we focus on generating high-level driving explanations as the vehicle drives. We present BEEF, for BEhavior Explanation with Fusion, a deep architecture which explains the behavior of a trajectory prediction model. Supervised by annotations of human driving decisions justifications, BEEF learns to fuse features from multiple levels. Leveraging recent advances in the multi-modal fusion literature, BEEF is carefully designed to model the correlations between high-level decisions features and mid-level perceptual features. The flexibility and efficiency of our approach are validated with extensive experiments on the HDD and BDD-X datasets.},
   doi = {10.1016/j.patcog.2021.108421}
 }
 
 @article{biyik:21,
   title={Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences},
   author={B{\i}y{\i}k, Erdem and Losey, Dylan P and Palan, Malayandi and Landolfi, Nicholas C and Shevchuk, Gleb and Sadigh, Dorsa},
   journal={The International Journal of Robotics Research},
   pages={45--67},
   year={2021},
   publisher={SAGE Publications Sage UK: London, England},
   abstract={The paper proposes an algorithm that uses demonstrations (passive information) to initialize a belief of the reward function and actively queries the user's preferences to learn the actual reward function efficiently. The algorithm decides when to use each type of information and when to stop querying the teacher. More importantly, it accounts for the human's skill to provide data by maximizing the utility of questions while minimizing the human's uncertainty over their answer. Using gain information is more data-efficient than the previous approach for generating queries.},
   doi={10.1177/02783649211041652}
 }
 
 %%% CCC %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @inproceedings{Caltagarione:2017:DrivingPathGeneration,
   title={LIDAR-based driving path generation using fully convolutional neural networks},
   author={Caltagirone, Luca and Bellone, Mauro and Svensson, Lennart and Wahde, Mattias},
   booktitle={2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)},
   pages={1--6},
   year={2017},
   organization={IEEE},
   abstract={In this work, a novel learning-based approach has been developed to generate driving paths by integrating LIDAR point clouds, GPS-IMU information, and Google driving directions. The system is based on a fully convolutional neural network that jointly learns to carry out perception and path generation from real-world driving sequences and that is trained using automatically generated training examples. Several combinations of input data were tested in order to assess the performance gain provided by specific information modalities. The fully convolutional neural network trained using all the available sensors together with driving directions achieved the best MaxF score of 88.13\% when considering a region of interest of 60 × 60 meters. By considering a smaller region of interest, the agreement between predicted paths and ground-truth increased to 92.60\%. The positive results obtained in this work indicate that the proposed system may help fill the gap between low-level scene parsing and behavior-reflex approaches by generating outputs that are close to vehicle control and at the same time human-interpretable.},
   doi={10.1109/ITSC.2017.8317618}
 }
 
 @article{ChakrabortyEtAl:2021:SurveyAdversarialAttacks,
   author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
   title = {A survey on adversarial attacks and defences},
   year = {2021},
   journal = {CAAI Transactions on Intelligence Technology},
   volume = {6},
   number = {1},
   pages = {25--45},
   url = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/cit2.12028},
   eprint = {https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/cit2.12028},
   abstract = {Abstract Deep learning has evolved as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. The advancement of deep learning has been so radical that today it can surpass human-level performance. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, efficient deep learning systems can be jeopardised by using crafted adversarial samples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. Herein, the authors attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate on the efficiency and challenges of recent countermeasures against them.},
   doi = {10.1049/cit2.12028}
 }
 
 @inproceedings{cederborg2015policy,
   title={Policy shaping with human teachers},
   author={Cederborg, Thomas and Grover, Ishaan and Isbell, Charles L and Thomaz, Andrea L},
   booktitle={Twenty-Fourth International Joint Conference on Artificial Intelligence},
   year={2015},
   abstract={In this work we evaluate the performance of a policy shaping algorithm using 26 human teachers. We examine if the algorithm is suitable for human generated data on two different boards in a pac-man domain, comparing performance to an oracle that provides critique based on one known winning policy. Perhaps surprisingly, we show that the data generated by our 26 participants yields even better performance for the agent than data generated by the oracle. This might be because humans do not discourage exploring multiple winning policies. Additionally, we evaluate the impact of different verbal instructions, and different interpretations of silence, finding that the usefulness of data is affected both by what instructions is given to teachers, and how the data is interpreted.}
 }
 
 @article{Christiano:2017:DeepRLHumanPreferences,
   title={Deep reinforcement learning from human preferences},
   author={Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
   journal={arXiv preprint arXiv:1706.03741},
   year={2017},
   abstract={For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
   doi = {10.48550/ARXIV.1706.03741}
 }
 
 @article{Collins:2019:Cognition,
   title = {Reinforcement learning: bringing together computation and cognition},
   author = {Anne Gabrielle Eva Collins},
   year = {2019},
   journal = {Current Opinion in Behavioral Sciences},
   volume = {29},
   pages = {63--68},
   abstract = {A key aspect of human intelligence is our ability to learn very quickly. This ability is still lacking in artificial intelligence. This article will highlight recent research showing how bringing together the fields of artificial intelligence and cognitive science may benefit both. Ideas from artificial intelligence have provided helpful formal theories to account for aspects of human learning. In return, ideas from cognitive science and neuroscience can also inform artificial intelligence research with directions to make algorithms more human-like. For example, recent work shows that human learning can only be understood in the context of multiple separate, interacting memory systems, rather than as a single, complex learner. This insight is starting to show promise in improving artificial agents’ learning efficiency.},
   doi = {10.1016/j.cobeha.2019.04.011}
 }
 
-@article{CuiEtAl:2020:EMPATHICFrameworkHumanFeedback,  
-author = {Cui, Yuchen and Zhang, Qiping and Allievi, Alessandro and Stone, Peter and Niekum, Scott and Knox, W.},  
-year = {2020},  
-month = {09},  
-pages = {},  
-title = {The EMPATHIC Framework for Task Learning from Implicit Human Feedback}  
-} 
-
 %%% DDD %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 @inproceedings{da2020uncertainty,
   title={Uncertainty-aware action advising for deep reinforcement learning agents},
   author={Da Silva, Felipe Leno and Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E},
   booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
   volume={34},
   pages={5792--5799},
   year={2020},
   doi={10.1609/aaai.v34i04.6036}
 }
 
 % TODO: what means and others? -> authors, which article is meant?
 @article{daniel2016hierarchical,
   title={Hierarchical relative entropy policy search},
   author={Daniel, Christian and Neumann, Gerhard and Kroemer, Oliver and Peters, Jan and others},
   journal={Journal of Machine Learning Research},
   volume={17},
   pages={1--50},
   year={2016},
   publisher={Massachusetts Institute of Technology Press (MIT Press)/Microtome Publishing}
 }
 
 @article{dazeley2021explainable,
   title={Explainable reinforcement learning for Broad-XAI: a conceptual framework and survey},
   author={Dazeley, Richard and Vamplew, Peter and Cruz, Francisco},
   journal={arXiv preprint arXiv:2108.09003},
   year={2021},
   abstract = {Broad Explainable Artificial Intelligence moves away from interpreting individual decisions based on a single datum and aims to provide integrated explanations from multiple machine learning algorithms into a coherent explanation of an agent's behaviour that is aligned to the communication needs of the explainee. Reinforcement Learning (RL) methods, we propose, provide a potential backbone for the cognitive model required for the development of Broad-XAI. RL represents a suite of approaches that have had increasing success in solving a range of sequential decision-making problems. However, these algorithms all operate as black-box problem solvers, where they obfuscate their decision-making policy through a complex array of values and functions. EXplainable RL (XRL) is relatively recent field of research that aims to develop techniques to extract concepts from the agent's: perception of the environment; intrinsic/extrinsic motivations/beliefs; Q-values, goals and objectives. This paper aims to introduce a conceptual framework, called the Causal XRL Framework (CXF), that unifies the current XRL research and uses RL as a backbone to the development of Broad-XAI. Additionally, we recognise that RL methods have the ability to incorporate a range of technologies to allow agents to adapt to their environment. CXF is designed for the incorporation of many standard RL extensions and integrated with external ontologies and communication facilities so that the agent can answer questions that explain outcomes and justify its decisions.},
   doi = {10.48550/ARXIV.2108.09003}
 }
 
 @inproceedings{de:17,
   title={How people explain action (and autonomous intelligent systems should too)},
   author={De Graaf, Maartje MA and Malle, Bertram F},
   booktitle={2017 AAAI Fall Symposium Series},
   year={2017},
   abstract ={To make Autonomous Intelligent Systems (AIS), such as virtual agents and embodied robots, “explainable” we need to understand how people respond to such systems and what expectations they have of them. Our thesis is that people will regard most AIS as intentional agents and apply the conceptual framework and psychological mechanisms of human behavior explanation to them. We present a well supported theory of how people explain human behavior and sketch what it would take to implement the underlying framework of explanation in AIS. The benefits will be considerable: When an AIS is able to explain its behavior in ways that people find comprehensible, people are more likely to form correct mental models of such a system and calibrate their trust in the system.}
 }
 
 @article{DeSaintsEtAl:2008:phri,
   title = {An atlas of physical human–robot interaction},
   author = {{De Santis}, Agostino and Siciliano, Bruno and {De Luca}, Alessandro and Bicchi, Antonio},
   year = {2008},
   journal = {Mechanism and Machine Theory},
   volume = {43},
   number = {3},
   pages = {253--270},
   abstract = {A broad spectrum of issues have to be addressed in order to tackle the problem of a safe and dependable physical Human–Robot Interaction (pHRI). In the immediate future, metrics related to safety and dependability have to be found in order to successfully introduce robots in everyday enviornments. While there are certainly also “cognitive” issues involved, due to the human perception of the robot (and vice versa), and other objective metrics related to fault detection and isolation, our discussion focuses on the peculiar aspects of “physical” interaction with robots. In particular, safety and dependability are the underlying evaluation criteria for mechanical design, actuation, and control architectures. Mechanical and control issues are discussed with emphasis on techniques that provide safety in an intrinsic way or by means of control components. Attention is devoted to dependability, mainly related to sensors, control architectures, and fault handling and tolerance. Suggestions are provided to draft metrics for evaluating safety and dependability in pHRI, and references to the works of the scientific groups involved in the pHRI research complete the study. The present atlas is a result of the EURON perspective research project “Physical Human–Robot Interaction in anthropic DOMains (PHRIDOM)”, aimed at charting the new territory of pHRI, and constitutes the scientific basis for the ongoing STReP project “Physical Human–Robot Interaction: depENDability and Safety (PHRIENDS)”, aimed at developing key components for the next generation of robots, designed to share their environment with people.},
   doi = {10.1016/j.mechmachtheory.2007.03.003}
 }
 
 @article{DoroudiThomasBrunskill:2017:ImportanceSampling,
   title={Importance Sampling for Fair Policy Selection},
   author={Doroudi, Shayan and Thomas, Philip S and Brunskill, Emma},
   journal={Grantee Submission},
   year={2017},
   publisher={ERIC},
   abstract={We consider the problem of off-policy policy selection in reinforcement learning: using historical data generated from running one policy to compare two or more policies. We show that approaches based on importance sampling can be "unfair"--they can select the worse of two policies more often than not. We give two examples where the unfairness of importance sampling could be practically concerning. We then present sufficient conditions to theoretically guarantee fairness and a related notion of safety. Finally, we provide a practical importance sampling-based estimator to help mitigate one of the systematic sources of unfairness resulting from using importance sampling for policy selection},
   doi={10.24963/ijcai.2018/729}
 }
 
 @inproceedings{dovsilovic2018explainable,
   title={Explainable artificial intelligence: A survey},
   author={Do{\v{s}}ilovi{\'c}, Filip Karlo and Br{\v{c}}i{\'c}, Mario and Hlupi{\'c}, Nikica},
   booktitle={2018 41st International convention on information and communication technology, electronics and microelectronics (MIPRO)},
   pages={0210--0215},
   year={2018},
   organization={IEEE},
   doi={10.23919/MIPRO.2018.8400040}
 }
 
 %%% EEE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @inproceedings{EderHarperLeonards:2014:HITLRoboticsSafetyAssurance,
   title={Towards the safety of human-in-the-loop robotics: Challenges and opportunities for safety assurance of robotic co-workers'},
   author={Eder, Kerstin and Harper, Chris and Leonards, Ute},
   booktitle={The 23rd IEEE International Symposium on Robot and Human Interactive Communication},
   pages={660--665},
   year={2014},
   organization={IEEE},
   abstract={The success of the human-robot co-worker team in a flexible manufacturing environment where robots learn from demonstration heavily relies on the correct and safe operation of the robot. How this can be achieved is a challenge that requires addressing both technical as well as human-centric research questions. In this paper we discuss the state of the art in safety assurance, existing as well as emerging standards in this area, and the need for new approaches to safety assurance in the context of learning machines. We then focus on robotic learning from demonstration, the challenges these techniques pose to safety assurance and outline opportunities to integrate safety considerations into algorithms “by design”. Finally, from a human-centric perspective, we stipulate that, to achieve high levels of safety and ultimately trust, the robotic co-worker must meet the innate expectations of the humans it works with. It is our aim to stimulate a discussion focused on the safety aspects of human-in-the-loop robotics, and to foster multidisciplinary collaboration to address the research challenges identified.},
   doi={10.1109/ROMAN.2014.6926328}
 }
 
 @misc{EC:LegalFramework,
   key={Regulatory framework proposal on artificial intelligence},
   title = {Regulatory framework proposal on artificial intelligence},
   year={2021},
   howpublished = {\url{https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai}},
   note = {(02.02.2022)}
 }
 
 @misc{EUResolution:2020:ethicalAI,
   key={Resolution on a framework of ethical artificial intelligence, robotics and related technologies},
   title = {European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020/2012(INL)},
   year={2020},
   howpublished = {\url{https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52020IP0275}},
   note = {(02.02.2022)}
 }
 
 @misc{EUDirective:2006:Machinery,
   key = {Machinery Directive 2006/42/EC},
   year = {2006},
   title = {Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on machinery, and amending Directive 95/16/EC},
   howpublished = {\url{https://eur-lex.europa.eu/eli/dir/2006/42/2019-07-26}},
   note = {(02.02.2022)}
 }
 
 @article{EvansEtAl:2021:ExplainabilityParadox,
    year = {2022},
    author = {Evans, Theodore and Retzlaff, Carl Orge and Geißler, Christian and Kargl, Michaela and Plass, Markus and Müller, Heimo and Kiehl, Tim-Rasmus and Zerbe, Norman and Holzinger, Andreas},
    title = {The explainability paradox: Challenges for xAI in digital pathology},
    journal = {Future Generation Computer Systems},
    volume = {133},
    number = {8},
    pages = {281--296},
    abstract = {The increasing prevalence of digitised workflows in diagnostic pathology opens the door to life-saving applications of artificial intelligence (AI). Explainability is identified as a critical component for the safety, approval and acceptance of AI systems for clinical use. Despite the cross-disciplinary challenge of building explainable AI (xAI), very few application- and user-centric studies in this domain have been carried out. We conducted the first mixed-methods study of user interaction with samples of state-of-the-art AI explainability techniques for digital pathology. This study reveals challenging dilemmas faced by developers of xAI solutions for medicine and proposes empirically-backed principles for their safer and more effective design.},
    doi = {10.1016/j.future.2022.03.009}
 }
 
 @article{eysenbach2018diversity,
   title={Diversity is all you need: Learning skills without a reward function},
   author={Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
   journal={arXiv preprint arXiv:1802.06070},
   year={2018},
   abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
   doi = {10.48550/ARXIV.1802.06070}
 }
 
 %%% FFF %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @article{florensa2017stochastic,
   title={Stochastic neural networks for hierarchical reinforcement learning},
   author={Florensa, Carlos and Duan, Yan and Abbeel, Pieter},
   journal={arXiv preprint arXiv:1704.03012},
   year={2017},
   abstract = {Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.},
   doi = {10.48550/ARXIV.1704.03012}
 }
 
 @inproceedings{fukuchi2017autonomous,
   title={Autonomous self-explanation of behavior for interactive reinforcement learning agents},
   author={Fukuchi, Yosuke and Osawa, Masahiko and Yamakawa, Hiroshi and Imai, Michita},
   booktitle={Proceedings of the 5th International Conference on Human Agent Interaction},
   publisher = {Association for Computing Machinery},
   address = {New York, NY, USA},
   pages={97--101},
   year={2017},
   doi = {10.1145/3125739.3125746}
 }
 
 @inproceedings{fukuchi2017application,
   title={Application of instruction-based behavior explanation to a reinforcement learning agent with changing policy},
   author={Fukuchi, Yosuke and Osawa, Masahiko and Yamakawa, Hiroshi and Imai, Michita},
   booktitle={International Conference on Neural Information Processing},
   editor={Derong Liu and Shengli Xie and Yuanqing Li and Dongbin Zhao and El-Sayed M. El-Alfy},
   pages={100--108},
   year={2017},
   organization={Springer},
   doi={10.1007/978-3-319-70087-8_11}
 }
 
 %%% GGG %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @article{gao2018reinforcement,
   title={Reinforcement learning from imperfect demonstrations},
   author={Gao, Yang and Xu, Huazhe and Lin, Ji and Yu, Fisher and Levine, Sergey and Darrell, Trevor},
   journal={arXiv preprint arXiv:1802.05313},
   year={2018},
   abstract={proposes a normalized actor-critic algorithm that learns from demonstrations, no supervised loss function is used in the framework. When demonstrations are biased sample from environment, other learning from demonstration methods will not penalize the bad actions explicitly, thus making the undesirable actions from demonstrator favorable. In the normalized actor-critic framework, the Q-function can be normalized over all the actions thus scaling down the Q-values of unfavourable actions. However, the effectiveness is demonstrated on toy Minecraft and Torcs (racing game). This approach can deal with imperfect demonstrations.},
   doi = {10.48550/ARXIV.1802.05313}
 }
 
 @article{GeirhosEtAl:2020:ShortcutLearningDNN,
   title={Shortcut learning in deep neural networks},
   author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
   journal={Nature Machine Intelligence},
   volume={2},
   number={11},
   pages={665--673},
   year={2020},
   publisher={Nature Publishing Group},
   abstract={Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today’s machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this Perspective we seek to distil how many of deep learning’s failures can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in comparative psychology, education and linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
   doi={10.1038/s42256-020-00257-z}
 }
 
 @article{GlanoisEtAl:2021:SurveyInterpretableRL,
   title={A Survey on Interpretable Reinforcement Learning},
   author={Glanois, Claire and Weng, Paul and Zimmer, Matthieu and Li, Dong and Yang, Tianpei and Hao, Jianye and Liu, Wulong},
   journal={arXiv preprint arXiv:2112.13112},
   year={2021},
   abstract={Although deep reinforcement learning has become a promising machine learning approach for sequential decision-making problems, it is still not mature enough for high-stake domains such as autonomous driving or medical applications. In such contexts, a learned policy needs for instance to be interpretable, so that it can be inspected before any deployment (e.g., for safety and verifiability reasons). This survey provides an overview of various approaches to achieve higher interpretability in reinforcement learning (RL). To that aim, we distinguish interpretability (as a property of a model) and explainability (as a post-hoc operation, with the intervention of a proxy) and discuss them in the context of RL with an emphasis on the former notion. In particular, we argue that interpretable RL may embrace different facets: interpretable inputs, interpretable (transition/reward) models, and interpretable decision-making. Based on this scheme, we summarize and analyze recent work related to interpretable RL with an emphasis on papers published in the past 10 years. We also discuss briefly some related research areas and point to some potential promising research directions.},
   doi = {10.48550/ARXIV.2112.13112}
 }
 
 @article{GoodfellowShlensSzegedy:2014:AdversarialExamples,
   title={Explaining and harnessing adversarial examples},
   author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
   journal={arXiv preprint arXiv:1412.6572},
   year={2014},
   abstract={Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
   doi = {10.48550/ARXIV.1412.6572}
 }
 
 @inproceedings{griffith2013policy,
   title={Policy shaping: Integrating human feedback with reinforcement learning},
   author={Griffith, Shane and Subramanian, Kaushik and Scholz, Jonathan and Isbell, Charles L and Thomaz, Andrea L},
   booktitle = {Advances in Neural Information Processing Systems},
   editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
   volume={26},
   year={2013},
   abstract = {A long term goal of Interactive Reinforcement Learning is to incorporate non-expert human feedback to solve complex tasks. State-of-the-art methods have approached this problem by mapping human information to reward and value signals to indicate preferences and then iterating over them to compute the necessary control policy. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct labels on the policy. We compare Advise to state-of-the-art approaches and highlight scenarios where it outperforms them and importantly is robust to infrequent and inconsistent human feedback.}
 }
 
 % TODO: arXiv id is not correct -> which version was used? please fix and add doi (if possible)
 @article{guan2020explanation,
   title={Explanation augmented feedback in human-in-the-loop reinforcement learning},
   author={Guan, Lin and Verma, Mudit and Guo, Sihang and Zhang, Ruohan and Kambhampati, Subbarao},
   journal={arXiv preprint arXiv:2006.14804},
   year={2020},
   abstract = {Human-in-the-loop Reinforcement Learning (HRL) aims to integrate human guid- ance with Reinforcement Learning (RL) algorithms to improve sample efficiency and performance. A common type of human guidance in HRL is binary evaluative “good" or “bad" feedback for queried states and actions. However, this type of learning scheme suffers from the problems of weak supervision and poor efficiency in leveraging human feedback. To address this, we present EXPAND (EXPlanation AugmeNted feeDback) which provides a visual explanation in the form of saliency maps from humans in addition to the binary feedback. EXPAND employs a state perturbation approach based on salient information in the state to augment the binary feedback. We choose five tasks, namely Pixel-Taxi and four Atari games, to evaluate this approach. We demonstrate the effectiveness of our method using two metrics: environment sample efficiency and human feedback sample efficiency. We show that our method significantly outperforms previous methods. We also analyze the results qualitatively by visualizing the agent’s attention. Finally, we present an ablation study to confirm our hypothesis that augmenting binary feedback with state salient information results in a boost in performance.}
 }
 
 @inproceedings{guo2021edge,
   title={EDGE: Explaining Deep Reinforcement Learning Policies},
   author={Guo, Wenbo and Wu, Xian and Khan, Usmann and Xing, Xinyu},
   booktitle={Advances in Neural Information Processing Systems},
   editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages = {12222--12236},
   volume={34},
   year={2021}
 }
 
 @article{Guo:2022:RLSurveyHumanPriorKnowledge,
   title={Survey of Reinforcement Learning based on Human Prior Knowledge},
   author={Guo, Zijing and Yao, Chendie and Feng, Yanghe and Xu, Yue},
   journal={Journal of Uncertain Systems},
   volume={15},
   number={01},
   pages={2230001},
   year={2022},
   publisher={World Scientific},
   abstract={At present, task planning is mainly solved by rules or operational research methods. The time complexity and space complexity of these methods increase exponentially with the growth of problem size. Thus, it is challenging to solve large-scale problems. Moreover, it is helpless to solve dynamic task planning problems. Reinforcement learning (RL) is often used to solve the dynamic planning problem of continuous decision making. RL agent constantly interacts with the environment to achieve the expected goal. However, the RL method meets challenges when handling the problem with ample state space. More sampling and exploration are needed to update the strategy gradient, and the convergence speed is slow. Humans ensure a quick start of learning by using prior knowledge, which reduces the exploration time of problems. Thus, we review the methods which combine human prior knowledge with RL through temporal node classification of human prior knowledge combined with RL. In this way, agents effectively reduce the sampling and exploration of the environment and eventually get the optimal strategy faster. Finally, we propose the development direction.},
   doi = {10.1142/S1752890922300011}
 }
 
 @article{GunningAha:2019:DARPA,
    year = {2019},
    author = {Gunning, David and Aha, David W.},
    title = {DARPA's Explainable Artificial Intelligence Program},
    journal = {AI Magazine},
    volume = {40},
    number = {2},
    pages = {44--58},
    abstract = {Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA’s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychological requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychological theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems’ explanations improve user understanding, user trust, and user task performance.},
    doi = {10.1609/aimag.v40i2.2850 }
 }
 
 @article{GuptaEtAl:2021:MARLCoordinationHAMMER,
   title={HAMMER: Multi-Level Coordination of Reinforcement Learning Agents via Learned Messaging},
   author={Gupta, Nikunj and Srinivasaraghavan, G and Mohalik, Swarup Kumar and Taylor, Matthew E},
   journal={arXiv preprint arXiv:2102.00824},
   year={2021},
   abstract={Cooperative multi-agent reinforcement learning (MARL) has achieved significant results, most notably by leveraging the representation learning abilities of deep neural networks. However, large centralized approaches quickly become infeasible as the number of agents scale, and fully decentralized approaches can miss important opportunities for information sharing and coordination. Furthermore, not all agents are equal - in some cases, individual agents may not even have the ability to send communication to other agents or explicitly model other agents. This paper considers the case where there is a single, powerful, central agent that can observe the entire observation space, and there are multiple, low powered, local agents that can only receive local observations and cannot communicate with each other. The job of the central agent is to learn what message to send to different local agents, based on the global observations, not by centrally solving the entire problem and sending action commands, but by determining what additional information an individual agent should receive so that it can make a better decision. After explaining our MARL algorithm, hammer, and where it would be most applicable, we implement it in the cooperative navigation and multi-agent walker domains. Empirical results show that 1) learned communication does indeed improve system performance, 2) results generalize to multiple numbers of agents, and 3) results generalize to different reward structures.},
   doi = {10.48550/ARXIV.2102.00824}
 }
 
 % TODO: maybe reformat
 @article{DBLP:journals/corr/abs-1912-12191,
   author    = {Piyush Gupta and
                Nikaash Puri and
                Sukriti Verma and
                Dhruv Kayastha and
                Shripad Deshmukh and
                Balaji Krishnamurthy and
                Sameer Singh},
   title     = {Explain Your Move: Understanding Agent Actions Using Focused Feature
                Saliency},
   journal   = {CoRR},
   volume    = {abs/1912.12191},
   year      = {2019},
   url       = {http://arxiv.org/abs/1912.12191},
   eprinttype = {arXiv},
   eprint    = {1912.12191},
   timestamp = {Mon, 24 Feb 2020 12:40:24 +0100},
   biburl    = {https://dblp.org/rec/journals/corr/abs-1912-12191.bib},
   bibsource = {dblp computer science bibliography, https://dblp.org}
 }
 
 %%% HHH %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @article{HaSchmidhuber:2018:CoreKnowledgeWorldModels,
   title={World models},
   author={Ha, David and Schmidhuber, J{\"u}rgen},
   journal={arXiv preprint arXiv:1803.10122},
   year={2018},
   abstract={We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. },
   doi = {10.5281/ZENODO.1207631}
 }
 
 @article{habibian:21,
   title={Here's What I've Learned: Asking Questions that Reveal Reward Learning},
   author={Habibian, Soheil and Jonnavittula, Ananth and Losey, Dylan P},
   journal={arXiv preprint arXiv:2107.01995},
   year={2021},
   abstract={The state-of-the-art preference learning focuses on informative questions; however, most people assume that the robot's questions reveal the robot's knowledge. This paper studies how robot questions influence humans' perception of a robot. The robot chooses informative questions that at the same time reveal its learning. A user study shows that people perceived that the robot learned similarly with purely informative queries or with the combined approach. However, users prefer revealing+informative questions since they find these questions easier to answer, more revealing about the robot's learning, and better focused on uncertain aspects of the task. A follow-up study determines that accounting what the human learns from the robot's learning helps decide which features are still unclear to the robot and whether the robot is ready to be deployed},
   doi = {10.48550/ARXIV.2107.01995}
 }
 
 @inproceedings{hadfield:17,
   title={Inverse reward design},
   author={Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart J and Dragan, Anca},
   booktitle={Advances in neural information processing systems},
   editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
   volume={30},
   year={2017},
   abstract={Autonomous agents optimize the reward function we give them. What they don’tknow is how hard it is for us to design a reward function that actually captureswhat we want.   When designing the reward,  we might think of some specifictraining scenarios, and make sure that the reward will lead to the right behaviorinthosescenarios. Inevitably, agents encounternewscenarios (e.g., new types ofterrain) where optimizing that same reward may lead to undesired behavior. Ourinsight is that reward functions are merelyobservationsabout what the designeractuallywants, and that they should be interpreted in the context in which they weredesigned. We introduceinverse reward design(IRD) as the problem of inferring thetrue objective based on the designed reward and the training MDP. We introduceapproximate methods for solving IRD problems, and use their solution to planrisk-averse behavior in test MDPs. Empirical results suggest that this approach canhelp alleviate negative side effects of misspecified reward functions and mitigatereward hacking.}
 }
 
 @article{Harmon:1997:ReinforcementLearningATutorial,
   title={Reinforcement Learning: A Tutorial.},
   author={Harmon, Mance E and Harmon, Stephanie S},
   journal = {Technical Report Defense Technical Information Center},
   year={1997},
   publisher={WRIGHT LAB WRIGHT-PATTERSON AFB OH},
   abstract={The purpose of this tutorial is to provide an introduction to reinforcement learning RL at a level easily understood by students and researchers in a wide range of disciplines. The intent is not to present a rigorous mathematical discussion that requires a great deal of effort on the part of the reader, but rather to present a conceptual framework that might serve as an introduction to a more rigorous study of RL. The fundamental principles and techniques used to solve RL problems are presented. The most popular RL algorithms are presented. Section 1 presents an overview of RL and provides a simple example to develop intuition of the underlying dynamic programming mechanism. In Section 2 the parts of a reinforcement learning problem are discussed. These include the environment, reinforcement function, and value function. Section 3 gives a description of the most widely used reinforcement learning algorithms. These include TDlambda and both the residual and direct forms of value iteration, Q-learning, and advantage learning. In Section 4 some of the ancillary issues of RL are briefly discussed, such as choosing an exploration strategy and a discount factor. The conclusion is given in Section 5. Finally, Section 6 is a glossary of commonly used terms followed by references and bibliography.}
 }
 
 @inproceedings{hasanbeig2021deepsynth,
   title={DeepSynth: Automata synthesis for automatic task segmentation in deep reinforcement learning},
   author={Hasanbeig, Mohammadhosein and Jeppu, Natasha Yogananda and Abate, Alessandro and Melham, Tom and Kroening, Daniel},
   booktitle={The Thirty-Fifth $\{$AAAI$\}$ Conference on Artificial Intelligence,$\{$AAAI$\}$},
   volume={2},
   pages={36},
   year={2021}
 }
 
 @inproceedings{HayesShah:2017:AutonomousPolicyExplanation,
   title={Improving robot controller transparency through autonomous policy explanation},
   author={Hayes, Bradley and Shah, Julie A},
   booktitle={2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI},
   pages={303--312},
   year={2017},
   organization={IEEE},
   abstract={Shared expectations and mutual understanding are critical facets of teamwork. Achieving these in human-robot collaborative contexts can be especially challenging, as humans and robots are unlikely to share a common language to convey intentions, plans, or justifications. Even in cases where human co-workers can inspect a robot's control code, and particularly when statistical methods are used to encode control policies, there is no guarantee that meaningful insights into a robot's behavior can be derived or that a human will be able to efficiently isolate the behaviors relevant to the interaction. We present a series of algorithms and an accompanying system that enables robots to autonomously synthesize policy descriptions and respond to both general and targeted queries by human collaborators. We demonstrate applicability to a variety of robot controller types including those that utilize conditional logic, tabular reinforcement learning, and deep reinforcement learning, synthesizing informative policy descriptions for collaborators and facilitating fault diagnosis by non-experts.},
   doi = {10.1145/2909824.3020233}
 }
 
 @inproceedings{hazan2019provably,
   title={Provably efficient maximum entropy exploration},
   author={Hazan, Elad and Kakade, Sham and Singh, Karan and Van Soest, Abby},
   booktitle = {Proceedings of the 36th International Conference on Machine Learning},
   editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
   volume = {97},
   series = {Proceedings of Machine Learning Research},
   pages={2681--2691},
   year={2019},
   organization={PMLR},
   abstract = {Suppose an agent is in a (possibly unknown) Markov Decision Process in the absence of a reward signal, what might we hope that an agent can efficiently learn to do? This work studies a broad class of objectives that are defined solely as functions of the state-visitation frequencies that are induced by how the agent behaves. For example, one natural, intrinsically defined, objective problem is for the agent to learn a policy which induces a distribution over state space that is as uniform as possible, which can be measured in an entropic sense. We provide an efficient algorithm to optimize such such intrinsically defined objectives, when given access to a black box planning oracle (which is robust to function approximation). Furthermore, when restricted to the tabular setting where we have sample based access to the MDP, our proposed algorithm is provably efficient, both in terms of its sample and computational complexities. Key to our algorithmic methodology is utilizing the conditional gradient method (a.k.a. the Frank-Wolfe algorithm) which utilizes an approximate MDP solver.}
 }
 
 @article{hein2017particle,
   title={Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies},
   author={Hein, Daniel and Hentschel, Alexander and Runkler, Thomas and Udluft, Steffen},
   journal={Engineering Applications of Artificial Intelligence},
   volume={65},
   pages={87--98},
   year={2017},
   publisher={Elsevier},
   abstract = {Fuzzy controllers are efficient and interpretable system controllers for continuous state and action spaces. To date, such controllers have been constructed manually or trained automatically either using expert-generated problem-specific cost functions or incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not found in most real-world reinforcement learning (RL) problems. In such applications, online learning is often prohibited for safety reasons because it requires exploration of the problem’s dynamics during policy training. We introduce a fuzzy particle swarm reinforcement learning (FPSRL) approach that can construct fuzzy RL policies solely by training parameters on world models that simulate real system dynamics. These world models are created by employing an autonomous machine learning technique that uses previously generated transition samples of a real system. To the best of our knowledge, this approach is the first to relate self-organizing fuzzy controllers to model-based batch RL. FPSRL is intended to solve problems in domains where online learning is prohibited, system dynamics are relatively easy to model from previously generated default policy transition samples, and it is expected that a relatively easily interpretable control policy exists. The efficiency of the proposed approach with problems from such domains is demonstrated using three standard RL benchmarks, i.e., mountain car, cart-pole balancing, and cart-pole swing-up. Our experimental results demonstrate high-performing, interpretable fuzzy policies.},
   doi = {10.1016/j.engappai.2017.07.005}
 }
 
 @article{hein2018interpretable,
   title={Interpretable policies for reinforcement learning by genetic programming},
   author={Hein, Daniel and Udluft, Steffen and Runkler, Thomas A},
   journal={Engineering Applications of Artificial Intelligence},
   volume={76},
   pages={158--169},
   year={2018},
   publisher={Elsevier},
   abstract = {The search for interpretable reinforcement learning policies is of high academic and industrial interest. Especially for industrial systems, domain experts are more likely to deploy autonomously learned controllers if they are understandable and convenient to evaluate. Basic algebraic equations are supposed to meet these requirements, as long as they are restricted to an adequate complexity. Here we introduce the genetic programming for reinforcement learning (GPRL) approach based on model-based batch reinforcement learning and genetic programming, which autonomously learns policy equations from pre-existing default state–action trajectory samples. GPRL is compared to a straightforward method which utilizes genetic programming for symbolic regression, yielding policies imitating an existing well-performing, but non-interpretable policy. Experiments on three reinforcement learning benchmarks, i.e., mountain car, cart–pole balancing, and industrial benchmark, demonstrate the superiority of our GPRL approach compared to the symbolic regression method. GPRL is capable of producing well-performing interpretable reinforcement learning policies from pre-existing default trajectory data.},
   doi = {10.1016/j.engappai.2018.09.007}
 }
 
 @inproceedings{hein2019generating,
   title={Generating interpretable reinforcement learning policies using genetic programming},
   author={Hein, Daniel and Udluft, Steffen and Runkler, Thomas A},
   booktitle={Proceedings of the Genetic and Evolutionary Computation Conference Companion},
   publisher = {Association for Computing Machinery},
   pages={23--24},
   year={2019},
   abstract = {The search for interpretable reinforcement learning policies is of high academic and industrial interest. Especially for industrial systems, domain experts are more likely to deploy autonomously learned controllers if they are understandable and convenient to evaluate. Basic algebraic equations are supposed to meet these requirements, as long as they are restricted to an adequate complexity. In our recent work "Interpretable policies for reinforcement learning by genetic programming" published in Engineering Applications of Artificial Intelligence 76 (2018), we introduced the genetic programming for reinforcement learning (GPRL) approach. GPRL uses model-based batch reinforcement learning and genetic programming and autonomously learns policy equations from preexisting default state-action trajectory samples. Experiments on three reinforcement learning benchmarks demonstrate that GPRL can produce human-interpretable policies of high control performance.},
   doi = {10.1145/3319619.3326755}
 }
 
 @book{Hermann:2011:Sonification,
   title={The sonification handbook},
   author={Hermann, Thomas and Hunt, Andy and Neuhoff, John G},
   year={2011},
   publisher={Logos Verlag Berlin}
 }
 
 % TODO: is this the real abstract or a summary?
 @inproceedings{hester2018deep,
   title={Deep q-learning from demonstrations},
   author={Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Osband, Ian and others},
   booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
   volume={32},
   year={2018},
   abstract={Using demonstrations in Deep RL using a combination of RL loss and supervised large margin classification loss to imitate the expert actions. However, experiments were done with human demonstrators on Atari games, no robotics domain considered}
 }
 
 @article{heuillet2021explainability,
   title={Explainability in deep reinforcement learning},
   author={Heuillet, Alexandre and Couthouis, Fabien and D{\'\i}az-Rodr{\'\i}guez, Natalia},
   journal={Knowledge-Based Systems},
   volume={214},
   pages={106685},
   issn = {0950-7051},
   year={2021},
   publisher={Elsevier},
   abstract = {A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent’s behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainability. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems.},
   doi = {10.1016/j.knosys.2020.106685}
 }
 
 @article{Hochreiter:1997:Lstm,
   title={Long short-term memory},
   author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
   journal={Neural computation},
   volume={9},
   number={8},
   pages={1735--1780},
   year={1997},
   publisher={MIT Press},
   abstract={Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O . 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
   doi={10.1162/neco.1997.9.8.1735}
 }
 
 @article{Hoffman:2018:MetricsXAI,
   title={Metrics for explainable AI: Challenges and prospects},
   author={Hoffman, Robert R and Mueller, Shane T and Klein, Gary and Litman, Jordan},
   journal={arXiv preprint arXiv:1812.04608},
   year={2018},
   abstract={The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.},
   doi = {10.48550/ARXIV.1812.04608}
 }
 
 @article{HolzingerEtAl:2019:Wiley-Paper,
   year = {2019},
   author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and Müller, Heimo},
   title = {Causability and Explainability of Artificial Intelligence in Medicine},
   journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
   volume = {9},
   number = {4},
   pages = {1--13},
   abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black‐box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use‐case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system.},
   doi = {10.1002/widm.1312}
 }
 
 @article{HolzingerMueller:2021:HumanAI,
   year = {2021},
   author = {Holzinger, Andreas and Mueller, Heimo},
   title = {Toward Human-AI Interfaces to Support Explainability and Causability in Medical AI},
   journal = {IEEE COMPUTER},
   volume = {54},
   number = {10},
   pages = {78--86},
   abstract = {Our concept of causability is a measure of whether and to what extent humans can understand a given machine explanation. We motivate causability with a clinical case from cancer research. We argue for using causability in medical artificial intelligence (AI) to develop and evaluate future human-AI interfaces.},
   doi = {10.1109/MC.2021.3092610}
 }
 
 @article{HolzingerEtAl:2021:MultiModalCausabilityGNN,
   year = {2021},
   author = {Holzinger, Andreas and Malle, Bernd and Saranti, Anna and Pfeifer, Bastian},
   title = {Towards Multi-Modal Causability with Graph Neural Networks enabling Information Fusion for explainable AI},
   journal = {Information Fusion},
   volume = {71},
   number = {7},
   pages = {28--37},
   abstract = {AI is remarkably successful and outperforms human experts in certain tasks, even in complex domains such as medicine. Humans on the other hand are experts at multi-modal thinking and can embed new inputs almost instantly into a conceptual knowledge space shaped by experience. In many fields the aim is to build systems capable of explaining themselves, engaging in interactive what-if questions. Such questions, called counterfactuals, are becoming important in the rising field of explainable AI (xAI). Our central hypothesis is that using conceptual knowledge as a guiding model of reality will help to train more explainable, more robust and less biased machine learning models, ideally able to learn from fewer data. One important aspect in the medical domain is that various modalities contribute to one single result. Our main question is “How can we construct a multi-modal feature representation space (spanning images, text, genomics data) using knowledge bases as an initial connector for the development of novel explanation interface techniques?”. In this paper we argue for using Graph Neural Networks as a method-of-choice, enabling information fusion for multi-modal causability (causability – not to confuse with causality – is the measurable extent to which an explanation to a human expert achieves a specified level of causal understanding). The aim of this paper is to motivate the international xAI community to further work into the fields of multi-modal embeddings and interactive explainability, to lay the foundations for effective future human–AI interfaces. We emphasize that Graph Neural Networks play a major role for multi-modal causability, since causal links between features can be defined directly using graph structures.},
   doi = {10.1016/j.inffus.2021.01.008}
 }
 
 @article{Holzinger:2016:iML,
   year = {2016},
   author = {Holzinger, Andreas},
   title = {Interactive Machine Learning for Health Informatics: When do we need the human-in-the-loop?},
   journal = {Brain Informatics},
   volume = {3},
   number = {2},
   pages = {119--131},
   abstract = {Machine learning (ML) is the fastest growing field in computer science, and health informatics is amongst the greatest challenges. The goal of ML is to develop algorithms which can learn and improve over time and can be used for predictions. Most ML researchers concentrate on automatic Machine Learning (aML), where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from big data with many training sets. However, in the health domain, sometimes we are confronted with a small number of data sets or rare events, where aML-approaches suffer of insufficient training samples. Here interactive Machine Learning (iML) may be of help, having its roots in Reinforcement Learning (RL), Preference Learning (PL) and Active Learning (AL). The term iML is not yet well used, so we define it as algorithms that can interact with agents and can optimize their learning behaviour through these interactions, where the agents can also be human. This human-in-the-loop can be beneficial in solving computationally hard problems, e.g., subspace clustering, protein folding, or k-anonymization of health data, where human expertise can help to reduce an exponential search space through heuristic selection of samples. Therefore, what would otherwise be an NP-hard problem, reduces greatly in complexity through the input and the assistance of a human agent involved in the learning phase.},
   keywords = {interactive Machine learning, health informatics},
   doi = {10.1007/s40708-016-0042-6}
 }
 
 @incollection{HolzingerEtAl:2016:iMLExperiment,
   year = {2016},
   author = {Holzinger, Andreas and Plass, Markus and Holzinger, Katharina and Crisan, Gloria Cerasela and Pintea, Camelia-M. and Palade, Vasile},
   title = {Towards interactive Machine Learning (iML): Applying Ant Colony Algorithms to solve the Traveling Salesman Problem with the Human-in-the-Loop approach},
   booktitle = {Springer Lecture Notes in Computer Science LNCS 9817},
   publisher = {Springer},
   address = {Heidelberg, Berlin, New York},
   pages = {81--95},
   abstract = {Most Machine Learning (ML) researchers focus on automatic Machine Learning (aML) where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from the availability of "big data". However, sometimes, for example in health informatics, we are confronted not a small number of data sets or rare events, and with complex problems where aML-approaches fail or deliver unsatisfactory results. Here, interactive Machine Learning (iML) may be of help and the "human-in-the-loop" approach may be beneficial in solving computationally hard problems, where human expertise can help to reduce an exponential search space through heuristics. In this paper, experiments are discussed which help to evaluate the effectiveness of the iML-"human-in-the-loop" approach, particularly in opening the "black box", thereby enabling a human to directly and indirectly manipulating and interacting with an algorithm. For this purpose, we selected the Ant Colony Optimization (ACO) framework, and use it on the Traveling Salesman Problem (TSP) which is of high importance in solving many practical problems in health informatics, e.g. in the study of proteins.},
   keywords = {interactive Machine Learning, Human-in-the-loop, Traveling Salesman Problem, Ant Colony Optimization},
   doi = {10.1007/978-3-319-45507-56}
 }
 
 @article{Holzinger:2019:HumanLoopAPIN,
   year = {2019},
   author = {Holzinger, Andreas and Plass, Markus and Kickmeier-Rust, Michael and Holzinger, Katharina and Crişan, Gloria Cerasela and Pintea, Camelia-M. and Palade, Vasile},
   title = {Interactive machine learning: experimental evidence for the human in the algorithmic loop},
   journal = {Applied Intelligence},
   volume = {49},
   number = {7},
   pages = {2401--2414},
   doi = {10.1007/s10489-018-1361-5}
 }
 
 @incollection{HolzWoj:2022:XAIOverview,
   year = {2022},
   author = {Holzinger, Andreas and Saranti, Anna and Molnar, Christoph and Biececk, Prezemyslaw and Samek, Wojciech},
   title = {Explainable AI Methods - A Brief Overview},
   booktitle = {XXAI - Lecture Notes in Artificial Intelligence LNAI 13200},
   publisher = {Springer},
   doi = {10.1007/978-3-031-04083-2_2}
 }
 
 @article{HolzingerEtAl:2020:QualityOfExplanations,
   year = {2020},
   author = {Holzinger, Andreas and Carrington, Andre and Mueller, Heimo},
   title = {Measuring the Quality of Explanations: The System Causability Scale (SCS). Comparing Human and Machine Explanations},
   journal = {KI - Künstliche Intelligenz (German Journal of Artificial intelligence), Special Issue on Interactive Machine Learning, Edited by Kristian Kersting, TU Darmstadt},
   volume = {34},
   number = {2},
   pages = {193--198},
   doi = {10.1007/s13218-020-00636-z}
 }
 
 @article{HolzingerMueller:2022:PersonasAI,
   year = {2022},
   author = {Holzinger, Andreas and Kargl, Michaela and Kipperer, Bettina and Regitnig, Peter and Plass, Markus and Müller, Heimo},
   title = {Personas for Artificial Intelligence (AI) An Open Source Toolbox},
   journal = {IEEE Access},
   volume = {10},
   pages = {23732--23747},
   doi = {10.1109/ACCESS.2022.3154776}
 }
 
 @article{Holzinger:2022:DigitalTrans,
   year = {2022},
   author = {Holzinger, Andreas and Saranti, Anna and Angerschmid, Alessa and Retzlaff, Carl Orge and Gronauer, Andreas and Pejakovic, Vladimir and Medel, Francisco and Krexner, Theresa and Gollob, Christoph and Stampfer, Karl},
   title = {Digital Transformation in Smart Farm and Forest Operations needs Human-Centered AI: Challenges and Future Directions},
   journal = {Sensors},
   volume = {22},
   number = {8},
   pages = {3043},
   doi = {10.3390/s22083043}
 }
 
 @incollection{Holzinger:2021:TrustAI,
   year = {2021},
   author = {Holzinger, Andreas},
   title = {The Next Frontier: AI We Can Really Trust},
   booktitle = {Proceedings of the ECML PKDD 2021, CCIS 1524},
   editor = {Kamp, Michael},
   publisher = {Springer Nature},
   address = {Cham},
   pages = {1--14},
   doi = {10.1007/978-3-030-93736-2_33}
 }
 
 
 @article{HudecEtAl-2021-Interpretable,
   year = {2021},
   author = {Hudec, Miroslav and Minarikova, Erika and Mesiar, Radko and Saranti, Anna and Holzinger, Andreas},
   title = {Classification by ordinal sums of conjunctive and disjunctive functions for explainable AI and interpretable machine learning solutions},
   journal = {Knowledge Based Systems},
   volume = {220},
   pages = {106916},
   abstract = {The main goal of classification is dividing entities into several classes. The classification considering uncertainty of belonging to the classes separates entities into the classes yes, no, maybe, where it is desirable to indicate the inclination towards belonging to yes or no. Neural networks have proven their high performance in sharp classification, but the solution is not traceable and therefore difficult or impossible for a human expert to interpret and to understand. Rule-based systems are explainable in principle, however, are based on formal inference structures and also have problems with interpretability due to their high complexity. We must stress that even human experts sometimes cannot explain, but rather construct mental models of the problem and consult these models to select the best possible solution. In our work, we propose classification by aggregation functions of the mixed behaviour by the variability in ordinal sums of conjunctive and disjunctive functions. In this way, domain experts should only assign the key observations regarding considered attributes. Consequently, the variability of functions provides room for machine learning to learn the best possible option from data. Such a solution is re-traceable, reproducible, and explainable to domain experts. In this paper we discuss the proposed approach on examples and outline the research steps in interactive machine learning with a human-in-the-loop via aggregation functions.},
   doi = {10.1016/j.knosys.2021.106916}
 }
 
 @article{HusseinEtAl:2017:ImitationLearning,
   author = {Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
   title = {Imitation Learning: A Survey of Learning Methods},
   year = {2017},
   issue_date = {March 2018},
   publisher = {Association for Computing Machinery},
   address = {New York, NY, USA},
   journal = {ACM Computing Surveys},
   month = {apr},
   articleno = {21},
   numpages = {35},
   volume = {50},
   number = {2},
   issn = {0360-0300},
   url = {https://doi.org/10.1145/3054912},
   abstract = {Imitation learning techniques aim to mimic human behavior in a given task. An agent (a learning machine) is trained to perform a task from demonstrations by learning a mapping between observations and actions. The idea of teaching by imitation has been around for many years; however, the field is gaining attention recently due to advances in computing and sensing as well as rising demand for intelligent applications. The paradigm of learning by imitation is gaining popularity because it facilitates teaching complex tasks with minimal expert knowledge of the tasks. Generic imitation learning methods could potentially reduce the problem of teaching a task to that of providing demonstrations, without the need for explicit programming or designing reward functions specific to the task. Modern sensors are able to collect and transmit high volumes of data rapidly, and processors with high computational power allow fast processing that maps the sensory data to actions in a timely manner. This opens the door for many potential AI applications that require real-time perception and reaction such as humanoid robots, self-driving vehicles, human computer interaction, and computer games, to name a few. However, specialized algorithms are needed to effectively and robustly learn models as learning by imitation poses its own set of challenges. In this article, we survey imitation learning methods and present design options in different steps of the learning process. We introduce a background and motivation for the field as well as highlight challenges specific to the imitation problem. Methods for designing and evaluating imitation learning tasks are categorized and reviewed. Special attention is given to learning methods in robotics and games as these domains are the most popular in the literature and provide a wide array of problems and methodologies. We extensively discuss combining imitation learning approaches using different sources and methods, as well as incorporating other motion learning methods to enhance imitation. We also discuss the potential impact on industry, present major applications, and highlight current and future research directions.},
   doi = {10.1145/3054912}
 }
 
 %%% III %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @article{ibarz2021train,
   title={How to train your robot with deep reinforcement learning: lessons we have learned},
   author={Ibarz, Julian and Tan, Jie and Finn, Chelsea and Kalakrishnan, Mrinal and Pastor, Peter and Levine, Sergey},
   journal={The International Journal of Robotics Research},
   volume={40},
   number={4-5},
   pages={698--721},
   year={2021},
   publisher={SAGE Publications Sage UK: London, England},
   doi={10.1177/0278364920987859}
 }
 
 @article{IsbellEtAl:2006:AdaptiveSocialAgent,
   author = {Isbell, Charles Lee and Kearns, Michael and Singh, Satinder and Shelton, Christian R. and Stone, Peter and Kormann, Dave},
   year = {2006},
   title = {Cobot in LambdaMOO: An Adaptive Social Statistics Agent},
   journal = {Autonomous Agents and Multi-Agent Systems},
   volume = {13},
   pages = {327--354},
   abstract = {We describe our development of Cobot, a novel software agent who lives in LambdaMOO, a popular virtual world frequented by hundreds of users. Cobot’s goal was to become an actual part of that community. Here, we present a detailed discussion of the functionality that made him one of the objects most frequently interacted with in LambdaMOO, human or artificial. Cobot’s fundamental power is that he has the ability to collect social statistics summarizing the quantity and quality of interpersonal interactions. Initially, Cobot acted as little more than a reporter of this information; however, as he collected more and more data, he was able to use these statistics as models that allowed him to modify his own behavior. In particular, cobot is able to use this data to “self-program,” learning the proper way to respond to the actions of individual users, by observing how others interact with one another. Further, Cobot uses reinforcement learning to proactively take action in this complex social environment, and adapts his behavior based on multiple sources of human reward. Cobot represents a unique experiment in building adaptive agents who must live in and navigate social spaces.},
   doi = {10.1007/s10458-006-0005-z}
 }
 
 @misc{ISO:13482:2014,
   key={EN ISO 13482:2014},
   title ={{EN ISO 13482:2014}},
   year={2014},
   howpublished = {\url{https://www.iso.org/standard/53820.html}},
   note = {(02.02.2022)}
 }
 
 %%% JJJ %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @article{JainEtAl:2021:EpistemicUncertaintyPrediction,
   author    = {Moksh Jain and
                Salem Lahlou and
                Hadi Nekoei and
                Victor Butoi and
                Paul Bertin and
                Jarrid Rector{-}Brooks and
                Maksym Korablyov and
                Yoshua Bengio},
   title     = {{DEUP:} Direct Epistemic Uncertainty Prediction},
   journal   = {arXiv preprint arXiv:2102.08501},
   year      = {2021},
   publisher = {arXiv},
   abstract  = {Epistemic uncertainty is the part of out-of-sample prediction error due to the lack of knowledge of the learner. Whereas previous work was focusing on model variance, we propose a principled approach for directly estimating epistemic uncertainty by learning to predict generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator of epistemic uncertainty includes the effect of model bias and can be applied in non-stationary learning environments arising in active learning or reinforcement learning. In addition to demonstrating these properties of Direct Epistemic Uncertainty Prediction (DEUP), we illustrate its advantage against existing methods for uncertainty estimation on downstream tasks including sequential model optimization and reinforcement learning. We also evaluate the quality of uncertainty estimates from DEUP for probabilistic classification of images and for estimating uncertainty about synergistic drug combinations.},
   doi = {10.48550/ARXIV.2102.08501}
 }
 
 % TODO: check journal and add doi if possible
 @article{jiang:21,
   title={Temporal-logic-based reward shaping for continuing reinforcement learning tasks},
   author={Jiang, Yuqian and Bharadwaj, Suda and Wu, Bo and Shah, Rishi and Topcu, Ufuk and Stone, Peter},
   journal={Good Systems-Published Research},
   year={2021},
   publisher={University of Texas at Austin},
   abstract={The paper proposes a framework for non-experienced users to interact with average-reward RL algorithms. More precisely, it allows teachers to give high-level advice. The approach receives the trainer input using temporal logic and provides guarantees optimality independent of the advice quality. Contrary to previous work that used LTL as hard constraints, the authors use LTL formulas to shape the environment reward. Instead of removing actions violating the LTL formulae and possibly causing suboptimal behavior, they are used to guide the RL agent in learning an optimal policy. The proposed framework speeds up the learning rate in comparison to differential Q-learning (the baseline RL algorithm). Poor quality advice causes the agent to learn suboptimal behavior when actions that violate the LTL formula are disallowed. The current approach still allows the agent to learn an optimal behavior regardless of the advice quality type.}
 }
 
 @inproceedings{jiang2019neural,
   title={Neural logic reinforcement learning},
   author={Jiang, Zhengyao and Luo, Shan},
   booktitle = {Proceedings of the 36th International Conference on Machine Learning},
   pages={3110--3119},
   year={2019},
   editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
   volume = {97},
   series = {Proceedings of Machine Learning Research},
   organization={PMLR}
 }
 
 @article{Jung:2020:ExplainableEmpiricalRiskMinimization,
   title={Explainable empirical risk minimization},
   author={Jung, Alexander},
   journal={arXiv preprint arXiv:2009.01492},
   year={2020},
   abstract={The successful application of machine learning (ML) methods becomes increasingly dependent on their interpretability or explainability. Designing explainable ML systems is instrumental to ensuring transparency of automated decision-making that targets humans. The explainability of ML methods is also an essential ingredient for trustworthy artificial intelligence. A key challenge in ensuring explainability is its dependence on the specific human user ("explainee"). The users of machine learning methods might have vastly different background knowledge about machine learning principles. One user might have a university degree in machine learning or related fields, while another user might have never received formal training in high-school mathematics. This paper applies information-theoretic concepts to develop a novel measure for the subjective explainability of the predictions delivered by a ML method. We construct this measure via the conditional entropy of predictions, given a user signal. This user signal might be obtained from user surveys or biophysical measurements. Our main contribution is the explainable empirical risk minimization (EERM) principle of learning a hypothesis that optimally balances between the subjective explainability and risk. The EERM principle is flexible and can be combined with arbitrary machine learning models. We present several practical implementations of EERM for linear models and decision trees. Numerical experiments demonstrate the application of EERM to detecting the use of inappropriate language on social media.},
   doi = {10.48550/ARXIV.2009.01492}
 }
 
 %%% KKK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @article{Karalus:2021:HITL-counterfactuals,
   author    = {Jakob Karalus and
                Felix Lindner},
-  title     = {Accelerating the Convergence of Human-in-the-Loop Reinforcement Learning with Counterfactual Explanations},
+  title     = {Accelerating the Convergence of Human-in-the-Loop Reinforcement Learning
+               with Counterfactual Explanations},
   journal   = {arXiv preprint arXiv:2108.01358},
   year      = {2021},
   publisher = {arXiv},
   biburl    = {https://dblp.org/rec/journals/corr/abs-2108-01358.bib},
   bibsource = {dblp computer science bibliography, https://dblp.org},
   doi = {10.48550/ARXIV.2108.01358}
 }
 
 @article{kartoun:10,
   title={A human-robot collaborative reinforcement learning algorithm},
   author={Kartoun, Uri and Stern, Helman and Edan, Yael},
   journal={Journal of Intelligent \& Robotic Systems},
   volume={60},
   number={2},
   pages={217--239},
   year={2010},
   publisher={Springer},
   abstract = {This paper presents a new reinforcement learning algorithm that enables collaborative learning between a robot and a human. The algorithm which is based on the Q(λ) approach expedites the learning process by taking advantage of human intelligence and expertise. The algorithm denoted as CQ(λ) provides the robot with self awareness to adaptively switch its collaboration level from autonomous (self performing, the robot decides which actions to take, according to its learning function) to semi-autonomous (a human advisor guides the robot and the robot combines this knowledge into its learning function). This awareness is represented by a self test of its learning performance. The approach of variable autonomy is demonstrated and evaluated using a fixed-arm robot for finding the optimal shaking policy to empty the contents of a plastic bag. A comparison between the CQ(λ) and the traditional Q(λ)-reinforcement learning algorithm, resulted in faster convergence for the CQ(λ) collaborative reinforcement learning algorithm.},
   doi={10.1007/s10846-010-9422-y}
 }
 
 @inproceedings{KhatibEtAl:1999:RihEnvironment,
   title={Robots in human environments}, 
   author={Khatib, O. and Yokoi, K. and Brock, O. and Chang, K. and Casal, A.},
   year={1999},
   booktitle={Proceedings of the First Workshop on Robot Motion and Control. RoMoCo'99 (Cat. No.99EX353)}, 
   publisher={IEEE},
   pages={213--221},
   abstract={Discusses the basic capabilities needed to enable robots to operate in human populated environments for accomplishing both autonomous tasks and human-guided tasks. These capabilities are key to many new emerging robotic applications in service, construction, field, underwater, and space. An important characteristic of these robots is the "assistance" ability they can bring to humans in performing various physical tasks. To interact with humans and operate in their environments, these robots must be provided with the functionality of mobility and manipulation. The article presents developments of models, strategies, and algorithms concerned with a number of autonomous capabilities that are essential for robot operations in human environments. These capabilities include: integrated mobility and manipulation, cooperative skills between multiple robots, interaction ability with humans, and efficient techniques for real-time modification of collision-free path. These capabilities are demonstrated on two holonomic mobile platforms designed and built at Stanford University in collaboration with Oak Ridge National Laboratories and Nomadic Technologies.},
   doi={10.1109/ROMOCO.1999.791078}
 }
 
 @inproceedings{kim2013learning,
   title={Learning from limited demonstrations},
   author={Kim, Beomjoon and Farahmand, Amir-massoud and Pineau, Joelle and Precup, Doina},
   booktitle={Advances in Neural Information Processing Systems},
   volume={26},
   year={2013},
   publisher = {Curran Associates, Inc.},
   abstract={This method incorporates  demonstrations as linear constraints in the Approximate Policy Iteration framework, posing it as a constraint convex optimization problem. Experiments are shown with optimal and sub-optimal demonstrations on car brake control simulation and Robot path finding task.}
 }
 
 @inproceedings{Knox:2008:TAMER,
   author={Bradley Knox, W. and Stone, Peter},
   booktitle={2008 7th IEEE International Conference on Development and Learning}, 
   title={TAMER: Training an Agent Manually via Evaluative Reinforcement}, 
   year={2008},
   volume={},
   number={},
   publisher={IEEE},
   pages={292--297},
   doi={10.1109/DEVLRN.2008.4640845}}
   
 @inproceedings{knox:13,
   title={Training a robot via human feedback: A case study},
   author={Knox, W. Bradley and Stone, Peter and Breazeal, Cynthia},
   booktitle={International Conference on Social Robotics},
   editor={Guido Herrmann and Martin J. Pearson and Alexander Lenz and Paul Bremner and Adam Spiers and Ute Leonards},
   pages={460--470},
   year={2013},
   organization={Springer},
   abstract={This paper uses Tamer in real robot training. The setup is simple;  the robot has four possible actions: turn left, right, stop, or move forward. However, the robot learned five different behaviors in relation to a training artifact, i.e., following the artifact or moving away from it. The authors report that the main issues causing unsuccessful learning sessions were the lack of transparency between the robot and the teacher. In one case, the actions seemed ambiguous at the beginning of the execution, and the misunderstanding of the action duration caused rewards to the wrong actions. In other cases, the teacher did not realize that the training artifact was out of range from the robot's sensors. The problems encountered when applying the Tamer framework in physical environments illustrate the need to provide feedback to the teacher, especially if we aim to have non-expert teachers.},
   doi={10.1007/978-3-319-02675-6_46}
 }
 
 @article{KoberBagnellPeters:2013:RLRoboticsSurvey,
   title={Reinforcement learning in robotics: A survey},
   author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
   journal={The International Journal of Robotics Research},
   volume={32},
   number={11},
   pages={1238--1274},
   year={2013},
   publisher={SAGE Publications Sage UK: London, England},
   abstract={Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
   doi={10.1177/0278364913495721}
 }
 
 @article{Koditschek:2021:RoboticsCompositionalLanguage,
   title={What Is Robotics? Why Do We Need It and How Can We Get It?},
   author={Koditschek, Daniel E},
   journal={Annual Review of Control, Robotics, and Autonomous Systems},
   volume={4},
   number = {1},
   pages={1--33},
   year={2021},
   publisher={Annual Reviews},
   abstract = {Robotics is an emerging synthetic science concerned with programming work. Robot technologies are quickly advancing beyond the insights of the existing science. More secure intellectual foundations will be required to achieve better, more reliable, and safer capabilities as their penetration into society deepens. Presently missing foundations include the identification of fundamental physical limits, the development of new dynamical systems theory, and the invention of physically grounded programming languages. The new discipline needs a departmental home in the universities, which it can justify both intellectually and by its capacity to attract new diverse populations inspired by the age-old human fascination with robots.},
   doi = {10.1146/annurev-control-080320-011601}
 }
 
 @book{Koller:2009:ProbabilisticGraphicalModelsBook,
   title={Probabilistic graphical models: principles and techniques},
   author={Koller, Daphne and Friedman, Nir},
   year={2009},
   publisher={MIT press}
 }
 
 % TODO: please add doi if possible
 @inproceedings{kulick2013active,
   title={Active Learning for Teaching a Robot Grounded Relational Symbols.},
   author={Kulick, Johannes and Toussaint, Marc and Lang, Tobias and Lopes, Manuel},
   booktitle={IJCAI},
   pages={1451--1457},
   year={2013},
   organization={Citeseer}
 }
 
 @article{Kulkarni:2021:EducationAIDashboard,
   title={Demonstrating REACT: a Real-time Educational AI-powered Classroom Tool},
   author={Kulkarni, Ajay and Gkountouna, Olga},
   journal={arXiv preprint arXiv:2108.07693},
   year={2021},
   abstract={We present a demonstration of REACT, a new Real-time Educational AI-powered Classroom Tool that employs EDM techniques for supporting the decision-making process of educators. REACT is a data-driven tool with a user-friendly graphical interface. It analyzes students' performance data and provides context-based alerts as well as recommendations to educators for course planning. Furthermore, it incorporates model-agnostic explanations for bringing explainability and interpretability in the process of decision making. This paper demonstrates a use case scenario of our proposed tool using a real-world dataset and presents the design of its architecture and user interface. This demonstration focuses on the agglomerative clustering of students based on their performance (i.e., incorrect responses and hints used) during an in-class activity. This formation of clusters of students with similar strengths and weaknesses may help educators to improve their course planning by identifying at-risk students, forming study groups, or encouraging tutoring between students of different strengths.},
   doi = {10.48550/ARXIV.2108.07693}
 }
 
 
 %%% LLL %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @article{LapuschkinEtAl:2016:LRP,
    year = {2016},
    author = {Lapuschkin, Sebastian and Binder, Alexander and Montavon, Gregoire and Müller, Klaus-Robert and Samek, Wojciech},
    title = {The LRP toolbox for artificial neural networks},
    journal = {The Journal of Machine Learning Research (JMLR)},
    volume = {17},
    number = {1},
    pages = {3938--3942},
    abstract = {The Layer-wise Relevance Propagation (LRP) algorithm explains a classifier's prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself. With the LRP Toolbox we provide platform-agnostic implementations for explaining the predictions of pretrained state of the art Caffe networks and stand-alone implementations for fully connected Neural Network models. The implementations for Matlab and python shall serve as a playing field to familiarize oneself with the LRP algorithm and are implemented with readability and transparency in mind. Models and data can be imported and exported using raw text formats, Matlab's .mat files and the .npy format for numpy or plain text.}
 }
 
 @article{Lapuschkin:2019:UnmaskingCleverHans,
   title={Unmasking Clever Hans predictors and assessing what machines really learn},
   author={Lapuschkin, Sebastian and W{\"a}ldchen, Stephan and Binder, Alexander and Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
   journal={Nature communications},
   volume={10},
   number={1},
   pages={1--8},
   year={2019},
   publisher={Nature Publishing Group},
   abstract={Current learning machines have successfully solved hard application problems, reaching high accuracy and displaying seemingly intelligent behavior. Here we apply recent techniques for explaining decisions of state-of-the-art learning machines and analyze various tasks from computer vision and arcade games. This showcases a spectrum of problem-solving behaviors ranging from naive and short-sighted, to well-informed and strategic. We observe that standard performance evaluation metrics can be oblivious to distinguishing these diverse problem solving behaviors. Furthermore, we propose our semi-automated Spectral Relevance Analysis that provides a practically effective way of characterizing and validating the behavior of nonlinear learning machines. This helps to assess whether a learned model indeed delivers reliably for the problem that it was conceived for. Furthermore, our work intends to add a voice of caution to the ongoing excitement about machine intelligence and pledges to evaluate and judge some of these recent successes in a more nuanced manner.},
   doi={10.1038/s41467-019-08987-4}
 }
 
 @inproceedings{le2018hierarchical,
   title={Hierarchical imitation and reinforcement learning},
   author={Le, Hoang and Jiang, Nan and Agarwal, Alekh and Dudik, Miroslav and Yue, Yisong and Daum{\'e} III, Hal},
   booktitle={International conference on machine learning},
   editor = {Dy, Jennifer and Krause, Andreas},
   pages={2917--2926},
   year={2018},
   publisher={PMLR},
   volume = {80},
   series = {Proceedings of Machine Learning Research},
   abstract = {We study how to effectively leverage expert feedback to learn sequential decision-making policies. We focus on problems with sparse rewards and long time horizons, which typically pose significant challenges in reinforcement learning. We propose an algorithmic framework, called hierarchical guidance, that leverages the hierarchical structure of the underlying problem to integrate different modes of expert interaction. Our framework can incorporate different combinations of imitation learning (IL) and reinforcement learning (RL) at different levels, leading to dramatic reductions in both expert effort and cost of exploration. Using long-horizon benchmarks, including Montezuma’s Revenge, we demonstrate that our approach can learn significantly faster than hierarchical RL, and be significantly more label-efficient than standard IL. We also theoretically analyze labeling cost for certain instantiations of our framework.}
 }
 
 @article{LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE,
   title={PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training},
   author={Lee, Kimin and Smith, Laura and Abbeel, Pieter},
   journal={arXiv preprint arXiv:2106.05091},
   publisher = {arXiv},
   year={2021},
   abstract={Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.},
   doi = {10.48550/ARXIV.2106.05091}
 }
 
 @article{Li:2017:DRLSurvey,
   title={Deep reinforcement learning: An overview},
   author={Li, Yuxi},
   journal={arXiv preprint arXiv:1701.07274},
   publisher = {arXiv},
   year={2017},
   abstract = {We give an overview of recent exciting achievements of deep reinforcement learn- ing (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value func- tion, in particular, Deep Q-Network (DQN), policy, reward, model and planning, exploration, and knowledge. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi- agent RL, hierarchical RL, and learning to learn. Then we discuss various appli- cations of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, business management, finance, healthcare, education, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
   doi = {10.48550/ARXIV.1701.07274}
 }
 
 @article{Li:2019:HumanCenteredRLSurvey,
   title={Human-centered reinforcement learning: A survey},
   author={Li, Guangliang and Gomez, Randy and Nakamura, Keisuke and He, Bo},
   journal={IEEE Transactions on Human-Machine Systems},
   volume={49},
   number={4},
   pages={337--349},
   year={2019},
   publisher={IEEE},
   abstract={Human-centered reinforcement learning (RL), in which an agent learns how to perform a task from evaluative feed- back delivered by a human observer, has become more and more popular in recent years. The advantage of being able to learn from human feedback for a RL agent has led to increasing applicability to real-life problems. This paper describes the state-of-the-art human- centered RL algorithms and aims to become a starting point for researchers who are initiating their endeavors in human-centered RL. Moreover, the objective of this paper is to present a compre- hensive survey of the recent breakthroughs in this field and provide references to the most interesting and successful works. After start- ing with an introduction of the concepts of RL from environmental reward, this paper discusses the origins of human-centered RL and its difference from traditional RL. Then we describe different in- terpretations of human evaluative feedback, which have produced many human-centered RL algorithms in the past decade. In ad- dition, we describe research on agents learning from both human evaluative feedback and environmental rewards as well as on im- proving the efficiency of human-centered RL. Finally, we conclude with an overview of application areas and a discussion of future work and open questions.},
   doi={10.1109/THMS.2019.2912447}
 }
 
 
 @inproceedings{LiangEtAl:2017:HITLReinforcementLearn,
   author = {Liang, Huanghuang and Yang, Lu and Cheng, Hong and Tu, Wenzhe and Xu, Mengjie},
   booktitle = {2017 Chinese Automation Congress (CAC)}, 
   title = {Human-in-the-loop reinforcement learning}, 
   year = {2017},
   pages = {4511--4518},
   abstract = {This paper focuses on presenting a human-in-the-loop reinforcement learning theory framework and foreseeing its application to driving decision making. Currently, the technologies in human-vehicle collaborative driving face great challenges, and do not consider the Human-in-the-loop learning framework and Driving Decision-Maker optimization under the complex road conditions. The main content of this paper aimed at presenting a study framework as follows: (1) the basic theory and model of the hybrid reinforcement learning; (2) hybrid reinforcement learning algorithm for human drivers; (3)hybrid reinforcement learning algorithm for autopilot; (4) Driving decision-maker verification platform. This paper aims at setting up the human-machine hybrid reinforcement learning theory framework and foreseeing its solutions to two kinds of typical difficulties about human-machine collaborative Driving Decision-Maker, which provides the basic theory and key technologies for the future of intelligent driving. The paper serves as a potential guideline for the study of human-in-the-loop reinforcement learning.},
   doi={10.1109/CAC.2017.8243575}
 }
 
 @article{likmeta2020combining,
   title={Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving},
   author={Likmeta, Amarildo and Metelli, Alberto Maria and Tirinzoni, Andrea and Giol, Riccardo and Restelli, Marcello and Romano, Danilo},
   journal={Robotics and Autonomous Systems},
   volume={131},
   pages={103568},
   year={2020},
   publisher={Elsevier},
   abstract = {The design of high-level decision-making systems is a topical problem in the field of autonomous driving. In this paper, we combine traditional rule-based strategies and reinforcement learning (RL) with the goal of achieving transparency and robustness. On the one hand, the use of handcrafted rule-based controllers allows for transparency, i.e., it is always possible to determine why a given decision was made, but they struggle to scale to complex driving scenarios, in which several objectives need to be considered. On the other hand, black-box RL approaches enable us to deal with more complex scenarios, but they are usually hardly interpretable. In this paper, we combine the best properties of these two worlds by designing parametric rule-based controllers, in which interpretable rules can be provided by domain experts and their parameters are learned via RL. After illustrating how to apply parameter-based RL methods (PGPE) to this setting, we present extensive numerical simulations in the highway and in two urban scenarios: intersection and roundabout. For each scenario, we show the formalization as an RL problem and we discuss the results of our approach in comparison with handcrafted rule-based controllers and black-box RL techniques.},
   doi = {10.1016/j.robot.2020.103568}
 }
 
 @article{lin:20,
   title={A review on interactive reinforcement learning from human social feedback},
   author={Lin, Jinying and Ma, Zhen and Gomez, Randy and Nakamura, Keisuke and He, Bo and Li, Guangliang},
   journal={IEEE Access},
   volume={8},
   pages={120757--120765},
   year={2020},
   publisher={IEEE},
   abstract={Reinforcement learning agent learns how to perform a task by interacting with the environment.The use of reinforcement learning in real-life applications has been limited because of the sample efficiencyproblem. Interactive reinforcement learning has been developed to speed up the agent’s learning and facilitateto learn from ordinary people by allowing them to provide social feedback, e.g, evaluative feedback, advice orinstruction. Inspired by real-life biological learning scenarios, there could be many ways to provide feedbackfor agent learning, such as via hardware delivered, natural interaction like facial expressions, speech orgestures. The agent can even learn from feedback via unimodal or multimodal sensory input. This paperreviews methods for interactive reinforcement learning agent to learn from human social feedback and theways of delivering feedback. Finally, we discuss some open problems and possible future research directions.},
   doi={10.1109/ACCESS.2020.3006254}
 }
 
 % TODO: other publisher? rejected -> no journal? please check
 @article{LiuAbbeel:2020:UnsupervisedActivePreTraining,
   title={Unsupervised Active Pre-Training for Reinforcement Learning},
   journal={ICLR 2021},
   note={Paper was rejected due to lack of novelty},
   author={Liu, Hao and Abbeel, Pieter},
   year={2020},
   abstract={We introduce a new unsupervised pre-training method for reinforcement learning called APT, which stands for Active Pre Training. APT learns a representation and a policy initialization by actively searching for novel states in reward-free environments. We use the contrastive learning framework for learning the representation from collected transitions. The key novel idea is to collect data during pre-training by maximizing a particle based entropy computed in the learned latent representation space. By doing particle based entropy maximization, we alleviate the need for challenging density modeling and are thus able to scale our approach to image observations. APT successfully learns meaningful representations as well as policy initializations without using any reward. We empirically evaluate APT on the Atari game suite and DMControl suite by exposing task-specific reward to agent after a long unsupervised pre-training phase. On Atari games, APT achieves human-level performance on 12 games and obtains highly competitive performance compared to canonical fully supervised RL algorithms. On DMControl suite, APT beats all baselines in terms of asymptotic performance and data efficiency and dramatically improves performance on tasks that are extremely difficult for training from scratch. Importantly, the pre-trained models can be fine-tuned to solve different tasks as long as the environment does not change. Finally, we also pre-train multi-environment encoders on data from multiple environments and show generalization to a broad set of RL tasks.}
 }
 
 @article{liu2021demonstration,
   title={Demonstration actor critic},
   author={Liu, Guoqing and Zhao, Li and Zhang, Pushi and Bian, Jiang and Qin, Tao and Yu, Nenghai and Liu, Tie-Yan},
   journal={Neurocomputing},
   volume={434},
   pages={194--202},
   year={2021},
   publisher={Elsevier},
   abstract={We study the problem of Reinforcement Learning from Demonstrations (RLfD), where the agent has access to not only reward signals from the environment, but also some available expert demonstrations. Recent works absorb ingredients from imitation learning and utilize demonstration data as reward reshaping. Despite their success, these methods update policy over these states seen in the demonstration data, in the same way as other states in the state space, overlooking the validity of direct supervision signals on these states. To address this issue, we propose a novel RLfD objective function with a new shaping reward, by optimizing which can directly leverage the supervision signal on these demonstrated states. We propose a general framework for policy optimization of the proposed objective, with convergence guarantees under the classic tabular setting. Based on that, we further make some approximations based on deep neural networks, and then introduce a new practical algorithm, called Demonstration Actor Critic (DAC) in large continuous domains. Extensive experiments on a range of popular benchmark sparse-reward tasks show that our method can lead to significant performance gains over several strong and off-the-shelf baselines.},
   doi = {10.1016/j.neucom.2020.12.116}
 }
 
 @inproceedings{LiuEtAl:2018:LinearModelUTrees,
   title={Toward interpretable deep reinforcement learning with linear model u-trees},
   author={Liu, Guiliang and Schulte, Oliver and Zhu, Wang and Li, Qingcan},
   booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
   pages={414--429},
   year={2018},
   organization={Springer},
   abstract={Deep Reinforcement Learning (DRL) has achieved impressive success in many applications. A key component of many DRL models is a neural network representing a Q function, to estimate the expected cumulative reward following a state-action pair. The Q function neural network contains a lot of implicit knowledge about the RL problems, but often remains unexamined and uninterpreted. To our knowledge, this work develops the first mimic learning framework for Q functions in DRL. We introduce Linear Model U-trees (LMUTs) to approximate neural network predictions. An LMUT is learned using a novel on-line algorithm that is well-suited for an active play setting, where the mimic learner observes an ongoing interaction between the neural net and the environment. Empirical evaluation shows that an LMUT mimics a Q function substantially better than five baseline methods. The transparent tree structure of an LMUT facilitates understanding the network’s learned strategic knowledge by analyzing feature influence, extracting rules, and highlighting the super-pixels in image inputs.},
   doi={10.1007/978-3-030-10928-8_25}
 }
 
 @article{LiuGuoMahmud:2021:HITLErrorDetectionFramework,
   title={When and Why does a Model Fail? A Human-in-the-loop Error Detection Framework for Sentiment Analysis},
   author={Liu, Zhe and Guo, Yufan and Mahmud, Jalal},
   journal={arXiv preprint arXiv:2106.00954},
   publisher = {arXiv},
   year={2021},
   abstract={Although deep neural networks have been widely employed and proven effective in sen- timent analysis tasks, it remains challenging for model developers to assess their models for erroneous predictions that might exist prior to deployment. Once deployed, emergent er- rors can be hard to identify in prediction run- time and impossible to trace back to their sources. To address such gaps, in this paper we propose an error detection framework for sentiment analysis based on explainable fea- tures. We perform global-level feature valida- tion with human-in-the-loop assessment, fol- lowed by an integration of global and local- level feature contribution analysis. Experimen- tal results show that, given limited human-in- the-loop intervention, our method is able to identify erroneous model predictions on un- seen data with high precision.},
   doi = {10.48550/ARXIV.2106.00954}
 }
 
 @article{LuetjensEverettHow:2018:RLModelUncertainty,
   author    = {Bjoern Lutjens and
                Michael Everett and
                Jonathan P. How},
   title     = {Safe Reinforcement Learning with Model Uncertainty Estimates},
   journal   = {arXiv preprint arXiv:1810.08700},
   year      = {2018},
   abstract  = {Many current autonomous systems are being designed with a strong reliance on black box predictions from deep neural networks (DNNs). However, DNNs tend to be overconfident in predictions on unseen data and can give unpredictable results for far-from-distribution test data. The importance of predictions that are robust to this distributional shift is evident for safety-critical applications, such as collision avoidance around pedestrians. Measures of model uncertainty can be used to identify unseen data, but the state-of-the-art extraction methods such as Bayesian neural networks are mostly intractable to compute. This paper uses MC-Dropout and Bootstrapping to give computationally tractable and parallelizable uncertainty estimates. The methods are embedded in a Safe Reinforcement Learning framework to form uncertainty-aware navigation around pedestrians. The result is a collision avoidance policy that knows what it does not know and cautiously avoids pedestrians that exhibit unseen behavior. The policy is demonstrated in simulation to be more robust to novel observations and take safer actions than an uncertainty-unaware baseline.},
   doi = {10.48550/ARXIV.1810.08700}
 }
 
 @inproceedings{lyu2019sdrl,
   title={SDRL: interpretable and data-efficient deep reinforcement learning leveraging symbolic planning},
   author={Lyu, Daoming and Yang, Fangkai and Liu, Bo and Gustafson, Steven},
   booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
   volume={33},
   numer={01},
   pages={2970--2977},
   year={2019},
   doi={10.1609/aaai.v33i01.33012970 }
 }
 
 %%% MMM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 @inproceedings{macglashan2017interactive,
   title={Interactive learning from policy-dependent human feedback},
   author={MacGlashan, James and Ho, Mark K and Loftin, Robert and Peng, Bei and Wang, Guan and Roberts, David L and Taylor, Matthew E and Littman, Michael L},
   booktitle = {Proceedings of the 34th International Conference on Machine Learning},
   editor = {Precup, Doina and Teh, Yee Whye},
   volume = {70},
   series = {Proceedings of Machine Learning Research},
   pages={2285--2294},
   year={2017},
   publisher={PMLR},
   abstract = {This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner’s current policy. We present empirical results that show this assumption to be false—whether human trainers give a positive or negative feedback for a decision is influenced by the learner’s current policy. Based on this insight, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that COACH can successfully learn multiple behaviors on a physical robot.}
 }
   
 @inproceedings{MadumalEtAl:2020:CausalRLCFs,
   title={Explainable reinforcement learning through a causal lens},
   author={Madumal, Prashan and Miller, Tim and Sonenberg, Liz and Vetere, Frank},
   booktitle={Proceedings of the AAAI conference on artificial intelligence},
   volume={34},
   pages={2493--2500},
   year={2020},
   abstract={Prominent theories in cognitive science propose that humans understand and represent the knowledge of the world through causal relationships. In making sense of the world, we build causal models in our mind to encode cause-effect relations of events and use these to explain why new events happen by referring to counterfactuals — things that did not happen. In this paper, we use causal models to derive causal explanations of the behaviour of model-free reinforcement learning agents. We present an approach that learns a structural causal model during reinforcement learning and encodes causal relationships between variables of interest. This model is then used to generate explanations of behaviour based on counterfactual analysis of the causal model. We computationally evaluate the model in 6 domains and measure performance and task prediction accuracy. We report on a study with 120 participants who observe agents playing a real-time strategy game (Starcraft II) and then receive explanations of the agents' behaviour. We investigate: 1) participants' understanding gained by explanations through task prediction; 2) explanation satisfaction and 3) trust. Our results show that causal model explanations perform better on these measures compared to two other baseline explanation models.},
   doi={10.1609/aaai.v34i03.5631}
 }
 
 @article{Madumal:2020:DistalEF,
   title={Distal Explanations for Model-free Explainable Reinforcement Learning},
   author={Prashan Madumal and Tim Miller and Liz Sonenberg and Frank Vetere},
   journal={arXiv preprint arXiv:2001.10284},
   year={2020},
   publisher = {arXiv},
   abstract={In this paper we introduce and evaluate a distal explanation model for model-free reinforcement learning agents that can generate explanations for `why' and `why not' questions. Our starting point is the observation that causal models can generate opportunity chains that take the form of `A enables B and B causes C'. Using insights from an analysis of 240 explanations generated in a human-agent experiment, we define a distal explanation model that can analyse counterfactuals and opportunity chains using decision trees and causal models. A recurrent neural network is employed to learn opportunity chains, and decision trees are used to improve the accuracy of task prediction and the generated counterfactuals. We computationally evaluate the model in 6 reinforcement learning benchmarks using different reinforcement learning algorithms. From a study with 90 human participants, we show that our distal explanation model results in improved outcomes over three scenarios compared with two baseline explanation models.},
   doi = {10.48550/ARXIV.2001.10284}
 }
 
 % there is no doi for the following paper
 @inproceedings{MandelEtAl:2017ActionsInHITL,
   title={Where to add actions in human-in-the-loop reinforcement learning},
   author={Mandel, Travis and Liu, Yun-En and Brunskill, Emma and Popovi{\'c}, Zoran},
   booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
   year={2017},
   abstract={In order for reinforcement learning systems to learn quickly in vast action spaces such as the space of all possible pieces of text or the space of all images, leveraging human intuition and creativity is key. However, a human-designed action space is likely to be initially imperfect and limited; furthermore, humans may improve at creating useful actions with practice or new information. Therefore, we propose a framework in which a human adds actions to a reinforcement learning system over time to boost performance. In this setting, however, it is key that we use human effort as efficiently as possible, and one significant danger is that humans waste effort adding actions at places (states) that aren't very important. Therefore, we propose Expected Local Improvement (ELI), an automated method which selects states at which to query humans for a new action. We evaluate ELI on a variety of simulated domains adapted from the literature, including domains with over a million actions and domains where the simulated experts change over time. We find ELI demonstrates excellent empirical performance, even in settings where the synthetic "experts" are quite poor.}
 }
 
 @misc{Marletto:2021:ScienceCanCant,
   title={The Science of Can and Can’t: A Physicist’s Journey through the Land of Counterfactuals},
   author={Marletto, C},
   year={2021},
   publisher={Vintage Books: New York, NY, USA}
 }
 
 @inproceedings{martinez2016learning,
   title={Learning relational dynamics of stochastic domains for planning},
   author={Mart{\'\i}nez, David and Alenya, Guillem and Torras, Carme and Ribeiro, Tony and Inoue, Katsumi},
   booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
   volume={26},
   pages={235--243},
   year={2016}
 }
 
 @article{martinez2017relational,
   title={Relational reinforcement learning with guided demonstrations},
   author={Mart{\'\i}nez, David and Alenya, Guillem and Torras, Carme},
   journal={Artificial Intelligence},
   volume={247},
   pages={295--312},
   year={2017},
   note = {Special Issue on AI and Robotics},
   publisher={Elsevier},
   abstract = {Model-based reinforcement learning is a powerful paradigm for learning tasks in robotics. However, in-depth exploration is usually required and the actions have to be known in advance. Thus, we propose a novel algorithm that integrates the option of requesting teacher demonstrations to learn new domains with fewer action executions and no previous knowledge. Demonstrations allow new actions to be learned and they greatly reduce the amount of exploration required, but they are only requested when they are expected to yield a significant improvement because the teacher's time is considered to be more valuable than the robot's time. Moreover, selecting the appropriate action to demonstrate is not an easy task, and thus some guidance is provided to the teacher. The rule-based model is analyzed to determine the parts of the state that may be incomplete, and to provide the teacher with a set of possible problems for which a demonstration is needed. Rule analysis is also used to find better alternative models and to complete subgoals before requesting help, thereby minimizing the number of requested demonstrations. These improvements were demonstrated in a set of experiments, which included domains from the international planning competition and a robotic task. Adding teacher demonstrations and rule analysis reduced the amount of exploration required by up to 60\% in some domains, and improved the success ratio by 35\% in other domains.},
   doi = {10.1016/j.artint.2015.02.006}
 }
 
 @article{MatthewsonPilarski:2022:DesigningAndEvaluatingHCIML,
   title={A Brief Guide to Designing and Evaluating Human-Centered Interactive Machine Learning}, author={Kory W. Mathewson and Patrick M. Pilarski},
   year={2022},
   journal={arXiv preprint arXiv:2204.09622},
   abstract = {Interactive machine learning (IML) is a field of research that explores how to leverage both human and computational abilities in decision making systems. IML represents a collaboration between multiple complementary human and machine intelligent systems working as a team, each with their own unique abilities and limitations. This teamwork might mean that both systems take actions at the same time, or in sequence. Two major open research questions in the field of IML are: "How should we design systems that can learn to make better decisions over time with human interaction?" and "How should we evaluate the design and deployment of such systems?" A lack of appropriate consideration for the humans involved can lead to problematic system behaviour, and issues of fairness, accountability, and transparency. Thus, our goal with this work is to present a human-centred guide to designing and evaluating IML systems while mitigating risks. This guide is intended to be used by machine learning practitioners who are responsible for the health, safety, and well-being of interacting humans. An obligation of responsibility for public interaction means acting with integrity, honesty, fairness, and abiding by applicable legal statutes. With these values and principles in mind, we as a machine learning research community can better achieve goals of augmenting human skills and abilities. This practical guide therefore aims to support many of the responsible decisions necessary throughout the iterative design, development, and dissemination of IML systems. }
 }
 
 @article{Mehrabi:2021:SurveyBiasFairness,
   title={A Survey on Bias and Fairness in Machine Learning},
   author={Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
   journal={ACM Computing Surveys (CSUR)},
   volume={54},
   number={6},
   pages={1--35},
   year={2021},
   publisher={ACM New York, NY, USA},
   abstract={With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
   doi = {10.1145/3457607}
 }
 
 @article{milani2022survey,
   title={A Survey of Explainable Reinforcement Learning},
   author={Milani, Stephanie and Topin, Nicholay and Veloso, Manuela and Fang, Fei},
   journal={arXiv preprint arXiv:2202.08434},
   publisher = {arXiv},
   year={2022},
   doi = {10.48550/ARXIV.2005.06247}
 }
 
 @article{Miller:2019:xAISocialSciencesInsights,
   title={Explanation in artificial intelligence: Insights from the social sciences},
   author={Miller, Tim},
   journal={Artificial intelligence},
   volume={267},
   pages={1--38},
   year={2019},
   publisher={Elsevier},
   doi={10.1016/j.artint.2018.07.007},
   abstract={There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.}
 }
 
 @article{Mnih:2013:PlayingAtariDeepRL,
   title={Playing atari with deep reinforcement learning},
   author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
   journal={arXiv preprint arXiv:1312.5602},
   year={2013},
   publisher = {arXiv},
   abstract={We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
   doi = {10.48550/ARXIV.1312.5602}
 }
 
 @article{MuratoreEtAl:2021:RobotLearningReview,
   title={Robot Learning from Randomized Simulations: A Review},
   author={Muratore, Fabio and Ramos, Fabio and Turk, Greg and Yu, Wenhao and Gienger, Michael and Peters, Jan},
   journal={arXiv preprint arXiv:2111.00956},
   publisher = {arXiv},
   year={2021},
   abstract={The rise of deep learning has caused a paradigm shift in robotics research, favoring methods that require large amounts of data. It is prohibitively expensive to generate such data sets on a physical platform. Therefore, state-of-the-art approaches learn in simulation where data generation is fast as well as inexpensive and subsequently transfer the knowledge to the real robot (sim-to-real). Despite becoming increasingly realistic, all simulators are by construction based on models, hence inevitably imperfect. This raises the question of how simulators can be modified to facilitate learning robot control policies and overcome the mismatch between simulation and reality, often called the 'reality gap'. We provide a comprehensive review of sim-to-real research for robotics, focusing on a technique named 'domain randomization' which is a method for learning from randomized simulations. },
   doi = {10.48550/ARXIV.2111.00956}
 }
 
 %%% NNN %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @inproceedings{nair2018overcoming,
   title={Overcoming exploration in reinforcement learning with demonstrations},
   author={Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
   booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
   pages={6292--6299},
   year={2018},
   organization={IEEE},
   abstract={Leveraging demonstrations in Deep RL using auxiliary behavior cloning objective function, handling sub-optimal demonstrations by considering behavior cloning loss where demonstrator policy is better than the actor network policy. The proposed approach shows promise on even harder robotics task like Pushing, Sliding, Pick and place as compared to the tasks considered in previous work},
   doi={10.1109/ICRA.2018.8463162}
 }
 
 @article{Najar:2021:RLWithHumanAdvice,
   title={Reinforcement learning with human advice: a survey},
   author={Najar, Anis and Chetouani, Mohamed},
   journal={Frontiers in Robotics and AI},
   volume={8},
   year={2021},
   publisher={Frontiers Media SA},
   abstract={In this paper, we provide an overview of the existing methods for integrating human advice into a reinforcement learning process. We first propose a taxonomy of the different forms of advice that can be provided to a learning agent. We then describe the methods that can be used for interpreting advice when its meaning is not determined beforehand. Finally, we review different approaches for integrating advice into the learning process.},
   doi={10.3389/frobt.2021.584075}
 }
 
 @article{NeftciAverbeck:2019:RLBiologicalSystems,
   title={Reinforcement learning in artificial and biological systems},
   author={Neftci, Emre O and Averbeck, Bruno B},
   journal={Nature Machine Intelligence},
   volume={1},
   number={3},
   pages={133--143},
   year={2019},
   publisher={Nature Publishing Group},
   abstract={There is and has been a fruitful flow of concepts and ideas between studies of learning in biological and artificial systems. Much early work that led to the development of reinforcement learning (RL) algorithms for artificial systems was inspired by learning rules first developed in biology by Bush and Mosteller, and Rescorla and Wagner. More recently, temporal-difference RL, devel- oped for learning in artificial agents, has provided a foundational framework for interpreting the activity of dopamine neurons. In this Review, we describe state-of-the-art work on RL in biological and artificial agents. We focus on points of contact between these disciplines and identify areas where future research can benefit from information flow between these fields. Most work in biological systems has focused on simple learning problems, often embedded in dynamic environments where flexibility and ongoing learning are important, similar to real-world learning problems faced by biological systems. In contrast, most work in artificial agents has focused on learning a single complex problem in a static environment. Moving forward, work in each field will benefit from a flow of ideas that represent the strengths within each discipline.},
   doi={10.1038/s42256-019-0025-4}
 }
 
 
 @article{NieBrunskillWagner:2021:LearningPolicies,
   title={Learning when-to-treat policies},
   author={Nie, Xinkun and Brunskill, Emma and Wager, Stefan},
   journal={Journal of the American Statistical Association},
   volume={116},
   number={533},
   pages={392--409},
   year={2021},
   publisher={Taylor \& Francis},
   abstract={Many applied decision-making problems have a dynamic component: The policymaker needs not only to choose whom to treat, but also when to start which treatment. For example, a medical doctor may choose between postponing treatment (watchful waiting) and prescribing one of several available treatments during the many visits from a patient. We develop an “advantage doubly robust” estimator for learning such dynamic treatment rules using observational data under the assumption of sequential ignorability. We prove welfare regret bounds that generalize results for doubly robust learning in the single-step setting, and show promising empirical performance in several different contexts. Our approach is practical for policy optimization, and does not need any structural (e.g., Markovian) assumptions.},
   doi={10.1080/01621459.2020.1831925}
 }
 
 @inproceedings{ng:99,
   title={Policy invariance under reward transformations: Theory and application to reward shaping},
   author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
   booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
   publisher = {Morgan Kaufmann Publishers Inc.},
   volume={99},
   numpages = {10},
   series = {ICML '99},
   pages={278--287},
   year={1999},
   abstract ={This paper investigates conditions under which modifications to the reward function of a Markov decision process preserve the optimal policy. It is shown that, besides the positive linear transformation familiar from utility theory, one can add a reward for transitions between states that is expressible as the difference in value of an arbitrary potential function applied to those states. Furthermore, this is shown to be a necessary condition for invariance, in the sense that any other transformation may yield suboptimal policies unless further assumptions are made about the underlying MDP. These results shed light on the practice of reward shaping, a method used in reinforcement learning whereby additional training rewards are used to guide the learning agent. In particular, some well-known "bugs" in reward shaping procedures are shown to arise from non-potential-based rewards, and methods are given for constructing shaping potentials corresponding to distance-based and subgoal-based heuristics. We show that such potentials can lead to substantial reductions in learning time.}
 }
 
 @inproceedings{nguyen2019review,
   title={Review of Deep Reinforcement Learning for Robot Manipulation},
   author={Nguyen, Hai and La, Hung},
   booktitle={2019 Third IEEE International Conference on Robotic Computing (IRC)},
   pages={590--595},
   year={2019},
   publisher={IEEE},
   doi={10.1109/IRC.2019.00120}
 }
 %%% PPP %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @article{PapallasEtAl:2020:OnlineReplanningTrajectories,
   title={Online Replanning with Human-in-The-Loop for Non-Prehensile Manipulation in Clutter—A Trajectory Optimization based Approach},
   author={Papallas, Rafael and Cohn, Anthony G and Dogar, Mehmet R},
   journal={IEEE Robotics and Automation Letters},
   volume={5},
   number={4},
   pages={5377--5384},
   year={2020},
   publisher={IEEE},
   abstract={We are interested in the problem where a number of robots, in parallel, are trying to solve reaching through clutter problems in a simulated warehouse setting. In such a setting, we investigate the performance increase that can be achieved by using a human-in-the-loop providing guidance to robot planners. These manipulation problems are challenging for autonomous planners as they have to search for a solution in a high-dimensional space. In addition, physics simulators suffer from the uncertainty problem where a valid trajectory in simulation can be invalid when executing the trajectory in the real-world. To tackle these problems, we propose an online-replanning method with a human-in-the-loop. This system enables a robot to plan and execute a trajectory autonomously, but also to seek high-level suggestions from a human operator if required at any point during execution. This method aims to minimize the human effort required, thereby increasing the number of robots that can be guided in parallel by a single human operator. We performed experiments in simulation and on a real robot, using an experienced and a novice operator. Our results show a significant increase in performance when using our approach in a simulated warehouse scenario and six robots.},
   doi={10.1109/LRA.2020.3006826}
 }
 
 @book{Pearl:2009:Causality,
    year = {2009},
    author = {Pearl, Judea},
    title = {Causality: Models, Reasoning, and Inference (2nd Edition)},
    publisher = {Cambridge University Press},
    address = {Cambridge},
    abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 3,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.}
 }
 
 @article{Pearl:2018:CausalCounterfactualInference,
   title={Causal and counterfactual inference},
   author={Pearl, Judea},
   journal={The Handbook of Rationality},
   pages={1--41},
   year={2018},
   abstract={All accounts of rationality presuppose knowledge of how actions affect the state of the world and how the world would change had alternative actions been taken. The paper presents a framework called Structural Causal Model (SCM) which operationalizes this knowledge and explicates how it can be derived from both theories and data. In particular, we show how counterfactuals are computed and how they can be embedded in a calculus that solves critical problems in the empirical sciences}
 }
 
 @book{Pearl:2018:TheBookOfWhy,
   title={The book of why: the new science of cause and effect},
   author={Pearl, Judea and Mackenzie, Dana},
   year={2018},
   publisher={Basic books}
 }
 
 @article{Pearl:2000:ModelsReasoningInference,
   title={Models, reasoning and inference},
   author={Pearl, Judea and others},
   journal={Cambridge, UK: CambridgeUniversityPress},
   volume={19},
   pages={2},
   year={2000}
 }
 
-@inproceedings{PengEtAl:2016:AdaptingAgentSpeed,  
-author = {Peng, Bei and MacGlashan, James and Loftin, Robert and Littman, Michael L. and Roberts, David L. and Taylor, Matthew E.},  
-title = {{A Need for Speed: Adapting Agent Action Speed to Improve Task Learning from Non-Expert Humans}},  
-booktitle = {{Proceedings of the 2016 International Conference on Autonomous Agents and Multiagent Systems ( {AAMAS} )}},  
-month = may,  
-year = {2016},  
-video = {[https://www.youtube.com/watch?v=AJQSGD_XPrk](https://www.youtube.com/watch?v=AJQSGD_XPrk)},  
-month_numeric = {5}  
-}
-
 % TODO: add doi if possible
 @inproceedings{PenkovR19,
   author    = {Svetlin Penkov and
                Subramanian Ramamoorthy},
   title     = {Learning Programmatically Structured Representations with Perceptor Gradients},
   booktitle = {Proceedings of the 7th International Conference on Learning Representations},
   publisher = {OpenReview.net},
   year      = {2019},
   url       = {https://openreview.net/forum?id=SJggZnRcFQ},
   timestamp = {Thu, 25 Jul 2019 14:25:40 +0200},
   biburl    = {https://dblp.org/rec/conf/iclr/PenkovR19.bib},
   bibsource = {dblp computer science bibliography, https://dblp.org}
 }
 
 @article{Pfeifer:2021:NetworkModuleDetection,
   title={Network module detection from multi-modal node features with a greedy decision forest for actionable explainable ai},
   author={Pfeifer, Bastian and Saranti, Anna and Holzinger, Andreas},
   journal={arXiv preprint arXiv:2108.11674},
   year={2021},
   abstract={Network-based algorithms are used in most domains of research and industry in a wide variety of applications and are of great practical use. In this work, we demonstrate subnetwork detection based on multi-modal node features using a new Greedy Decision Forest for better interpretability. The latter will be a crucial factor in retaining experts and gaining their trust in such algorithms in the future. To demonstrate a concrete application example, we focus in this paper on bioinformatics and systems biology with a special focus on biomedicine. However, our methodological approach is applicable in many other domains as well. Systems biology serves as a very good example of a field in which statistical data-driven machine learning enables the analysis of large amounts of multi-modal biomedical data. This is important to reach the future goal of precision medicine, where the complexity of patients is modeled on a system level to best tailor medical decisions, health practices and therapies to the individual patient. Our glass-box approach could help to uncover disease-causing network modules from multi-omics data to better understand diseases such as cancer.},
   doi = {10.48550/ARXIV.2108.11674}
 }
 
 @article{popova2018deep,
   title={Deep reinforcement learning for de novo drug design},
   author={Popova, Mariya and Isayev, Olexandr and Tropsha, Alexander},
   journal={Science advances},
   volume={4},
   number={7},
   pages={eaap7885},
   year={2018},
   publisher={American Association for the Advancement of Science},
   abstract = {We introduce an artificial intelligence approach to de novo design of molecules with desired physical or biological properties. We have devised and implemented a novel computational strategy for de novo design of molecules with desired properties termed ReLeaSE (Reinforcement Learning for Structural Evolution). On the basis of deep and reinforcement learning (RL) approaches, ReLeaSE integrates two deep neural networks—generative and predictive—that are trained separately but are used jointly to generate novel targeted chemical libraries. ReLeaSE uses simple representation of molecules by their simplified molecular-input line-entry system (SMILES) strings only. Generative models are trained with a stack-augmented memory network to produce chemically feasible SMILES strings, and predictive models are derived to forecast the desired properties of the de novo–generated compounds. In the first phase of the method, generative and predictive models are trained separately with a supervised learning algorithm. In the second phase, both models are trained jointly with the RL approach to bias the generation of new chemical structures toward those with the desired physical and/or biological properties. In the proof-of-concept study, we have used the ReLeaSE method to design chemical libraries with a bias toward structural complexity or toward compounds with maximal, minimal, or specific range of physical properties, such as melting point or hydrophobicity, or toward compounds with inhibitory activity against Janus protein kinase 2. The approach proposed herein can find a general use for generating targeted chemical libraries of novel compounds optimized for either a single desired property or multiple properties.},
   doi = {10.1126/sciadv.aap7885}
 }
 
 @article{PujolDustdar:2021:FogRobotics,
   title={Fog Robotics—Understanding the Research Challenges},
   author={Pujol, Victor Casamayor and Dustdar, Schahram},
   journal={IEEE Internet Computing},
   volume={25},
   number={5},
   pages={10--17},
   year={2021},
   publisher={IEEE},
   abstract={Fog robotics is an emerging topic that derives from cloud robotics, but similarly as fog computing, the applications require low latency connections in order to be useful in real life environments. This article presents a new perspective to the topic in which not only robotics takes advantage of the fog computing paradigm, but fog computing is able to leverage the robotics technology in order to enhance its features. This work highlights the benefits obtained by each technology when it is mixed with the other and sketches the relevant topics to research in order to make this partnership possible.},
   doi={10.1109/MIC.2021.3060963}
 }
 
 @inproceedings{PuiuttaVeith:2020:xAIRLSurvey,
   title={Explainable reinforcement learning: A survey},
   author={Puiutta, Erika and Veith, Eric MSP},
   booktitle={International Cross-Domain Conference for Machine Learning and Knowledge Extraction},
   pages={77--95},
   year={2020},
   organization={Springer},
   abstract={Explainable Artificial Intelligence (XAI), i.e., the development of more transparent and interpretable AI models, has gained increased traction over the last few years. This is due to the fact that, in conjunction with their growth into powerful and ubiquitous tools, AI models exhibit one detrimental characteristic: a performance-transparency trade-off. This describes the fact that the more complex a model’s inner workings, the less clear it is how its predictions or decisions were achieved. But, especially considering Machine Learning (ML) methods like Reinforcement Learning (RL) where the system learns autonomously, the necessity to understand the underlying reasoning for their decisions becomes apparent. Since, to the best of our knowledge, there exists no single work offering an overview of Explainable Reinforcement Learning (XRL) methods, this survey attempts to address this gap. We give a short summary of the problem, a definition of important terms, and offer a classification and assessment of current XRL methods. We found that a) the majority of XRL methods function by mimicking and simplifying a complex model instead of designing an inherently simple one, and b) XRL (and XAI) methods often neglect to consider the human side of the equation, not taking into account research from related fields like psychology or philosophy. Thus, an interdisciplinary effort is needed to adapt the generated explanations to a (non-expert) human user in order to effectively progress in the field of XRL and XAI in general.},
   doi={doi.org/10.1007/978-3-030-57321-8_5}
 }
 
 @book{Pumperla:2019:DeepLearningGameOfGo,
   title={Deep learning and the game of Go},
   author={Pumperla, Max and Ferguson, Kevin},
   volume={231},
   year={2019},
   publisher={Manning Publications Company},
 }
 
 %%% RRR %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @inproceedings{Ribeiro:2016:WhyShouldITrustYou,
   title={"Why should i trust you?" Explaining the predictions of any classifier},
   author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
   booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
   publisher = {Association for Computing Machinery},
   series = {KDD '16},
   pages={1135--1144},
   year={2016},
   abstract={Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
   doi = {10.1145/2939672.2939778}
 }
 
 @article{RodriguezEtAl:2021:DeepCovidxAI,
   title={Deepcovid: An operational deep learning-driven framework for explainable real-time covid-19 forecasting},
   author={Rodriguez, Alexander and Tabassum, Anika and Cui, Jiaming and Xie, Jiajia and Ho, Javen and Agarwal, Pulak and Adhikari, Bijaya and Prakash, B Aditya},
   journal={medRxiv},
   pages={2020--09},
   year={2021},
   publisher={Cold Spring Harbor Laboratory Press},
   abstract={How do we forecast an emerging pandemic in real time in a purely data-driven manner? How to leverage rich heterogeneous data based on various signals such as mobility, testing, and/or disease exposure for forecasting? How to handle noisy data and generate uncertainties in the forecast? In this paper, we present DEEPCOVID, an operational deep learning framework designed for real-time COVID-19 forecasting. DEEP-COVID works well with sparse data and can handle noisy heterogeneous data signals by propagating the uncertainty from the data in a principled manner resulting in meaningful uncertainties in the forecast. The deployed framework also consists of modules for both real-time and retrospective exploratory analysis to enable interpretation of the forecasts. Results from real-time predictions (featured on the CDC website and FiveThirtyEight.com) since April 2020 indicates that our approach is competitive among the methods in the COVID-19 Forecast Hub, especially for short-term predictions.},
   doi={10.1101/2020.09.28.20203109}
 }
 
 
 @article{RoyEtAl:2021:RLRoboticsChallenges,
   title={From Machine Learning to Robotics: Challenges and Opportunities for Embodied Intelligence},
   author={Roy, Nicholas and Posner, Ingmar and Barfoot, Tim and Beaudoin, Philippe and Bengio, Yoshua and Bohg, Jeannette and Brock, Oliver and Depatie, Isabelle and Fox, Dieter and Koditschek, Dan and others},
   journal={arXiv preprint arXiv:2110.15245},
   publisher = {arXiv},
   year={2021},
   abstract={Machine learning has long since become a keystone technology, accelerating science and applications in a broad range of domains. Consequently, the notion of applying learning methods to a particular problem set has become an established and valuable modus operandi to advance a particular field. In this article we argue that such an approach does not straightforwardly extended to robotics — or to embodied intelligence more generally: systems which engage in a purposeful exchange of energy and information with a physical environment. In particular, the purview of embodied intelligent agents extends significantly beyond the typical considerations of main-stream machine learning approaches, which typically (i) do not consider operation under conditions significantly different from those encountered during training; (ii) do not consider the often substantial, long-lasting and potentially safety-critical nature of interactions during learning and deployment; (iii) do not require ready adaptation to novel tasks while at the same time (iv) effectively and efficiently curating and extending their models of the world through targeted and deliberate actions. In reality, therefore, these limitations result in learning-based systems which suffer from many of the same operational shortcomings as more traditional, engineering-based approaches when deployed on a robot outside a well defined, and often narrow operating envelope. Contrary to viewing embodied intelligence as another application domain for machine learning, here we argue that it is in fact a key driver for the advancement of machine learning technology. In this article our goal is to highlight challenges and opportunities that are specific to embodied intelligence and to propose research directions which may significantly advance the state-of-the-art in robot learning.},
   doi = {10.48550/ARXIV.2110.15245}
 }
 
 @inproceedings{RuanEtAl:2019:Quizbot,
   title={Quizbot: A dialogue-based adaptive learning system for factual knowledge},
   author={Ruan, Sherry and Jiang, Liwei and Xu, Justin and Tham, Bryce Joe-Kun and Qiu, Zhengneng and Zhu, Yeshuang and Murnane, Elizabeth L and Brunskill, Emma and Landay, James A},
   booktitle={Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
   pages={1--13},
   year={2019},
   abstract={
   Advances in conversational AI have the potential to enable more engaging and effective ways to teach factual knowledge. To investigate this hypothesis, we created QuizBot, a dialogue-based agent that helps students learn factual knowledge in science, safety, and English vocabulary. We evaluated QuizBot with 76 students through two within-subject studies against a flashcard app, the traditional medium for learning factual knowledge. Though both systems used the same algorithm for sequencing materials, QuizBot led to students recognizing (and recalling) over 20\% more correct answers than when students used the flashcard app. Using a conversational agent is more time consuming to practice with, but in a second study, of their own volition, students spent 2.6x more time learning with QuizBot than with flashcards and reported preferring it strongly for casual learning. Our results in this second study showed QuizBot yielded improved learning gains over flashcards on recall. These results suggest that educational chatbot systems may have beneficial use, particularly for learning outside of traditional settings.},
   doi={10.1145/3290605.3300587}
 }
 
 %%% SSS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @article{SalviniEtAl:2021:ISO13482:2014,
   author = {Salvini, Pericle and Paez-Granados, Diego and Billard, Aude},
   title = {On the Safety of Mobile Robots Serving in Public Spaces: Identifying Gaps in EN ISO 13482:2014 and Calling for a New Standard},
   year = {2021},
   journal = {Transactions on Human-Robot Interaction},
   issue_date = {September 2021},
   publisher = {Association for Computing Machinery},
   volume = {10},
   number = {3},
   numpages = {27},
   abstract = {Since 2014, a specific standard has been dedicated for the safety certification of personal care robots, which operate in close proximity to humans. These robots serve as information providers, object transporters, personal mobility carriers, and security patrollers. In this article, we point out the shortcomings concerning EN ISO 13482:2014, which encompasses guidelines regarding the safety and design of personal care robots. In particular, we argue that the current standard is not suitable for guaranteeing people's safety when these robots operate in public spaces. Specifically, the standard lacks requirements to protect pedestrians and bystanders. The guideline implicitly assumes that private spaces, such as households and offices, present the same hazards as in public spaces. We highlight the existence of at least three properties pertaining to robots’ use in public spaces. These properties include (1) crowds, (2) social norms and proxemics rules, and (3) people's misbehaviours. We discuss how these properties impact robots’ safety. This article aims to raise stakeholders’ awareness on individuals’ safety when robots are deployed in public spaces. This could be achieved by integrating the gaps present in EN ISO 13482:2014 or by creating a new dedicated standard.},
   doi = {10.1145/3442678}
 }
 
 @inproceedings{Saranti:2019:LearningCompetencePGMs,
   title={Insights into learning competence through probabilistic graphical models},
   author={Saranti, Anna and Taraghi, Behnam and Ebner, Martin and Holzinger, Andreas},
   booktitle={International cross-domain conference for machine learning and knowledge extraction},
   editor={Andreas Holzinger and Peter Kieseberg and A Min Tjoa and Edgar Weippl},
   pages={250--271},
   year={2019},
   organization={Springer},
   publisher={Springer},
   abstract={One-digit multiplication problems is one of the major fields in learning mathematics at the level of primary school that has been studied over and over. However, the majority of related work is focusing on descriptive statistics on data from multiple surveys. The goal of our research is to gain insights into multiplication misconceptions by applying machine learning techniques. To reach this goal, we trained a probabilistic graphical model of the students’ misconceptions from data of an application for learning multiplication. The use of this model facilitates the exploration of insights into human learning competence and the personalization of tutoring according to individual learner’s knowledge states. The detection of all relevant causal factors of the erroneous students answers as well as their corresponding relative weight is a valuable insight for teachers. Furthermore, the similarity between different multiplication problems - according to the students behavior - is quantified and used for their grouping into clusters. Overall, the proposed model facilitates real-time learning insights that lead to more informed decisions.},
   doi={10.1007/978-3-030-29726-8_16}
 }
 
-@incollection{Saranti:2009,
+@incollection{Saranti:2009:QuantumHarmonicOscSonification,
   title={Quantum harmonic oscillator sonification},
   author={Saranti, Anna and Eckel, Gerhard and Pirr{\'o}, David},
   booktitle={Auditory Display},
   editor={Sølvi Ystad and Mitsuko Aramaki and Richard Kronland-Martinet and Kristoffer Jensen},
   pages={184--201},
   year={2009},
   publisher={Springer},
   abstract={This work deals with the sonification of a quantum mechanical system and the processes that occur as a result of its quantum mechanical nature and interactions with other systems. The quantum harmonic oscillator is not only regarded as a system with sonifiable characteristics but also as a storage medium for quantum information. By representing sound information quantum mechanically and storing it in the system, every process that unfolds on this level is inherited and reflected by the sound. The main profit of this approach is that the sonification can be used as a first insight for two models: a quantum mechanical system model and a quantum computation model.},
   doi={10.1007/978-3-642-12439-6_10}
 }
 
 
 @article{SchnakeMontavon:2020:XAIgraphs,
    year = {2020},
    author = {Schnake, Thomas and Eberle, Oliver and Lederer, Jonas and Nakajima, Shinichi and Schütt, Kristof T. and Müller, Klaus-Robert and Montavon, Grégoire},
    title = {XAI for Graphs: Explaining Graph Neural Network Predictions by Identifying Relevant Walks},
    journal = {arXiv:2006.03589},
    abstract = {Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph into the neural network structure, common explainable AI (XAI) approaches are not applicable. To a large extent, GNNs have remained black-boxes for the user so far. In this paper, we contribute by proposing a new XAI approach for GNNs. Our approach is derived from high-order Taylor expansions and is able to generate a decomposition of the GNN prediction as a collection of relevant walks on the input graph. We find that these high-order Taylor expansions can be equivalently (and more simply) computed using multiple backpropagation passes from the top layer of the GNN to the first layer. The explanation can then be further robustified and generalized by using layer-wise-relevance propagation (LRP) in place of the standard equations for gradient propagation. Our novel method which we denote as `GNN-LRP' is tested on scale-free graphs, sentence parsing trees, molecular graphs, and pixel lattices representing images. In each case, it performs stably and accurately, and delivers interesting and novel application insights.}
 }
 
 @incollection{Schneeberger:2020:legalAI,
    year = {2020},
    author = {Schneeberger, David and Stoeger, Karl and Holzinger, Andreas},
    title = {The European legal framework for medical AI},
    booktitle = {International Cross-Domain Conference for Machine Learning and Knowledge Extraction, Springer LNCS 12279},
    publisher = {Springer},
    pages = {209--226},
    doi = {10.1007/978-3-030-57321-8_12}
 }
 
 @article{Scurto:2021:DesigningDeepRLHumanParameterExploration,
   title={Designing deep reinforcement learning for human parameter exploration},
   author={Scurto, Hugo and Kerrebroeck, Bavo Van and Caramiaux, Baptiste and Bevilacqua, Fr{\'e}d{\'e}ric},
   journal={ACM Transactions on Computer-Human Interaction (TOCHI)},
   volume={28},
   number={1},
   pages={1--35},
   year={2021},
   publisher={ACM New York, NY, USA},
   abstract={Software tools for generating digital sound often present users with high-dimensional, parametric interfaces, that may not facilitate exploration of diverse sound designs. In this article, we propose to investigate artificial agents using deep reinforcement learning to explore parameter spaces in partnership with users for sound design. We describe a series of user-centred studies to probe the creative benefits of these agents and adapting their design to exploration. Preliminary studies observing users’ exploration strategies with parametric interfaces and testing different agent exploration behaviours led to the design of a fully-functioning prototype, called Co-Explorer, that we evaluated in a workshop with professional sound designers. We found that the Co-Explorer enables a novel creative workflow centred on human–machine partnership, which has been positively received by practitioners. We also highlight varied user exploration behaviours throughout partnering with our system. Finally, we frame design guidelines for enabling such co-exploration workflow in creative digital applications.},
   doi = {10.1145/3414472}
 }
 
 @book{ShneidermanEtAl:2016:GoldenRulesHCI,
   title={Designing the user interface: strategies for effective human-computer interaction},
   author={Shneiderman, Ben and Plaisant, Catherine and Cohen, Maxine S and Jacobs, Steven and Elmqvist, Niklas and Diakopoulos, Nicholas},
   year={2016},
   publisher={Pearson}
 }
 
 @article{ShteingartLoewenstein:2014:RLHumanBehavior,
   title = {Reinforcement learning and human behavior},
   journal = {Current Opinion in Neurobiology},
   volume = {25},
   pages = {93--98},
   year = {2014},
   note = {Theoretical and computational neuroscience},
   issn = {0959-4388},
   author = {Hanan Shteingart and Yonatan Loewenstein},
   abstract = {The dominant computational approach to model operant learning and its underlying neural activity is model-free reinforcement learning (RL). However, there is accumulating behavioral and neuronal-related evidence that human (and animal) operant learning is far more multifaceted. Theoretical advances in RL, such as hierarchical and model-based RL extend the explanatory power of RL to account for some of these findings. Nevertheless, some other aspects of human behavior remain inexplicable even in the simplest tasks. Here we review developments and remaining challenges in relating RL models to human operant learning. In particular, we emphasize that learning a model of the world is an essential step before or in parallel to learning the policy in RL and discuss alternative models that directly learn a policy without an explicit world model in terms of state-action pairs.},
   doi = {10.1016/j.conb.2013.12.004}
 }
 
 @article{ShuXiongSocher:2017:HierarchicalTaskExplanation,
   title={Hierarchical and interpretable skill acquisition in multi-task reinforcement learning},
   author={Shu, Tianmin and Xiong, Caiming and Socher, Richard},
   journal={arXiv preprint arXiv:1712.07294},
   year={2017},
   publisher = {arXiv},
   abstract={Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills. },
   doi = {10.48550/ARXIV.1712.07294}
 }
 
 @inproceedings{silva2020optimization,
   title={Optimization methods for interpretable differentiable decision trees applied to reinforcement learning},
   author={Silva, Andrew and Gombolay, Matthew and Killian, Taylor and Jimenez, Ivan and Son, Sung-Hyun},
   booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
   editor = {Chiappa, Silvia and Calandra, Roberto},
   pages={1855--1865},
   year={2020},
   organization={PMLR},
   abstract = { Decision trees are ubiquitous in machine learning for their ease of use and interpretability. Yet, these models are not typically employed in reinforcement learning as they cannot be updated online via stochastic gradient descent. We overcome this limitation by allowing for a gradient update over the entire tree that improves sample complexity affords interpretable policy extraction. First, we include theoretical motivation on the need for policy-gradient learning by examining the properties of gradient descent over differentiable decision trees. Second, we demonstrate that our approach equals or outperforms a neural network on all domains and can learn discrete decision trees online with average rewards up to 7x higher than a batch-trained decision tree. Third, we conduct a user study to quantify the interpretability of a decision tree, rule list, and a neural network with statistically significant results (p &lt; 0.001).}
 }
 
 @article{Silver:2018:AlphaZero,
   title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
   author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
   journal={Science},
   volume={362},
   number={6419},
   pages={1140--1144},
   year={2018},
   publisher={American Association for the Advancement of Science},
   abstract={The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
   doi = {10.1126/science.aar6404}
 }
 
 @article{SongEtAl:2019:ExplainableGraphBasedRecommendations,
   author    = {Weiping Song and
                Zhijian Duan and
                Ziqing Yang and
                Hao Zhu and
                Ming Zhang and
                Jian Tang},
   title     = {Explainable Knowledge Graph-based Recommendation via Deep Reinforcement
                Learning},
   journal   = {arXiv preprint arXiv:1906.09506},
   publisher = {arXiv},
   year      = {2019},
   abstract = {This paper studies recommender systems with knowledge graphs, which can effectively address the problems of data sparsity and cold start. Recently, a variety of methods have been developed for this problem, which generally try to learn effective representations of users and items and then match items to users according to their representations. Though these methods have been shown quite effective, they lack good explanations, which are critical to recommender systems. In this paper, we take a different route and propose generating recommendations by finding meaningful paths from users to items. Specifically, we formulate the problem as a sequential decision process, where the target user is defined as the initial state, and the edges on the graphs are defined as actions. We shape the rewards according to existing state-of-the-art methods and then train a policy function with policy gradient methods. Experimental results on three real-world datasets show that our proposed method not only provides effective recommendations but also offers good explanations},
   doi = {10.48550/ARXIV.1906.09506}
 }
 
 
 @article{SrinivasLaskinAbbeel:2020:ContrastiveUnsupervisedRL,
   title={Curl: Contrastive unsupervised representations for reinforcement learning},
   author={Srinivas, Aravind and Laskin, Michael and Abbeel, Pieter},
   journal={arXiv preprint arXiv:2004.04136},
   publisher = {arXiv},
   year={2020},
   abstract={We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features},
   doi = {10.48550/ARXIV.2004.04136}
 }
 
 @article{Stoeger:2021:MedicalAI,
   year = {2021},
   author = {Stoeger, Karl and Schneeberger, David and Holzinger, Andreas},
   title = {Medical Artificial Intelligence: The European Legal Perspective},
   journal = {Communications of the ACM},
   volume = {64},
   number = {11},
   pages = {34--36},
   abstract = {Although the European Commission proposed new legislation for the use of "high-risk artificial intelligence" earlier this year, the existing European fundamental rights framework already provides some clear guidance on the use of medical AI.},
   doi = {10.1145/3458652}
 }
 
 @inproceedings{SturmEtAl:2015:InteractiveHeatmap,
   year = {2015},
   author = {Sturm, Werner and Schaefer, Till and Schreck, Tobias and Holzinger, Andeas and Ullrich, Torsten},
   title = {Extending the Scaffold Hunter Visualization Toolkit with Interactive Heatmaps },
   booktitle = {EG UK Computer Graphics and Visual Computing CGVC 2015},
   editor = {Borgo, Rita and Turkay, Cagatay},
   publisher = {Euro Graphics (EG)},
   pages = {77--84},
   abstract = {In many application areas, large amounts of data arise, which are often hard to interpret or make use of by humans. Interactive visualization can help to overview and explore large amounts of data. An example is in the life sciences, where databases of chemical compounds need to be analyzed in terms of similarities of molecular properties. Scientists then need to explore this data in an efficient way. The Scaffold Hunter framework is an Open Source software system for interactive visualization of high dimensional data. In this paper, we present an extension of Scaffold Hunter with an interactive heatmap, which ties in tightly with a dendrogram visualization. We added specific interaction modalities and views tailored to the analysis of chemical compounds. Zooming capabilities allow to start from an overview of the data (showing all data elements at once) down to a detail-on-demand view which includes chemical structural views of molecules. We show how the interactive heatmap with clustered rows and columns can bring new insights into the data regarding various properties. The implementation is made available for researchers and practitioners to use. },
   keywords = {interactive visualization, interactive heatmaps, visusal analytics, chemical compounds, chemical molecules},
   doi = {10.2312/cgvc.20151247}
 }
 
 @inproceedings{suay:11,
   title={Effect of human guidance and state space size on interactive reinforcement learning},
   author={Suay, Halit Bener and Chernova, Sonia},
   booktitle={2011 Ro-Man},
   pages={1--6},
   year={2011},
   organization={IEEE},
   abstract ={The Interactive Reinforcement Learning algorithm enables a human user to train a robot by providing rewards in response to past actions and anticipatory guidance to guide the selection of future actions. Past work with soft- ware agents has shown that incorporating user guidance into the policy learning process through Interactive Reinforcement Learning significantly improves the policy learning time by reducing the number of states the agent explores. We present the first study of Interactive Reinforcement Learning in real- world robotic systems. We report on four experiments that study the effects that teacher guidance and state space size have on policy learning performance. We discuss modifications made to apply Interactive Reinforcement Learning to a real- world system and show that guidance significantly reduces the learning rate, and that its positive effects increase with state space size.},
   doi={10.1109/ROMAN.2011.6005223}
 }
 
 % TODO: add doi if possible
 @inproceedings{sun2019program,
   title={Program guided agent},
   author={Sun, Shao-Hua and Wu, Te-Lin and Lim, Joseph J},
   booktitle={International Conference on Learning Representations},
   year={2019}
 }
 
 @article{Sun:2021:TopologyPerturbationGNNs,
   title={Preserve, Promote, or Attack? GNN Explanation via Topology Perturbation},
   author={Sun, Yi and Valente, Abel and Liu, Sijia and Wang, Dakuo},
   journal={arXiv preprint arXiv:2103.13944},
   year={2021},
   abstract={Prior works on formalizing explanations of a graph neural network (GNN) focus on a single use case - to preserve the prediction results through identifying important edges and nodes. In this paper, we develop a multi-purpose interpretation framework by acquiring a mask that indicates topology perturbations of the input graphs. We pack the framework into an interactive visualization system (GNNViz) which can fulfill multiple purposes: Preserve,Promote, or Attack GNN's predictions. We illustrate our approach's novelty and effectiveness with three case studies: First, GNNViz can assist non expert users to easily explore the relationship between graph topology and GNN's decision (Preserve), or to manipulate the prediction (Promote or Attack) for an image classification task on MS-COCO; Second, on the Pokec social network dataset, our framework can uncover unfairness and demographic biases; Lastly, it compares with state-of-the-art GNN explainer baseline on a synthetic dataset.},
   doi = {10.48550/ARXIV.2103.13944}
 }
 
 @book{SuttonBarto:2018:RLIntroduction,
   title={Reinforcement learning: An introduction},
   author={Sutton, Richard S and Barto, Andrew G},
   year={2018},
   publisher={MIT press}
 }
 
 %%% TTT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @inproceedings{TabrezHayes:2019:xRLTextualExplanations,
   title={Improving human-robot interaction through explainable reinforcement learning},
   author={Tabrez, Aaquib and Hayes, Bradley},
   booktitle={2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
   pages={751--753},
   year={2019},
   organization={IEEE},
   doi={10.1109/HRI.2019.8673198},
   abstract={Gathering the most informative data from humans without overloading them remains an active research area in AI, and is closely coupled with the problems of determining how and when information should be communicated to others [12]. Current decision support systems (DSS) are still overly simple and static, and cannot adapt to changing environments we expect to deploy in modern systems [3], [4], [9], [11]. They are intrinsically limited in their ability to explain rationale versus merely listing their future behaviors, limiting a human's understanding of the system [2], [7]. Most probabilistic assessments of a task are conveyed after the task/skill is attempted rather than before [10], [14], [16]. This limits failure recovery and danger avoidance mechanisms. Existing work on predicting failures relies on sensors to accurately detect explicitly annotated and learned failure modes [13]. As such, important non-obvious pieces of information for assessing appropriate trust and/or course-of-action (COA) evaluation in collaborative scenarios can go overlooked, while irrelevant information may instead be provided that increases clutter and mental workload. Understanding how AI models arrive at specific decisions is a key principle of trust [8]. Therefore, it is critically important to develop new strategies for anticipating, communicating, and explaining justifications and rationale for AI driven behaviors via contextually appropriate semantics.}
 }
 
 @article{taylor2009transfer,
   title={Transfer learning for reinforcement learning domains: A survey.},
   author={Taylor, Matthew E and Stone, Peter},
   journal={Journal of Machine Learning Research},
   volume={10},
   number={7},
   pages = {1633--1685},
   year={2009}
 }
 
 @inproceedings{taylor2011integrating,
   title={Integrating reinforcement learning with human demonstrations of varying ability},
   author={Taylor, Matthew E and Suay, Halit Bener and Chernova, Sonia},
   booktitle={The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2},
   publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
   address = {Richland, SC},
   series = {AAMAS '11},
   pages={617--624},
   year={2011},
   organization={Citeseer},
   abstract = {This work introduces Human-Agent Transfer (HAT), an algorithm that combines transfer learning, learning from demonstration and reinforcement learning to achieve rapid learning and high performance in complex domains. Using experiments in a simulated robot soccer domain, we show that human demonstrations transferred into a baseline policy for an agent and refined using reinforcement learning significantly improve both learning time and policy performance. Our evaluation compares three algorithmic approaches to incorporating demonstration rule summaries into transfer learning, and studies the impact of demonstration quality and quantity, as well as the effect of combining demonstrations from multiple teachers. Our results show that all three transfer methods lead to statistically significant improvement in performance over learning without demonstration. The best performance was achieved by combining the best demonstrations from two teachers.}
 }
 
 @article{TaylorEtAl:2021:InteractiveRLHIPPOGym,
   title={Improving Reinforcement Learning with Human Assistance: An Argument for Human Subject Studies with HIPPO Gym},
   author={Taylor, Matthew E and Nissen, Nicholas and Wang, Yuan and Navidi, Neda},
   journal={arXiv preprint arXiv:2102.02639},
   publisher = {arXiv},
   year={2021},
   abstract={Reinforcement learning (RL) is a popular machine learning paradigm for game playing, robotics control, and other sequential decision tasks. However, RL agents often have long learning times with high data requirements because they begin by acting randomly. In order to better learn in complex tasks, this article argues that an external teacher can often significantly help the RL agent learn. OpenAI Gym is a common framework for RL research, including a large number of standard environments and agents, making RL research significantly more accessible. This article introduces our new open-source RL framework, the Human Input Parsing Platform for Openai Gym (HIPPO Gym), and the design decisions that went into its creation. The goal of this platform is to facilitate human-RL research, again lowering the bar so that more researchers can quickly investigate different ways that human teachers could assist RL agents, including learning from demonstrations, learning from feedback, or curriculum learning.},
   doi = {10.48550/ARXIV.2102.02639}
 }
 
 % TODO: authors: others?, please add doi
 @inproceedings{Thomaz:2006:RLWithHumanTeachers,
   title={Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance},
   author={Thomaz, Andrea Lockerd and Breazeal, Cynthia and others},
   booktitle={Aaai},
   volume={6},
   pages={1000--1005},
   year={2006},
   organization={Boston, MA},
   abstract={While reinforcement learning (RL) is not traditionally designed for interactive supervisory input from a human teacher, several works in both robot and software agents have adapted it for human input by letting a human trainer control the reward signal. In this work, we experimentally examine the assumption underlying these works, namely that the human-given reward is compatible with the traditional RL reward signal. We describe an experimental platform with a simulated RL robot and present an analysis of real-time human teaching behavior found in a study in which untrained subjects taught the robot to perform a new task. We report three main observations on how people administer feedback when teaching a robot a task through reinforcement learning: (a) they use the reward channel not only for feedback, but also for future-directed guidance; (b) they have a positive bias to their feedback -possibly using the signal as a motivational channel; and (c) they change their behavior as they develop a mental model of the robotic learner. In conclusion, we discuss future extensions to RL to accommodate these lessons.}
 }
 
-@article{TickleEtAl:1998:HistoryNN,
-author={Tickle, A.B. and Andrews, R. and Golea, M. and Diederich, J.},  journal={IEEE Transactions on Neural Networks},   title={The truth will come to light: directions and challenges in extracting the knowledge embedded within trained artificial neural networks},   year={1998},  volume={9},  number={6},  pages={1057-1068},  abstract={To date, the preponderance of techniques for eliciting the knowledge embedded in trained artificial neural networks (ANN's) has focused primarily on extracting rule-based explanations from feedforward ANN's. The ADT taxonomy for categorizing such techniques was proposed in 1995 to provide a basis for the systematic comparison of the different approaches. This paper shows that not only is this taxonomy applicable to a cross section of current techniques for extracting rules from trained feedforward ANN's but also how the taxonomy can be adapted and extended to embrace a broader range of ANN types (e,g., recurrent neural networks) and explanation structures. In addition we identify some of the key research questions in extracting the knowledge embedded within ANN's including the need for the formulation of a consistent theoretical basis for what has been, until recently, a disparate collection of empirical results.},  keywords={},  doi={10.1109/72.728352},  ISSN={1941-0093},  month={Nov},}
-
 @article{TomarEtAl:2021:LearnPixelControlRepresentations,
   title={Learning Representations for Pixel-based Control: What Matters and Why?},
   author={Tomar, Manan and Mishra, Utkarsh A and Zhang, Amy and Taylor, Matthew E},
   journal={arXiv preprint arXiv:2111.07775},
   publisher = {arXiv},
   year={2021},
   abstract={Learning representations for pixel-based control has garnered significant attention recently in reinforcement learning. A wide range of methods have been proposed to enable efficient learning, leading to sample complexities similar to those in the full state setting. However, moving beyond carefully curated pixel data sets (centered crop, appropriate lighting, clear background, etc.) remains challenging. In this paper, we adopt a more difficult setting, incorporating background distractors, as a first step towards addressing this challenge. We present a simple baseline approach that can learn meaningful representations with no metric-based learning, no data augmentations, no world-model learning, and no contrastive learning. We then analyze when and why previously proposed methods are likely to fail or reduce to the same performance as the baseline in this harder setting and why we should think carefully about extending such methods beyond the well curated environments. Our results show that finer categorization of benchmarks on the basis of characteristics like density of reward, planning horizon of the problem, presence of task-irrelevant components, etc., is crucial in evaluating algorithms. Based on these observations, we propose different metrics to consider when evaluating an algorithm on benchmark tasks. We hope such a data-centric view can motivate researchers to rethink representation learning when investigating how to best apply RL to real-world tasks. },
   doi = {10.48550/ARXIV.2111.07775}
 }
 
 @inproceedings{topin2021iterative,
   title={Iterative Bounding MDPs: Learning Interpretable Policies via Non-Interpretable Methods},
   author={Topin, Nicholay and Milani, Stephanie and Fang, Fei and Veloso, Manuela},
   booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
   volume={35},
   pages={9923--9931},
   year={2021}
 }
 
 @inproceedings{toro2019learning,
   title={Learning reward machines for partially observable reinforcement learning},
   author={Icarte, Rodrigo Toro and Waldie, Ethan and Klassen, Toryn and Valenzano, Rick and Castro, Margarita and McIlraith, Sheila},
   booktitle = {Advances in Neural Information Processing Systems},
   editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
   volume={32},
   year={2019}
 }
 
 @inproceedings{icarte2018using,
   title={Using reward machines for high-level task specification and decomposition in reinforcement learning},
   author={Icarte, Rodrigo Toro and Klassen, Toryn and Valenzano, Richard and McIlraith, Sheila},
   booktitle = {Proceedings of the 35th International Conference on Machine Learning},
   editor = {Dy, Jennifer and Krause, Andreas},
   volume = {80},
   series = {Proceedings of Machine Learning Research},
   pages={2107--2116},
   year={2018},
   organization={PMLR},
   abstract = {In this paper we propose Reward Machines {—} a type of finite state machine that supports the specification of reward functions while exposing reward function structure to the learner and supporting decomposition. We then present Q-Learning for Reward Machines (QRM), an algorithm which appropriately decomposes the reward machine and uses off-policy q-learning to simultaneously learn subpolicies for the different components. QRM is guaranteed to converge to an optimal policy in the tabular case, in contrast to Hierarchical Reinforcement Learning methods which might converge to suboptimal policies. We demonstrate this behavior experimentally in two discrete domains. We also show how function approximation methods like neural networks can be incorporated into QRM, and that doing so can find better policies more quickly than hierarchical methods in a domain with a continuous state space.}
 }
 
 @inproceedings{torrey2013teaching,
   title={Teaching on a budget: Agents advising agents in reinforcement learning},
   author={Torrey, Lisa and Taylor, Matthew},
   booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
   publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
   address = {Richland, SC},
   pages={1053--1060},
   year={2013},
   abstract = {This paper introduces a teacher-student framework for reinforcement learning. In this framework, a teacher agent instructs a student agent by suggesting actions the student should take as it learns. However, the teacher may only give such advice a limited number of times. We present several novel algorithms that teachers can use to budget their advice effectively, and we evaluate them in two experimental domains: Mountain Car and Pac-Man. Our results show that the same amount of advice, given at different moments, can have different effects on student learning, and that teachers can significantly affect student learning even when students use different learning methods and state representations.},
   doi={10.5555/2484920.2485086}
 }
 
 %%% VVV %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @article{vecerik2017leveraging,
   title={Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards},
   author={Vecerik, Mel and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
   journal={arXiv preprint arXiv:1707.08817},
   publisher = {arXiv},
   year={2017},
   abstract={Leveraging human demonstrations along with an off-policy RL algorithm for continuous actions (DDPG) for robotics task like peg insertion, clip insertion, cable insertion etc. A separate replay buffer for demonstrations is maintained, prioritized replay is used to sample both agent and human demonstrations along with some other techniques},
   doi = {10.48550/ARXIV.1707.08817}
 }
 
 @InProceedings{VermaEtAl:2018:ProgrammaticallyInterpretableRL,
   title = 	 {Programmatically Interpretable Reinforcement Learning},
   author =       {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
   booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
   pages = 	 {5045--5054},
   year = 	 {2018},
   editor = 	 {Dy, Jennifer and Krause, Andreas},
   volume = 	 {80},
   series = 	 {Proceedings of Machine Learning Research},
   month = 	 {10--15 Jul},
   publisher =    {PMLR},
   url = 	 {https://proceedings.mlr.press/v80/verma18a.html},
   abstract = 	 {We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural “oracle”. We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.}
 }
 
 @inproceedings{verma2019imitation,
   title={Imitation-projected programmatic reinforcement learning},
   author={Verma, Abhinav and Le, Hoang and Yue, Yisong and Chaudhuri, Swarat},
   booktitle = {Advances in Neural Information Processing Systems},
   editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
   volume={32},
   year={2019},
   publisher = {Curran Associates, Inc.}
 }
 
 @article{Vu:2020:PGMExplainer,
   title={Pgm-explainer: Probabilistic graphical model explanations for graph neural networks},
   author={Vu, Minh N and Thai, My T},
   journal={arXiv preprint arXiv:2010.05788},
   publisher = {arXiv},
   year={2020},
   abstract={In Graph Neural Networks (GNNs), the graph structure is incorporated into the learning of node representations. This complex structure makes explaining GNNs' predictions become much more challenging. In this paper, we propose PGM-Explainer, a Probabilistic Graphical Model (PGM) model-agnostic explainer for GNNs. Given a prediction to be explained, PGM-Explainer identifies crucial graph components and generates an explanation in form of a PGM approximating that prediction. Different from existing explainers for GNNs where the explanations are drawn from a set of linear functions of explained features, PGM-Explainer is able to demonstrate the dependencies of explained features in form of conditional probabilities. Our theoretical analysis shows that the PGM generated by PGM-Explainer includes the Markov-blanket of the target prediction, i.e. including all its statistical information. We also show that the explanation returned by PGM-Explainer contains the same set of independence statements in the perfect map. Our experiments on both synthetic and real-world datasets show that PGM-Explainer achieves better performance than existing explainers in many benchmark tasks.},
   doi = {10.48550/ARXIV.2010.05788}
 }
 
 %%% WWW %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @inproceedings{Wang:2022:SkillPreferences,
   title={Skill preferences: Learning to extract and execute robotic skills from human feedback},
   author={Wang, Xiaofei and Lee, Kimin and Hakhamaneshi, Kourosh and Abbeel, Pieter and Laskin, Michael},
   booktitle={Conference on Robot Learning},
   pages={1259--1268},
   year={2022},
   organization={PMLR},
   abstract={A promising approach to solving challenging long-horizon tasks has been to extract behavior priors (skills) by fitting generative models to large offline datasets of demonstrations. However, such generative models inherit the biases of the underlying data and result in poor and unusable skills when trained on imperfect demonstration data. To better align skill extraction with human intent we present Skill Preferences (SkiP), an algorithm that learns a model over human preferences and uses it to extract human-aligned skills from offline data. After extracting human-preferred skills, SkiP also utilizes human feedback to solve downstream tasks with RL. We show that SkiP enables a simulated kitchen robot to solve complex multi-step manipulation tasks and substantially outperforms prior leading RL algorithms with human preferences as well as leading skill extraction algorithms without human preferences.}
 }
 
 @article{Warnell:2018:DeepTAMER, 
   title={Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces}, 
   volume={32}, 
   url={https://ojs.aaai.org/index.php/AAAI/article/view/11485}, 
   abstractNote={While recent advances in deep reinforcement learning have allowed autonomous learning agents to succeed at a variety of complex tasks, existing algorithms generally require a lot of training data. One way to increase the speed at which agents are able to learn to perform tasks is by leveraging the input of human trainers. Although such input can take many forms, real-time, scalar-valued feedback is especially useful in situations where it proves difficult or impossible for humans to provide expert demonstrations. Previous approaches have shown the usefulness of human input provided in this fashion (e.g., the TAMER framework), but they have thus far not considered high-dimensional state spaces or employed the use of deep learning. In this paper, we do both: we propose DeepTAMER, an extension of the TAMER framework that leverages the representational power of deep neural networks in order to learn complex tasks in just a short amount of time with a human trainer. We demonstrate Deep TAMER’s success by using it and just 15 minutes of human-provided feedback to train an agent that performs better than humans on the Atari game of Bowling - a task that has proven difficult for even state-of-the-art reinforcement learning methods.}, 
   number={1}, 
   journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
   author={Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter}, 
   year={2018}, 
   month={Apr.} 
 }
 
 @article{WellsBednarz:2021:xAIRLSurvey,
   title={Explainable ai and reinforcement learning—a systematic review of current approaches and trends},
   author={Wells, Lindsay and Bednarz, Tomasz},
   journal={Frontiers in artificial intelligence},
   volume={4},
   pages={48},
   year={2021},
   publisher={Frontiers},
   abstract={Research into Explainable Artificial Intelligence (XAI) has been increasing in recent years as a response to the need for increased transparency and trust in AI. This is particularly important as AI is used in sensitive domains with societal, ethical, and safety implications. Work in XAI has primarily focused on Machine Learning (ML) for classification, decision, or action, with detailed systematic reviews already undertaken. This review looks to explore current approaches and limitations for XAI in the area of Reinforcement Learning (RL). From 520 search results, 25 studies (including 5 snowball sampled) are reviewed, highlighting visualization, query-based explanations, policy summarization, human-in-the-loop collaboration, and verification as trends in this area. Limitations in the studies are presented, particularly a lack of user studies, and the prevalence of toy-examples and difficulties providing understandable explanations. Areas for future study are identified, including immersive visualization, and symbolic representation.},
   doi={10.3389/frai.2021.550030}
 }
 
 @article{WuEtAl:2021:HITLMLSurvey,
   title={A Survey of Human-in-the-loop for Machine Learning},
   author={Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang},
   journal={arXiv preprint arXiv:2108.00941},
   publisher = {arXiv},
   year={2021},
   abstract={Human-in-the-loop aims to train an accurate prediction model with minimum cost by integrating human knowledge and experience. Humans can provide training data for machine learning applications and directly accomplish tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent human-in-the-loop. Using the above categorization, we summarize major approaches in the field; along with their technical strengths/ weaknesses, we have simple classification and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for human-in-the-loop and motivates interested readers to consider approaches for designing effective human-in-the-loop solutions.},
   doi = {10.48550/ARXIV.2108.00941}
 }
 
 @article{WuEtAl:2021:HITLDRLAutonomousDriving,
   title={Human-in-the-Loop Deep Reinforcement Learning with Application to Autonomous Driving},
   author={Wu, Jingda and Huang, Zhiyu and Huang, Chao and Hu, Zhongxu and Hang, Peng and Xing, Yang and Lv, Chen},
   journal={arXiv preprint arXiv:2104.07246},
   publisher = {arXiv},
   year={2021},
   abstract={Due to the limited smartness and abilities of machine intelligence, currently autonomous vehicles are still unable to handle all kinds of situations and completely replace drivers. Because humans exhibit strong robustness and adaptability in complex driving scenarios, it is of great importance to introduce humans into the training loop of artificial intelligence, leveraging human intelligence to further advance machine learning algorithms. In this study, a real-time human-guidance-based deep reinforcement learning (Hug-DRL) method is developed for policy training of autonomous driving. Leveraging a newly designed control transfer mechanism between human and automation, human is able to intervene and correct the agent's unreasonable actions in real time when necessary during the model training process. Based on this human-in-the-loop guidance mechanism, an improved actor-critic architecture with modified policy and value networks is developed. The fast convergence of the proposed Hug-DRL allows real-time human guidance actions to be fused into the agent's training loop, further improving the efficiency and performance of deep reinforcement learning. The developed method is validated by human-in-the-loop experiments with 40 subjects and compared with other state-of-the-art learning approaches. The results suggest that the proposed method can effectively enhance the training efficiency and performance of the deep reinforcement learning algorithm under human guidance, without imposing specific requirements on participant expertise and experience.},
   doi = {10.48550/ARXIV.2104.07246}
 }
 
 %%% XXX %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @inproceedings{XinEtAl:2018:HITLMLFeedbackLoop,
   title={Accelerating human-in-the-loop machine learning: Challenges and opportunities},
   author={Xin, Doris and Ma, Litian and Liu, Jialin and Macke, Stephen and Song, Shuchen and Parameswaran, Aditya},
   booktitle={Proceedings of the second workshop on data management for end-to-end machine learning},
   publisher = {Association for Computing Machinery},
   address = {New York, NY, USA},
   pages={1--4},
   year={2018},
   abstract={Development of machine learning (ML) workflows is a tedious process of iterative experimentation: developers repeatedly make changes to workflows until the desired accuracy is attained. We describe our vision for a "human-in-the-loop" ML system that accelerates this process: by intelligently tracking changes and intermediate results over time, such a system can enable rapid iteration, quick responsive feedback, introspection and debugging, and background execution and automation. We finally describe Helix, our preliminary attempt at such a system that has already led to speedups of upto 10x on typical iterative workflows against competing systems.},
   doi={doi.org/10.1145/3209889.3209897}
 }
 
 @article{XiongEtAl:2020:Robustness,
   title={Robustness to adversarial attacks in learning-enabled controllers},
   author={Xiong, Zikang and Eappen, Joe and Zhu, He and Jagannathan, Suresh},
   journal={arXiv preprint arXiv:2006.06861},
   publisher = {arXiv},
   year={2020},
   abstract={Learning-enabled controllers used in cyber-physical systems (CPS) are known to be susceptible to adversarial attacks. Such attacks manifest as perturbations to the states generated by the controller's environment in response to its actions. We consider state perturbations that encompass a wide variety of adversarial attacks and describe an attack scheme for discovering adversarial states. To be useful, these attacks need to be natural, yielding states in which the controller can be reasonably expected to generate a meaningful response. We consider shield-based defenses as a means to improve controller robustness in the face of such perturbations. Our defense strategy allows us to treat the controller and environment as black-boxes with unknown dynamics. We provide a two-stage approach to construct this defense and show its effectiveness through a range of experiments on realistic continuous control domains such as the navigation control-loop of an F16 aircraft and the motion control system of humanoid robots. },
   doi = {10.48550/ARXIV.2006.06861}
 }
 
 
 %%% YYY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 @article{Yang:2021:TrailNearOptimalSuboptimal,
   title={TRAIL: Near-Optimal Imitation Learning with Suboptimal Data},
   author={Yang, Mengjiao and Levine, Sergey and Nachum, Ofir},
   journal={arXiv preprint arXiv:2110.14770},
   publisher = {arXiv},
   year={2021},
   abstract={The aim in imitation learning is to learn effective policies by utilizing near-optimal expert demonstrations. However, high-quality demonstrations from human experts can be expensive to obtain in large numbers. On the other hand, it is often much easier to obtain large quantities of suboptimal or task-agnostic trajectories, which are not useful for direct imitation, but can nevertheless provide insight into the dynamical structure of the environment, showing what could be done in the environment even if not what should be done. We ask the question, is it possible to utilize such suboptimal offline datasets to facilitate provably improved downstream imitation learning? In this work, we answer this question affirmatively and present training objectives that use offline datasets to learn a factored transition model whose structure enables the extraction of a latent action space. Our theoretical analysis shows that the learned latent action space can boost the sample-efficiency of downstream imitation learning, effectively reducing the need for large near-optimal expert datasets through the use of auxiliary non-expert data. To learn the latent action space in practice, we propose TRAIL (Transition-Reparametrized Actions for Imitation Learning), an algorithm that learns an energy-based transition model contrastively, and uses the transition model to reparametrize the action space for sample-efficient imitation learning. We evaluate the practicality of our objective through experiments on a set of navigation and locomotion tasks. Our results verify the benefits suggested by our theory and show that TRAIL is able to improve baseline imitation learning by up to 4x in performance.},
   doi = {10.48550/ARXIV.2110.14770}
 }
 
 @article{YangEtAl:2021:AutoCurriculumMARS,
   title={Diverse Auto-Curriculum is Critical for Successful Real-World Multiagent Learning Systems},
   author={Yang, Yaodong and Luo, Jun and Wen, Ying and Slumbers, Oliver and Graves, Daniel and Ammar, Haitham Bou and Wang, Jun and Taylor, Matthew E},
   journal={arXiv preprint arXiv:2102.07659},
   publisher = {arXiv},
   year={2021},
   abstract={Multiagent reinforcement learning (MARL) has achieved a remarkable amount of success in solving various types of video games. A cornerstone of this success is the auto-curriculum framework, which shapes the learning process by continually creating new challenging tasks for agents to adapt to, thereby facilitating the acquisition of new skills. In order to extend MARL methods to real-world domains outside of video games, we envision in this blue sky paper that maintaining a diversity-aware auto-curriculum is critical for successful MARL applications. Specifically, we argue that behavioural diversity is a pivotal, yet under-explored, component for real-world multiagent learning systems, and that significant work remains in understanding how to design a diversity-aware auto-curriculum. We list four open challenges for auto-curriculum techniques, which we believe deserve more attention from this community. Towards validating our vision, we recommend modelling realistic interactive behaviours in autonomous driving as an important test bed, and recommend the SMARTS/ULTRA benchmark. },
   doi = {10.48550/ARXIV.2102.07659}
 }
 
 % TODO: authors: others?
 @inproceedings{yang2021efficient,
   title={Efficient deep reinforcement learning via adaptive policy transfer},
   author={Yang, Tianpei and Hao, Jianye and Meng, Zhaopeng and Zhang, Zongzhang and Hu, Yujing and Chen, Yingfeng and Fan, Changjie and Wang, Weixun and Liu, Wulong and Wang, Zhaodong and others},
   booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
   pages={3094--3100},
   year={2021},
   doi = {10.24963/ijcai.2020/428}
 }
 
 %%% ZZZ %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 @article{ZagalJavierVallejos:2004:RealityGap,
   title = {Back to reality: Crossing the reality gap in evolutionary robotics},
   journal = {IFAC Proceedings Volumes},
   volume = {37},
   number = {8},
   pages = {834--839},
   year = {2004},
   note = {IFAC/EURON Symposium on Intelligent Autonomous Vehicles, Lisbon, Portugal, 5-7 July 2004},
   issn = {1474-6670},
   url = {https://www.sciencedirect.com/science/article/pii/S1474667017320840},
   author = {Juan Cristóbal Zagal and Javier Ruiz-del-Solar and Paul Vallejos},
   keywords = {Evolutionary Robotics, Autonomous Systems, On-line Learning},
   abstract = {In this work a new method to evolutionary robotics is proposed, it combines into asingle framework, learning from reality and simulations. An illusory sub-system is incorporated as an integral part of an autonomous system. The adaptation of the illusory system results from minimizing differences of robot behavior evaluations in reality and in simulations. Behavior guides the illusory adaptation by sampling task-relevant instances of the world. Thus explicit calibration is not required. We remark two attributes of the presented methodology: (i) it is a promising approach for crossing the reality-gap among simulation and reality in evolutionary robotics, and (ii) it allows to generate automatically models and theories of the real robot environment expressed as simulations. We present validation experiments on locomotive behavior acquisition for legged robots.},
   doi = {10.1016/S1474-6670(17)32084-0}
 }
 
 @article{zanzotto2019human,
   title={Viewpoint: Human-in-the-loop Artificial Intelligence},
   author={Zanzotto, Fabio Massimo},
   journal={Journal of Artificial Intelligence Research},
   volume={64},
   pages={243--252},
   year={2019},
   abstract={Little by little, newspapers are revealing the bright future that Artificial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future may have a possible dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers may need financial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Many learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, many of these workers are shooting themselves in the feet. In this paper, we propose Human-in-the-loop Artificial Intelligence (HitAI) as a fairer paradigm for AI systems. Recognizing that any AI system has humans in the loop, HitAI will reward these aware and unaware knowledge producers with a different scheme: decisions of AI systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. As modern Merry Men, HitAI researchers should fight for a fairer Robin Hood Artificial Intelligence that gives back what it steals.},
   doi={10.1613/jair.1.11345 }
 }
 
 % TODO: authors: others? who?
 @article{zhang2020explainable,
   title={Explainable recommendation: A survey and new perspectives},
   author={Zhang, Yongfeng and Chen, Xu and others},
   journal={Foundations and Trends{\textregistered} in Information Retrieval},
   volume={14},
   number={1},
   pages={1--101},
   year={2020},
   publisher={Now Publishers, Inc.},
   doi = {10.1561/1500000066}
 }
 
 @incollection{Zhang:2020:AlphaZero,
   title={AlphaZero},
   author={Zhang, Hongming and Yu, Tianyang},
   booktitle={Deep Reinforcement Learning},
   pages={391--415},
   year={2020},
   publisher={Springer},
   abstract={In this chapter, we introduce combinatorial games such as chess and Go and take Gomoku as an example to introduce the AlphaZero algorithm, a general algorithm that has achieved superhuman performance in many challenging games. This chapter is divided into three parts: the first part introduces the concept of combinatorial games, the second part introduces the family of algorithms known as Monte Carlo Tree Search, and the third part takes Gomoku as the game environment to demonstrate the details of the AlphaZero algorithm, which combines Monte Carlo Tree Search and deep reinforcement learning from self-play.},
   doi={10.1007/978-981-15-4095-0_15}
 }
 
 @inproceedings{Zhang:2020:human_out_loop,
   title={Can Humans Be out of the Loop?},
   author={Junzhe Zhang and Elias Bareinboim},
   booktitle={First Conference on Causal Learning and Reasoning (CLeaR 2022},
   year={2020},
   url={https://openreview.net/forum?id=P0f91v5fTK}
 }
 
 @inproceedings{zhang2019leveraging,
   title={Leveraging human guidance for deep reinforcement learning tasks},
   author={Zhang, Ruohan and Torabi, Faraz and Guan, Lin and Ballard, Dana H and Stone, Peter},
   booktitle={Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)},
   publisher = {International Joint Conferences on Artificial Intelligence Organization},             
   pages = {6339--6346},
   year={2019},
   doi={10.24963/ijcai.2019/884}
 }
 
 @inproceedings{Zhang:2020:CausalImitationLearning,
   title={Causal imitation learning with unobserved confounders},
   author={Zhang, Junzhe and Kumor, Daniel and Bareinboim, Elias},
   booktitle = {Advances in Neural Information Processing Systems},
   volume={33},
   editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
   pages={12263--12274},
   year={2020},
   abstract={One of the common ways children learn is by mimicking adults. Imitation learning focuses on learning policies with suitable performance from demonstrations generated by an expert, with an unspecified performance measure, and unobserved reward signal. Popular methods for imitation learning start by either directly mimicking the behavior policy of an expert (behavior cloning) or by learning a reward function that prioritizes observed expert trajectories (inverse reinforcement learning). However, these methods rely on the assumption that covariates used by the expert to determine her/his actions are fully observed. In this paper, we relax this assumption and study imitation learning when sensory inputs of the learner and the expert differ. First, we provide a non-parametric, graphical criterion that is complete (both necessary and sufficient) for determining the feasibility of imitation from the combinations of demonstration data and qualitative assumptions about the underlying environment, represented in the form of a causal model. We then show that when such a criterion does not hold, imitation could still be feasible by exploiting quantitative knowledge of the expert trajectories. Finally, we develop an efficient procedure for learning the imitating policy from experts' trajectories.}
 }
 
 @inproceedings{zhang2021kogun,
   title={KoGuN: accelerating deep reinforcement learning via integrating human suboptimal knowledge},
   author={Zhang, Peng and Hao, Jianye and Wang, Weixun and Tang, Hongyao and Ma, Yi and Duan, Yihai and Zheng, Yan},
   booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
   pages={2291--2297},
   year={2021}
 }
 
 @inproceedings{Zhou:2019:CgcNet,
   title={Cgc-net: Cell graph convolutional network for grading of colorectal cancer histology images},
   author={Zhou, Yanning and Graham, Simon and Alemi Koohbanani, Navid and Shaban, Muhammad and Heng, Pheng-Ann and Rajpoot, Nasir},
   booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},
   publisher={IEEE},
   pages={388--398},
   year={2019},
   abstract={Colorectal cancer (CRC) grading is typically carried out by assessing the degree of gland formation within histology images. To do this, it is important to consider the overall tissue micro-environment by assessing the cell-level information along with the morphology of the gland. However, current automated methods for CRC grading typically utilise small image patches and therefore fail to incorporate the entire tissue micro-architecture for grading purposes. To overcome the challenges of CRC grading, we present a novel cell-graph convolutional neural network (CGC-Net) that converts each large histology image into a graph, where each node is represented by a nucleus within the original image and cellular interactions are denoted as edges between these nodes according to node similarity. The CGC-Net utilises nuclear appearance features in addition to the spatial location of nodes to further boost the performance of the algorithm. To enable nodes to fuse multi-scale information, we introduce Adaptive GraphSage, which is a graph convolution technique that combines multi-level features in a data-driven way. Furthermore, to deal with redundancy in the graph, we propose a sampling technique that removes nodes in areas of dense nuclear activity. We show that modeling the image as a graph enables us to effectively consider a much larger image (around 16x larger) than traditional patch-based approaches and model the complex structure of the tissue micro-environment. We construct cell graphs with an average of over 3,000 nodes on a large CRC histology image dataset and report state-of-the-art results as compared to recent patch-based as well as contextual patch-based techniques, demonstrating the effectiveness of our method.},
   doi={10.1109/ICCVW.2019.00050}
 }
 
 @article{ZhouEtAl:2021:QualitySurvey,
    year = {2021},
    author = {Zhou, Jianlong and Gandomi, Amir H. and Chen, Fang and Holzinger, Andreas},
    title = {Evaluating the Quality of Machine Learning Explanations: A Survey on Methods and Metrics},
    journal = {Electronics},
    volume = {10},
    number = {5},
    pages = {593},
    abstract = {The most successful Machine Learning (ML) systems remain complex black boxes to end-users, and even experts are often unable to understand the rationale behind their decisions. The lack of transparency of such systems can have severe consequences or poor uses of limited valuable resources in medical diagnosis, financial decision-making, and in other high-stake domains. Therefore, the issue of ML explanation has experienced a surge in interest from the research community to application domains. While numerous explanation methods have been explored, there is a need for evaluations to quantify the quality of explanation methods to determine whether and to what extent the offered explainability achieves the defined objective, and compare available explanation methods and suggest the best explanation from the comparison for a specific task. This survey paper presents a comprehensive overview of methods proposed in the current literature for the evaluation of ML explanations. We identify properties of explainability from the review of definitions of explainability. The identified properties of explainability are used as objectives that evaluation metrics should achieve. The survey found that the quantitative metrics for both model-based and example-based explanations are primarily used to evaluate the parsimony/simplicity of interpretability, while the quantitative metrics for attribution-based explanations are primarily used to evaluate the soundness of fidelity of explainability. The survey also demonstrated that subjective measures, such as trust and confidence, have been embraced as the focal point for the human-centered evaluation of explainable systems. The paper concludes that the evaluation of ML explanations is a multidisciplinary research topic. It is also not possible to define an implementation of evaluation metrics, which can be applied to all explanation methods. },
    doi = {10.3390/electronics10050593}
 }
 
 @inproceedings{zhu2020object,
   title={Object-oriented dynamics learning through multi-level abstraction},
   author={Zhu, Guangxiang and Wang, Jianhao and Ren, Zhizhou and Lin, Zichuan and Zhang, Chongjie},
   booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
   volume={34},
   pages={6989--6998},
   year={2020},
   doi={10.1609/aaai.v34i04.6183}
 }
 
 @inproceedings{ZiebartEtAl:2008:ImitationLearningNavigation,
   title={Maximum entropy inverse reinforcement learning.},
   author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K and others},
   booktitle={Aaai},
   volume={8},
   pages={1433--1438},
   year={2008},
   organization={Chicago, IL, USA},
   abstract={Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Prob- lems. This approach reduces learning to the problem of re- covering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behav- ior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling real- world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.}
 }
\ No newline at end of file
