Directions:
\begin{itemize}
   \item Types of interaction: 1. Student initiated 2. Teacher-initiated 3. Collaborative, single/multi-agent
   \item Skill levels of interacting parties: expert, novice, mediocre
   \item Interactive explanations? (Symbolic RL, Interpretable policies)
    \item Interactive learning in RL (along with Robotics)
\end{itemize}



% % \\\sd{Possible directions:\\
% % \begin{itemize}
% %     \item Explanation based interactive RL methods?
% %     \item Visual GUI dependent approaches to aid explanation in RL? (one where policies can be visualized using heat maps?)
% %     \item Do we want to mention the usual advice exchange methods between humans and agent in this survey? (action-advising, preference, reward shaping, policy shaping, demonstrations)
% %     \item Should the focus be mainly on interactive methods in RL for Robotics?
% % \end{itemize}}


%% IL Subsection

%Structure
% Human Teachers in IL
% What is IL
% Benefit of Human Teacher, Provide Demonstrations
% Remove Bias in Demos
% User Interfaces needed

%This is more along the history of humans coming in?
Reinforcement Learning with human teachers is an idea that exists since 2006: \cite{Thomaz:2006:RLWithHumanTeachers}. Until that time, the agent was learning individually, with the human only providing the reward scheme. The more detailed this was, the more the agent was performing under the influence of the strategies that the human thought of as profitable. For example, to teach an agent to play chess, one can be intrigued to reward particular actions that have short term benefits but at the same time, this excludes the possibility of the agent uncovering a long term benefit out of this action. Therefore, one of the basic reward schemes supporting exploration is the ``Minimum Time to Goal'' \cite{Harmon:1997:ReinforcementLearningATutorial}, \cite{SuttonBarto:2018:RLIntroduction}: for every state other than the final one, the reward is set to $-1$. The final state can have a reward $\geq 0$, 
so that the fewer steps reaching the final state needed, the greater is the overall reward. This can lead to longer episodes and a much longer training time overall.  


%What is interactive learning (mention some more history)
Interactive Reinforcement Learning aims at involving humans along with an RL-agent so that the involving parties (human and RL agent) can interact with each other to improve the agent's performance/policy. The role of human is crucial in such settings because they are characterized as teachers with domain knowledge and contextual experience. 
%They are useful in either identifying the agent's mistakes or providing extra knowledge as inductive bias\cite{} which can scale up the RL agent's learning in terms of sample-efficiency and/or quality.
Usually, the first indicator for the need for human intervention is model performance, since a bad model is more likely to produce bad policies. However, good models can also lead to bad policies at deployment scenarios. In many cases, human intervention is simulated (using another agent as pseudo-human) since both the parties benefits as the need to know where the imperfections of the model are can be detected before deployment. Another indicator is the agent requiring numerous interaction with the environment, thus leading to very long training time. In such scenarios, a human-in-the-loop can provide useful guidance as inductive bias~\cite{} to the RL-agent in order to learn faster. Human agent interactions can either be teacher-initiated~\cite{}, student-initiated~\cite{} or mixed~\cite{} depending on which party initiates the interaction. A survey paper that presents a taxonomy of all methods that evaluate and incorporate human advice for reward, value and policy shaping, as well as decision biasing, can be found in  \cite{Najar:2021:RLWithHumanAdvice, Arzate:2020:SurveyInteractiveRL}.



% Beyond reward shaping, action pruning is another way that agent-agnostic human-in-the-loop RL\cite{Abel:2017:AgentAgnosticHumanInTheLoopRL} can improve the learning of an RL agent. Some states that are known to be non-profitable or in some contexts even unsafe can be excluded by the user. Furthermore, the state representation is also a part of the RL algorithm that is directly influenced by the choices of the designer. What kind and how much information, as well as the relations between them, are design choices made typically before the training starts. One of the main future goals is that this gets to be adaptive, with the human-in-the-loop adding and removing relevant and correspondingly non-relevant features in the state representation driven not only by the observed action sequence and performance of the model but also from xAI. 

%Providing demonstrations by expert
Imitation learning~\cite{SchaalXXX} has been long used to train models from just human demonstrations in a supervised manner. Inspired by this direction, teacher demonstrations of varying optimality have been used inside RL in addition to the agent's online experience to speed up the training process. Before the advent of Deep RL, \cite{kim2013learning} incorporated  demonstrations as linear constraints in the Approximate Policy Iteration framework, posing it as a constrained convex optimization problem. \cite{WangXXX} inferred the confidence of predicted actions from  sub-optimal demonstrations and used this confidence value to decide whether to take this action or randomly explore other actions. Along the Deep RL literature, offline human demonstration data has been used with popular off-policy algorithms for discrete\cite{hester2018deep} and continuous action spaces\cite{nair2018overcoming,vecerik2017leveraging} which includes a separate loss function to mimic the expert behavior. However, some of these~\cite{hester2018deep} have been used just for simple games without application to more complex domains in Robotics. Most of these existing methods assume human demonstations to be provided prior to training without the human getting an idea of the agent's policy.

% Teacher Initiated

Reinforcement Learning with human teachers is an idea that exists since 2006: \cite{Thomaz:2006:RLWithHumanTeachers}. Until that time, the agent was learning individually, with the human only providing the reward scheme. The more detailed this was, the more the agent was performing under the influence of the strategies that the human thought of as profitable. For example, to teach an agent to play chess, one can be intrigued to reward particular actions that have short term benefits but at the same time, this excludes the possibility of the agent uncovering a long term benefit out of this action. Therefore, one of the basic reward schemes supporting exploration is the ``Minimum Time to Goal'' \cite{Harmon:1997:ReinforcementLearningATutorial}, \cite{SuttonBarto:2018:RLIntroduction}: for every state other than the final one, the reward is set to $-1$. The final state can have a reward $\geq 0$, so that the fewer steps reaching the final state needed, the greater is the overall reward. This can lead to longer episodes and a much longer training time overall. 

%This para is about user interfaces
For effective interaction, the user must evaluate the agent by visualization and explanation of its policies, reward scheme (known or inferred) and even the data that are used before and during the training. This can only be accomplished with the use of adequate UI; even sound interfaces that use the techniques of audification and sonification \cite{Hermann:2011:Sonification}, \cite{Saranti:2009:QuantumHarmonicOscSonification} can be used to guide the human in RL parameter adjusting \cite{Scurto:2021:DesigningDeepRLHumanParameterExploration}. 
A user interface has a central role to the implementation of interactive user intervention to the reward scheme \cite{Thomaz:2006:RLWithHumanTeachers}. The human is characterized as ``trainer'' and can interfere at any time point of the training process asynchronously, providing a reward in the range of $[-1, +1]$ (kind of a thumbs up or down). This eventually speeds up the training time for that particular task. Nevertheless, at that time the user did not have any additional information about the estimated long-term benefit of the proposed action that might be provided by xAI methods - most importantly counterfactual ones. Users might observe that the strategy of an agent is not effective or the sequence of actions seems ``unnatural'' instead of reshaping the reward function (which can become very complex and consider a lot of aspects to optimize). This and other works like \cite{Christiano:2017:DeepRLHumanPreferences} and \cite{LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE} acquire human feedback by letting the human compare the learned action sequences and avoid reward exploitation. The relationship between rewards and actions has even been studied in the opposite direction; researchers investigate also what users can infer about the reward scheme by observing actions of the agent \cite{Abbeel:2004:InverseRL}.

%Removing bias in demonstrations
 Offline human demonstrations may contain biases which can, in turn, be removed by experts  \cite{Wang:2022:SkillPreferences}. A model can be learned from the offline data and then used as a generative one to create an arbitrary amount of data that will help to detect biases easier. Furthermore, users provide feedback on the learned skills, express their preference thereby improve the effectiveness of the algorithm. xAI in that case can also shed light on the biases of the model even before the model is used to generate the data, so that the human expert can - apart from the observed learned agent skills - can have another view of the model and thereby infer a different interpretation of the data \cite{Ribeiro:2016:WhyShouldITrustYou}, \cite{Mehrabi:2021:SurveyBiasFairness}.

%% Causal Learning

% That showed that there is a connection between important parts of the input that are indicative for the discriminative task and the relative certainty of the classifier about its prediction. It is important to note, that the authors do not explicitly state this association as causal (which is scientifically correct), but still open the path for the exploration of causal dependencies between elements in the data and performance of the AI model.
% (To better understand what a negatively relevant element is, think of a classifier that discriminates between images of dogs and images of cats. After being trained by a convenient dataset, with the images of both classes having the animal in the middle and so on, at test time the neural network is confronted by an image having both a dog and parts of a cat. No matter what class the prediction will be, the certainty of the prediction is expected to be very small. If the prediction is ``dog'', then the elements of the image that belong to the cat are negatively relevant to this class and vice versa. So if a method removes those elements and replaces them with some background information like grass for example, then the prediction performance towards the class ``dog'' is expected to increase). 

In this work, the researchers show that after the application of the xAI method Layer-wise Relevance Propagation (LRP) \cite{Bach:2015:LayerWiseRelevancePropagation}, the removal of elements of the input that positively contribute to a correct prediction will induce a monotonic decrease in the prediction performance. To create a causal model and detect the random variables involved, as well as their values, many removal and additions of positively and negatively relevant elements - the ones that indicate parts that ``speak against'' the predicted class - must be made. 

%% CHallenges subsection


%Human versus machine perception

%Several research works consider the similarities and differences of perception capabilities of humans and machine learning models in the medical domain \cite{Makino:2020:DifferencesHumanMachineMedical}. Although both of them can be modelled by the same Probabilistic Graphical Model (PGM) \cite{Koller:2009:ProbabilisticGraphicalModels}, \cite{Saranti:2019:LearningCompetencePGM}, as far as structure goes, they do have significant differences in the learned parameters and predicted outcomes of the posterior variables. Humans are in some cases more robust to perturbations of the input data; it can be shown that machine learning models don't just suffer from dataset shift but also information loss. Furthermore, there is a discrepancy between them, when comparing what parts in the input image were helpful for the diagnosis. By comparison with radiologists diagnoses, researchers can enhance the dataset with images that will occur in medical practice and make the machine learning models more robust to perturbations.


% \subsubsection{Opportunities}
% Large opportunities for reinforcement learning can be found in reducing data requirements for training agents. This can be done via various means, such as Contrastive Learning, MT-Learning, Imitation Learning or Curriculum Learning. In any case can a reduction of explicit data requirements mean a faster deployment of the agent and ideally a system that is more robust in general.

% Generalization and robustness can also be enhanced with approaches of multi-task learning and and meta-learning. Here, we can leverage new modalities and sensors, embue systems with logic (eg. by high-level representations) and focus on inductive biases such as core knowledge \cite{RoyEtAl:2021:RLRoboticsChallenges}. Both approaches also most often include an increase overall performance, especially noted with Multiple Tasks, HITL methods, Sys1Sys2 Approaches of \cite{RoyEtAl:2021:RLRoboticsChallenges}.


% Cut on the 27.05.22

 %The human should also be able to provide advice in an intuitive/easy way which might later be converted into an appropriate format of interest in the underlying algorithm.

% In recent times, RL has been successfully applied to solve many real-world problems, ranging from drug discovery~\citep{popova2018deep} to robot manipulations~\citep{nguyen2019review}. While this is an exciting avenue for research, RL systems still face a lot of bottlenecks, including sample inefficiency, sim-to-real transfer issues, generalization, insufficient simulator model, etc., to name a few. Interactive learning~\citep{Arzate:2020:SurveyInteractiveRL} aims to involve a human-in-the-loop to address a few of these challenges by leveraging domain knowledge and rich human experience. Such interactions could either be human-initiated~\citep{torrey2013teaching}, student-initiated~\citep{da2020uncertainty} or jointly initiated~\citep{amir2016interactive} by both the parties. A human could either be involved during training~\citep{Knox:2008:TAMER} or in the deployment phase of the RL model~\citep{guo2021edge}. 

% Reinforcement learning is partially inspired by learning rules discovered in biological systems, where real-world learning problems surface \citep{NeftciAverbeck:2019:RLBiologicalSystems}. This also shows the fundamentality of Reinforcement Learning in general, and serves as inspiration how RL approaches could help us with creating more intelligent agents. But research also shows that the research RL such as advances in hierarchical and model-based RL helps with explaining findings in biological systems \citep{ShteingartLoewenstein:2014:RLHumanBehavior}. Interactive learning could show to be one of those cases, since parents teaching their offspring is one of the fundamental ways of learning in nature. Furthermore, the combination with exploring and playing is well reflected in HITL approaches like pre-training (exploring) and hierarchical learning (playing).

% % A human-in-the-loop approach can incorporate the human in the training of a Deep RL agent to improve its performance and strategically eliminate non-appropriate policies by targeted intervention. This strategy is called Interactive Reinforcement Learning \citep{Arzate:2020:SurveyInteractiveRL} and contains beyond the reward, feedback from the user. To do that, the user must evaluate the agent by visualization and explanation of its policies, reward scheme (known or inferred) and even the data that are used before and during the training. This can only be accomplished with the use of adequate UI; even sound interfaces that use the techniques of audification and sonification \citep{Hermann:2011:Sonification}, the work \citep{Saranti:2009:QuantumHarmonicOscSonification} can be used to guide the human in RL parameter adjusting \citep{Scurto:2021:DesigningDeepRLHumanParameterExploration}. 

% % TODO: Image of interactive RL 

% % Human as teacher

% In interactive learning, the human is characterized as a teacher and the teaching loop can contain different types of critique, advice modalities, guidance that can be fed back to the RL algorithm. A comprehensive survey of various types of human guidance in Deep RL can be found in \citep{zhang2019leveraging}. Usually, the first indicator for the need for human intervention is model performance, since a bad model is more likely to produce bad policies. However, good models can also lead to bad policies as well at deployment scenarios because of the sim to real gap. In many cases, human intervention is simulated from pseudo-agents since both the potential benefits and the need to know where the imperfections of the model are can be detected before deployment. The designer of the interactive framework must also take into account that the human interventions might not always be perfect or beneficial; the user might need special training and an informative user interface to effectively improve the RL algorithm. The human should also be able to provide advice in an intuitive/easy way which might later be converted into an appropriate format of interest in the underlying algorithm.

% % The human is characterized as a teacher and the teaching loop can contain different types of critique, action advice, guidance that can be fed back to the RL algorithm. Usually, the first indicator for the need for human intervention is model performance, since a bad model is more likely to produce bad policies. Still, good models can also lead to bad policies as well at deployment scenarios. In many cases, human intervention is simulated since both the potential benefits as the need to know where the imperfections of the model are can be detected before deployment. The designer of the ecosystem with the human-in-the-loop must also take into account that the human interventions might not always be perfect or beneficial; the user might need special training and an informative user interface to effectively improve the RL algorithm.
% %-- to here
% % how does that fit in above?
% %A human teacher can target each of the components of the agent's MDP to give feedback. The various types of feedback provided by a teacher can be on the reward function, agent policy, exploration guidance or value functions for agent training. 

% \citet{Arzate:2020:SurveyInteractiveRL} classify methods of interactive RL according to the way human feedback tailors an RL dimension. As such, a method could modify the reward function, the agent's policy, the exploration process, or the value function. 

% In reward shaping RL, designers alternate between designing and evaluating the reward function until the agent performs as desired \citep{ng:99}. Reward shaping is useful in sparse reward environments and facilitates the reward specification in complex domains. However, it has two inherent problems: 1) There is a delay between the action's occurrence and the human feedback. 2) Since sometimes is impossible to anticipate all possible scenarios, the agent could find irrational behaviors that hinder the achievement of the designer's desires \citep{Arzate:2020:SurveyInteractiveRL}. 

%  \citet{Thomaz:2006:RLWithHumanTeachers} characterize the human as ''trainer'' who can interfere at any time point of the training process asynchronously, providing a reward in the range of [-1, +1] (kind of a thumbs up or down). This eventually speeds up the training time for that particular task. Directly reshaping the reward, i.e., without a trainer's feedback,  can become very complex since the designer needs to consider many aspects to define a reward function that captures what we actually want \citep{hadfield:17}.  \citet{Knox:2008:TAMER} propose the TAMER framework where the reward is replaced by human evaluative reinforcement. In this setting, the trainer qualifies state-action pairs as positive or negative. Later, \citet{knox:13} use TAMER to train real robots, where reinforcement is provided using two control buttons. This and other works like \citep{Christiano:2017:DeepRLHumanPreferences} and \citep{LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE} acquire human feedback by letting the human compare the learned action sequences and avoid reward exploitation. The relationship between rewards and actions has even been studied in the opposite direction; researchers investigate also what users can infer about the reward scheme by observing actions of the agent \citep{Abbeel:2004:InverseRL}. 

% While giving feedback speeds up agent learning, it can also cause fatigue or boredom to human trainers \citep{akalin:21}, which reduces the advice quality and frequency, resulting in a diminished cumulative reward. Additionally, some settings require expert or knowledgeable trainers [cite]. Therefore, it is desirable to account for non-expert or na\"ive trainers. \citet{jiang:21} propose a framework for non-experienced users to interact with average-reward RL algorithms. More precisely, it allows teachers to give high-level advice, reducing the required amount of advice and mental load. The approach receives the trainer input using linear temporal logic (LTL) and guarantees optimality independent of the advice quality. LTL formulas shape the environment reward and guide the RL agent in learning an optimal policy.
  
% Methods that consider modifying the agent's policy are called policy shaping. These methods augment an agent's policy directly using human knowledge. This technique does not require a well formulated reward function, but it assumes the trainer knows a near-optimal policy to guide the agent. \citet{griffith2013policy} uses feedback (+1,-1) from simulated agents just like the TAMER framework discussed previously to directly guide the agent’s policy and is oblivious to noisy human advice. This was later extended to advice from human teachers by \citep{cederborg2015policy} and addressed the real time issue of interpreting “silence” of human trainers. \citet{macglashan2017interactive} proposed COACH and showed that human feedback is dependent on the agent’s current policy. This method interpreted feedback as the advantage function typically used in actor-critic algorithms to update the policy parameters.

% Finding a good policy in RL requires exploring the state space. Guided exploration process methods use human knowledge to guide the agent's exploration by suggesting states with a high reward. These methods require the trainer to identify good policies. \citet{Thomaz:2006:RLWithHumanTeachers} observed that humans try not only to give feedback on past actions but also to guide future actions; their Interactive RL algorithm was implemented on a physical robot by \citep{suay:11}. The algorithm combines guidance with exploration, allowing the robot to outperform the trainer. Besides suggesting desirable states, users can also remove access to dangerous spaces. Action pruning is another way where human-in-the-loop RL can guide exploration and improve learning \citep{Abel:2017:AgentAgnosticHumanInTheLoopRL}. In addition to reducing the branching factor, the user can exclude some state-action pairs known to be non-profitable or even unsafe.


% Value functions represent the future expected reward an agent could achieve, contrary to reward functions that specify the current reward. When considering value functions, an approach called augmented value function combines a value function created by human feedback with the value function of the agent. Using this approach, \citet{kartoun:10} propose a self-aware robot that controls its learning. The robot chooses between its value function and Q values directly manipulated by a human, through a linguistic-based interface, to learn how to shake a bag and release a knot tying it. \citet{jiang:21} propose a real-time human-guidance-based deep reinforcement learning method for policy training in autonomous driving domains. \citet{WuEtAl:2021:HITLDRLAutonomousDriving} use a control transfer mechanism that allows a human driver to intervene and correct unreasonable agent actions. The intervention occurs in real-time during the model training process, and human guidance is used to develop an improved actor-critic architecture with modified policy and value networks. Demonstrations \citep{hester2018deep,vecerik2017leveraging,nair2018overcoming} from humans can also augment the value function by biasing the value function parameters in accordance to the  actions taken by the expert. These approaches have been particularly successful in complex robotics tasks like pushing, sliding etc which can easily be demonstrated by humans to guide the noisy agents during the initial training stages.  

% Typically, the offline data that will be used for training the Deep RL algorithm are either generated or selected by humans. By default, they may contain biases which can, in turn, be removed by experts \citep{Wang:2022:SkillPreferences}. A model can be learned from the offline data and then used as a generative one to create an arbitrary amount of data that will help to detect biases easier. Furthermore, users provide feedback on the learned skills, express their preference thereby improve the effectiveness of the algorithm. xAI in that case can also shed light on the biases of the model even before the model is used to generate the data, so that the human expert - apart from the observed learned agent skills - can have another view of the model and thereby infer a different interpretation of the data \citep{Ribeiro:2016:WhyShouldITrustYou,Mehrabi:2021:SurveyBiasFairness}. A survey paper that presents a taxonomy of all methods that evaluate and incorporate human advice for reward, value and policy shaping, as well as decision biasing, is provided in \citep{Najar:2021:RLWithHumanAdvice}.

% Using human knowledge to train RL algorithms can only be accomplished using adequate user interfaces (UIs). The type of UI (hardware-delivered or natural interaction) determines the degree of expertise required and can affect the quality of the feedback \citep{lin:20}. Keyboard keys, mouse clicks with sliders, and game controllers are examples of UIs in hardware-delivered interactions, and experts or knowledgeable trainers generally use these UIs. On the other hand, sound interfaces that use the techniques of audification and sonification \citep{Hermann:2011:Sonification, Saranti:2009:QuantumHarmonicOscSonification,kartoun:10,Scurto:2021:DesigningDeepRLHumanParameterExploration}, cameras to capture facial expressions \citep{arakawa:18}, etc. are examples of UIs for natural interaction that non-expert users prefer. 

% Like any interaction, interactive RL requires a level of agent-human understanding. Researchers found that most people training an AI agent assume that their behavior reveals their knowledge \citep{habibian:21}. Hence, some approaches account for this human belief by making the robot behavior interpretable. A learning robot may query the trainer to learn the true reward function. The robot then selects different behaviors and asks people about their preferences. \citet{habibian:21} study the influence of robots' questions on how their trainers perceive them. In their approach, the robot chooses informative questions that simultaneously reveal its learning. Compared to other approaches that do not account for human perception, \citet{habibian:21} found out that people prefer revealing+informative questions since they find these questions easier to answer, more revealing about the robot's learning, and better focused on uncertain aspects of the task. A follow-up study determined that accounting what the human learns from the robot's learning helps decide which features are still unclear to the robot and whether the robot is ready to be deployed. 

% Besides interpretability, explainability can also improve interactions between humans and agents. \citet{TabrezHayes:2019:xRLTextualExplanations} focused on establishing a shared mental model between robots and humans. They assumed possible disparity between robots and people's models and proposed an interactive approach where the robot uses explanations to fix the human model of the reward function. While explanations usually assume a human explainee, interactive RL may require giving explanations to the agent. One common way of providing feedback is to evaluate agent actions as positive or negative \citep{Knox:2008:TAMER,knox:13,arakawa:18,macglashan2017interactive}. However, this limited feedback could improve if the trainer explains why specific actions are wrong. \citet{guan2020explanation} augment the binary evaluative feedback with visual explanations using saliency maps from humans. In addition to improving the agent's sample efficiency, the approach also reduces the human input required. 




% Role of UI, Human as Trainer and reward shaping
%Human feedback can improve the learning rates significantly due to prior knowledge about the task to be executed \citep{Arzate:2020:SurveyInteractiveRL}. Moreover, the behaviour of the agent can be customised through the integration of human feedback. Thus the inclusion of different user groups could lead to various outcomes in the agent's behaviour and skills. However, designing a reward function that adheres to this, even in unseen situations, is an extremely complex task. Moreover, since humans are included in the learning process, it must be ensured that the agent received feedback. Users might get bored or irritated due to the actions of the agent and therefore the feedback is not consistent completely missing \citep{IsbellEtAl:2006:AdaptiveSocialAgent}. Hence, the intentions of users and their communication with the agent should be considered. According to the target user group, the feedback type must be customizable. For example, binary feedback in the form of ''thumbs up'' or ''down'' might not lead to the desired outcome. Users may punish the agent for wrongdoing, but may not reinforce positive behaviour. Thus, the design of the feedback type plays an important role in the overall performance.

%%%%%%%%%%
%A user interface has a central role to the implementation of interactive user intervention to the reward scheme \citep{Thomaz:2006:RLWithHumanTeachers}. The human is characterized as ``trainer'' and can interfere at any time point of the training process asynchronously, providing a reward in the range of $[-1, +1]$ (kind of a thumbs up or down). This eventually speeds up the training time for that particular task. Nevertheless, at that time the user did not have any additional information about the estimated long-term benefit of the proposed action that might be provided by xAI methods - most importantly counterfactual ones. Users might observe that the strategy of an agent is not effective or the sequence of actions seems ``unnatural'' instead of reshaping the reward function (which can become very complex and consider a lot of aspects to optimize). This and other works like \citep{Christiano:2017:DeepRLHumanPreferences} and \citep{LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE} acquire human feedback by letting the human compare the learned action sequences and avoid reward exploitation. The relationship between rewards and actions has even been studied in the opposite direction; researchers investigate also what users can infer about the reward scheme by observing actions of the agent \citep{Abbeel:2004:InverseRL}.

% Action Pruning

%Beyond reward shaping, action pruning is another way that agent-agnostic human-in-the-loop RL\citep{Abel:2017:AgentAgnosticHumanInTheLoopRL} can improve the learning of an RL agent. Some states that are known to be non-profitable or in some contexts even unsafe can be excluded by the user. Furthermore, the state representation is also a part of the RL algorithm that is directly influenced by the choices of the designer. What kind and how much information, as well as the relations between them, are design choices made typically before the training starts. One of the main future goals is that this gets to be adaptive, with the human-in-the-loop adding and removing relevant and correspondingly non-relevant features in the state representation driven not only by the observed action sequence and performance of the model but also from xAI. A survey paper that presents a taxonomy of all methods that evaluate and incorporate human advice for reward, value and policy shaping, as well as decision biasing, is provided in \citep{Najar:2021:RLWithHumanAdvice}.

% Application of IL/xAI

%Typically, the offline data that will be used for training the Deep RL algorithm are either generated or selected by humans. By default, they may contain biases which can, in turn, be removed by experts  \citep{Wang:2022:SkillPreferences}. A model can be learned from the offline data and then used as a generative one to create an arbitrary amount of data that will help to detect biases easier. Furthermore, users provide feedback on the learned skills, express their preference thereby improve the effectiveness of the algorithm. xAI in that case can also shed light on the biases of the model even before the model is used to generate the data, so that the human expert can - apart from the observed learned agent skills - can have another view of the model and thereby infer a different interpretation of the data \citep{Ribeiro:2016:WhyShouldITrustYou,Mehrabi:2021:SurveyBiasFairness}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Content removed from Developing Section


%% The content of this subsection is now included in IRL
%\subsection{Counterfactuals}

%We already highlighted the use of counterfactual explanations, since they are very intuitive for the human user. Karalus \emph{et al.} adopt the classical approach by \citep{Knox:2008:TAMER}, referred to as TAMER as their principle framework. They build on a more recent extension, DeepTAMER \citep{Warnell:2018:DeepTAMER} to enhance human feedback with counterfactual explanations. In case of a negative feedback, the humans can communicate to the the agent \emph{if} the action $a$ was performed in a different state $s'$, the feedback would have been positive. The authors choose to limit the counterfactual feedback only to the negative reward cases where they think they will be of most substantial benefit. The counterfactual feedback based on both actions and states is demonstrated to yield significant improvements in the speed of convergence. This approach could also be incorporated for the "Using" phase, but has to be optional since its added complexity could be a hindrance to the user.

%Another example for the application of counterfactuals is provided by \citep{Pearl:2009:Causality}, which use Dynamical Structural Causal Models (DSCM) as their framework to explicitly model the differences in capabilities of the agent and the human operator as the world states evolve over time. In this framework, the agent views the human feedback as the intended action and adjusts it (using counterfactual reasoning), if the action is sub-optimal. A trade-off between autonomy and optimality is demonstrated, meaning that fully autonomous agents are likely to be sub-optimal and could only achieve optimality if they receive critical feedback from their human operators. The counterfactual approach proposed by the authors improves on standard methods even when human advice is imperfect.


%% none of the cited papers have explainability, moved the relevant ones to background
%\subsection{Approaches specific to Agent Learning}

%We furthermore identify three larger, specific categories of approaches for Agent Learning: preference-based learning, imitation learning and querying.

%In preference-based learning, an agent proposes options to human, which chooses a preferred behavior - as seen in the PEBBLE framework by \citep{LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE}, where the reward model is learned by actively querying the preferences of the HITL between different behavior approaches. Another example is provided by \citep{HudecEtAl-2021-Interpretable}, where experts assign only best possible option with classification by aggregation function, which allows the agent to rapidly learn the best possible options. The third example is to offer two options for the HITL to select a preference from. This approaches enable the agent to better determine when to ask the human to alter the state space or to add actions \citep{MandelEtAl:2017ActionsInHITL}.

%The second approach is imitation learning, where an agent learns by observing and imitating a human domain expert and has been applied in fields like autonomous driving, assistive robotics and humanoid robots, where it helps to solve the problem of high-dimensional reward functions. The agents aim is to solve a given task by observing demonstrations of a human teacher \citep{HusseinEtAl:2017:ImitationLearning}. However, the agent needs to be able to generalise the observed data, in order to solve unknown tasks of the same kind. Thus the agent has to learn policies for the human demonstration. Nevertheless, the agent needs to extract information from its surroundings as well as any changes thereof. Further, the agent needs to learn the mapping between the current situation and the observed behaviour. \citet{ZiebartEtAl:2008:ImitationLearningNavigation} show how the approach of imitation learning can be applied to navigation tasks, where route preferences can be modeled more accurately by mimicking observed behavior.

%In the third approach of querying, the agent can actively ask the teacher for support when he encounters problems.
% Replanning with human guidance for object gripping
%An example for that is the online replanning framework proposed by \citep{PapallasEtAl:2020:OnlineReplanningTrajectories}, where the robot asks human when stuck in local minimum. The human can then suggest an optimal route, enabling the human to manage a fleet of robots.

% HITL Driving where Human actions override agents
%Another example is in HITL driving system, where the teacher can provide real time correction to the agents actions, which then also performs imitation learning on those corrections. The evaluation of this approach showed that humans even prefer retaining more control to more performance \citep{WuEtAl:2021:HITLDRLAutonomousDriving}.

%HITL Error detection for Sentiment Analysis
%\citep{LiuGuoMahmud:2021:HITLErrorDetectionFramework} give an example how a model can identify prediction errors and prompt them to human user. For that, it presents the top N global features for a learned sentiment analysis model for human supervision. This minimizes querying and focuses on most important features with the strongest contribution.
