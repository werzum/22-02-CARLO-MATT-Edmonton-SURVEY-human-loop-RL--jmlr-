%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2022-01-22 ah - last revision 2022-05-29 15:15 CET ah
\documentclass[twoside,11pt]{article}
% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{jmlr2e}
\usepackage{bm}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{array, multirow}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{booktabs}

\usepackage{makecell}%To keep spacing of text in tables
\setcellgapes{4pt}%parameter for the spacing

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\DeclareUnicodeCharacter{2212}{−}


\usepackage{xcolor}
\usepackage[normalem]{ulem}
\newcommand{\addtxt}[1]{{\textcolor[rgb]{0.0,0.5,0.25}{{ #1}}}}
\newcommand{\sd}[1]{\textcolor{red}{[#1 \textsc{--Srijita}]}}
\newcommand{\MET}[1]{\textcolor{blue}{MET: #1}}
\newcommand{\AH}[1]{\textcolor{green}{AH: #1}}
\newcommand{\ytp}[1]{\textcolor{orange}{Tianpei: #1}}
\newcommand{\MO}[1]{\textcolor{red}{Mohammad: #1}}


% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

\usepackage{lastpage}
\jmlrheading{21}{2022}{1-\pageref{LastPage}}{3/22; Revised
x/22}{X/22}{20-212}{Retzlaff, Saranti, Angerschmid, Mousavi, Das, Wayllace, Afshari, Yang, Taylor, Holzinger}

% Short headings should be running head and authors last names

\ShortHeadings{Human-in-the-Loop Reinforcement Learning}{Retzlaff, Saranti, Angerschmid, Mousavi, Das, Wayllace, Afshari, Yang, Taylor, Holzinger}
\firstpageno{1}

% added to fix Todolist error 
\setlength {\marginparwidth }{2cm}

\begin{document}

\title{Human-in-the-loop Reinforcement Learning: A Survey of Requirements, Challenges, and Opportunities}

\author{\name Carl Orge Retzlaff \email carl.retzlaff@human-centered.ai\\ 
\addr Human-Centered AI Lab, University of Natural Resources and Life Sciences Vienna, Austria\\
and DAI Lab, TU Berlin, Germany\\
\AND
\name Srijita Das\email srijita1@ualberta.ca \\
\addr Department of Computing Science \\ 
University of Alberta, Canada
\AND
\name Christabel Wayllace\email wayllace@ualberta.ca \\
\addr Department of Computing Science \\ University of Alberta, Canada\\
\AND
\name Payam Mousavi \email payam.mousavi@amii.ca \\
\addr Alberta Machine Intelligence Institute, Canada\\
\AND
\name Anna Saranti \email anna.saranti@human-centered.ai \\
\addr Human-Centered AI Lab, University of Natural Resources and Life Sciences Vienna, Austria\\
\AND
\name Alessa Angerschmid \email alessa.angerschmid@human-centered.ai \\
\addr Human-Centered AI Lab, University of Natural Resources and Life Sciences Vienna, Austria\\
\AND
\name Mohammad Afshari\email mafshari@ualberta.ca \\
\addr Department of Computing Science \\ University of Alberta, Canada\\
\AND
\name Tianpei Yang\email tianpei.yang@ualberta.ca \\
\addr Department of Computing Science \\ University of Alberta, Canada\\
\AND
\name Matthew E.~Taylor \email matthew.e.taylor@ualberta.ca \\
\addr Department of Computing Science\\
Alberta Machine Intelligence Institute\\
University of Alberta, Canada
\AND
\name Andreas Holzinger \email andreas.holzinger@human-centered.ai \\
\addr Human-Centered AI Lab, University of Natural Resources and Life Sciences Vienna, Austria \\
and xAI Lab, Alberta Machine Intelligence Institute\\
University of Alberta, Canada
}


\editor{N.N.}

\maketitle

\newpage

\begin{abstract}%
Artificial intelligence (AI) in general, and reinforcement learning (RL) in particular, holds the promise of agents learning autonomously to accomplish difficult tasks with superhuman performance. However, even when an agent will eventually perform its task autonomously, we argue that RL is fundamentally a human-in-the-loop paradigm. For instance, an RL agent learns to act in a Markov decision process (MDP), but it is a \emph{human} who sets up the agent and specifies the MDP. Without a human, the agent would have \textit{no concept} of what a reward is or how to define it.  

In this article, we argue that human-centrality is a key to successful RL, which has not been adequately considered in existing literature. We show how the application of xAI and specific improvements to existing explainability techniques can enable a better human-agent interaction in Human-in-the-Loop (HITL) RL applications for all types of users,  whether for lay people, subject matter experts, or machine learning experts.

The central insights from this article are summarized in Table \ref{table:Explanations_table}, giving an overview of the different phases for the agent deployment, with their respective requirements, aspects of human involvement, explanation types, and new challenges. 

Our goal is that readers of this article will agree with our argument that RL is fundamentally a human-agent interactive process. Furthermore, readers will learn how current state-of-the-art explainability methods can be used in HITL RL, be able to identify low risk, high return applications of explainability research in RL, and appreciate longer-term research goals required to bring this field to fruition. We finally propose the vision of a human-robot collaboration that allows both humans and robots to realize their full potential and cooperate utilizing their respective strengths.

\end{abstract}


\begin{keywords}
Reinforcement Learning, Human-in-the-loop, Explainable AI, Embodied Intelligence, Explainability
\end{keywords}


\section{Introduction}
\label{sec:introduction}

%RL = autonomous learning
%But, really humans involved throughout
%1) where humans interface with RL and 2) why explainability is critical
%Selecting algorithm, parameters, designing MDP, etc.
%Pretraining/incorporating knowledge in features, etc.
%Interactive teaching (curriculum learning), designing when to change MDP/agent (return to previous step)
%Decision to deploy: trust, safety
%During deployment: re-training, stopping, returning to drawing board
%
%Point of this article
%1) RL is human-in-the-loop
%2) Explainability is critical
%3) What we can do, what should do next (low hanging fruit), what's a long way off still


Reinforcement learning~\citep{SuttonBarto:2018:RLIntroduction} (RL) is a general framework where an agent can autonomously learn how to take actions in order to best maximize the discounted long term sum of rewards. RL agents have had many impressive successes in board games, video games, robotics, natural language processing, and other applications \citep{Li:2017:DRLSurvey}. One of the benefits of RL is that agents can learn to outperform humans, sometimes coming up with novel (and unanticipated) strategies.

The RL paradigm fits well with the long-standing goals of artificial intelligence in that agents can be placed in an environment and autonomously learn how to solve difficult problems. However, we argue that this framing overlooks the significant human input and biases encoded into every RL problem. We argue that: 
\begin{center}
\fbox{
    \parbox{0.95\textwidth}{
         \begin{enumerate}
            \item Reinforcement learning is fundamentally a Human-in-the-Loop paradigm.
            \item Explainability is critical for the success of real-world \\RL applications.
        \end{enumerate}
     }%
}
\end{center}

First, we argue that RL is a Human-in-the-Loop (HITL) paradigm and identify four stages where human involvement is critical to the goal of deploying and using RL agents. We emphasize that those stages are cyclic and only loosely ordered in the presented sequence, meaning that the individual phases can be repeated during the entire process of deployment.

This article is also a position paper. We argue that ignoring humans and treating RL as a fundamentally autonomous learning paradigm is shortsighted --- we will highlight where and how explainability can play a critical role in those four steps of human-agent collaboration. 

Thirdly, this article is also a survey on explainability in RL. We identify three types of challenges and frame them as generations of technologies to emphasize future developments and the steps needed to take to achieve the larger goal of human-in-the-loop RL. The long-term challenges are discussed extensively in Subsection~\ref{sec:ThirdGeneration}.

The following steps contain the identified,distinct stages for RL deployment and how humans are involved in each.

\vspace{2mm}
\emph{1: Initial Agent Development}

The development phase is characterized by the software and systems experts laying the technical groundwork. Here, the focus is on the RL model itself.

The human defines the problem to be solved by the agent, and selects the agent’s environment accordingly.

For that, human machine learning and/or subject matter specialists construct a Markov decision process (MDP), defining the state space, action space, and reward function. All three have a critical impact on the speed of learning, the agent's final performance, and what policy is learned. 

The human specialists also set the agent's algorithm and hyperparameters and decides whether to incorporate prior knowledge by adding handcrafted features or transferring knowledge from an existing model. Another consideration is whether the agent should pre-train on existing data.
\vspace{2mm}

\emph{2: During Agent Learning}

The model is trained in an interactive fashion with a sample user, under close supervision of the developers. This phase focuses on understanding the model perceptions and fundamentals, and assesses the agent’s cooperation capabilities with the end user. 

Humans can decide to disallow the agent from selecting invalid or suboptimal actions. The agent's action selection can also be expanded with a bias by an expert to  enable faster learning rates. Furthermore, the developers can decide to include an interactive paradigm with human demonstration, feedback, advice, or other types of cooperation.
Eventually, the subject matter expert judges if the learning process is successful or if the MDP (or other agent components) should be revised. This can be either because the agent is learning too slowly or because the policy being learned is otherwise unsuited for the problem.
\vspace{2mm}

\emph{3: Agent Evaluation}

The third phase is characterized by testing the behavior of the trained model, which requires tools which enable comparability of quantify the learning progress and can furthermore scale up to to thoroughly evaluate larger-scale models.

Developers and subject matter experts exhaustively test the learned model.
The developers need to ensure that no erroneous behavior or glitches have emerged during training, focusing mainly on the syntactical level. 

The subject matter expert will need to understand and evaluate the learned policies for sensible micro- and macro-behavior, therefore focusing more on an evaluation of the semantic behavior. 

The judgement for either moving forward with the deployment to market or engaging in another development-learning-evaluation cycle lies in this phase and is made by the developers and project owners. The subject matter expert must decide if the learned policy is ready for deployment, if more training is needed, or if the problem definition needs to be changed. 
\vspace{2mm}

\emph{4: Deployment of Learned Agent}

A human typically needs to argue why the agent is safe and should be deployed in the real world from a vendor perspective. The development team will need to determine if the agent should continually learn, if its policy should be frozen, or if it should retrain if an environment change makes this necessary.  

The customer and end user then makes the final deployment decision, determining usage context and specific application of the agent in the field.

The agent is then deployed in the real world and interacts with the end user. It now has to provide an efficient interaction for different users, while also building trust by providing explanations for its behavior.
\vspace{2mm}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{img/HITL Deployment Workflow.pdf}
    \caption{Overview of the sequence of the four steps to deployment. The ideal path from the Initial Formulation is shown with the bold arrows and goes over Agent Development, Agent Learning and Agent Evaluation to Agent Deployment. The dotted arrows show the possible paths for revising different aspects of the agent and are centered around the Agent Evaluation step for assessing the model flaws.}
    \label{fig:Deployment_Workflow}
\end{figure}

%Should be in which paragraph above?
Figure \ref{fig:Deployment_Workflow} gives an overview of the sequence of steps for the agent deployment.

We furthermore argue that explainability is a critical and underdeveloped technology in each of these four stages. Before training, explainability can help show the impact of algorithm or hyperparameter selection, how prior knowledge biases the agent, and how pre-training changes the agent's learning process. During training and testing, explainability can show the impact of biasing via an existing controller or human advice, how learning is progressing, or how the current policy functions. In the deployment phase, explainability can help users understand the final policy, improve trust, evaluate safety, and understand the policy's stability.

We therefore want to shift the discussion about RL to embrace human interaction and cooperation. We furthermore provide an entry point into this exciting area of contemporary research at the intersection of explainability on RL, with the goal of giving also non-experts a starting point for HITL RL research. Throughout the paper, we focus on human-robot interaction and therefore touch mainly on topics concerning embodied intelligence. But where appropriate, we will also refer to and discuss topics about the superset of human-agent interaction. 

We distinguish between domain expert and end user by considering a person with sufficient experience and authority in their field domain expert, while an end-users represents the customers who buy and use a product available on the market. While the end-user has a background in the respective field, he can not provide guidance on designing such a tool.

The paper is structured as follows: After the introduction and motivation, we provide in Section~\ref{sec:background} an overview of background work for explainability and interactive learning. We furthermore review fundamental and current challenges for embodied intelligence. Then, we explore the four phases for the deployment of HITL RL systems and analyze where to apply explainability in HITL RL:
\begin{enumerate}
	\item Development (Section~\ref{sec:Developing})
	\item Agent Learning (Section~\ref{sec:AgentLearning})
	\item Evaluation (Section~\ref{sec:Evaluation})
	\item Deployment (Section~\ref{sec:Deployment})
\end{enumerate}

Each of the phases has specific requirements for the success of human involvement. Section~\ref{sec:discussion} discusses the general challenges observed and how future solutions can be shaped to overcome them. Finally, Section~\ref{sec:conclusion} outlines current problems and goals in the field and proposes future work.

\begin{table*}[!htbp]
	\tiny
	\resizebox{1\linewidth}{!}{
		\begin{tabular}{|c||l|l|l|l|}
			\cline{1-5}
			\multicolumn{1}{|c||}{ }  & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{ }\\
			\multicolumn{1}{|c||}{\textsc{Phase}}  & \multicolumn{1}{c|}{\textsc{Explanation Requirements}} & \multicolumn{1}{c|}{\textsc{Human Involvement }} & \multicolumn{1}{c|}{\textsc{Explanation} } & \multicolumn{1}{c|}{\textsc{Long-term Research Directions}}\\
			\multicolumn{1}{|c||}{ }  & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{ }\\
			\cline{1-5}
			\multirow{14}{*}{\rotatebox{90}{\hspace{0em}\textsc{Development}}} 
			&  &  & & \\
			& $\cdot$Comparable to other model&$\cdot$Define problem &\textbf{Techniques:} &	$\cdot$Create interactive, thorough, and\\
			& $\cdot$versions&$\cdot$Construct state space, &$\cdot$Structural models: &$ $ comparable model summaries\\
			& $\cdot$Fast and shallow for&$ $ action space, and reward &$ $ relational,graphical,decision tree, &$\cdot$Use compositional, \\
			& $ $ superficial inspection&	$ $ function  	&	 $ $ structural causal model	&	$\cdot$ Further integrate causal learning	\\
			& $\cdot$Slow but exhaustive for  &	$\cdot$Design model	&	 $\cdot$Structural representations: 	&	$ $ into HITL approaches	\\
			& $ $ deep inspection &	&	 $ $ symbolic, program, first-order logic	&	\\
			&&	 	&$\cdot$Post-hoc explanations:	  	& 	\\
			&&	 	&	 $ $ Counterfactuals	&	\\
			&&	 	&	 \textbf{Directionality:}	&	\\
			&&	 	&	 $\cdot$Unidirectional (Model to User)	&	\\
			&&	 	&	 \textbf{Users:}	&	\\		
			&&	 	&	 $\cdot$RL experts	&	\\
			&  &  & & \\
			\cline{1-5}
			\multirow{13}{*}{\rotatebox{90}{\hspace{0em}\textsc{Agent Learning}}} 
			&  &  & & \\
			& $\cdot$Informative for human  &	 $\cdot$Give evaluative feedback	&	 \textbf{Techniques:} 	&	$\cdot$Make use of imitation learning	\\
			& $ $ trainers&	$\cdot$Deliver action-advice	&	 $\cdot$Structural representations: human	&	$ $ and preference-based learning as 	\\
			& $\cdot$Understandable by domain &	 $\cdot$Input preferences 	&	 $ $ preferences querying 	&	$ $ complementary approaches	\\
			& $ $ experts&	 $\cdot$Provide demonstrations 	&	$\cdot$Visualization: saliency map  	&	$\cdot$Adapt xAI approaches to HITL\\
			&&&$\cdot$Post-hoc explanations: & $ $ context\\
			&&&$ $ counterfactuals & $\cdot$Fully support and integrate \\
			&&&\textbf{Directionality:} & $ $ human-as-teacher approach \\
			&  & 	&	 $\cdot$Bidirectional 	&$\cdot$Find hybrid methods of different\\
			&   & 	&	 \textbf{Users:}	&	$ $ kinds of human interaction	\\
			&   & 	&	 $\cdot$Domain experts	&	$ $	\\
			&   & 	&	 $\cdot$RL experts	&	\\
			&  &  & & \\
			\cline{1-5}
			\multirow{14}{*}{\rotatebox{90}{\hspace{0em}\textsc{Evaluation}}}
			&  &  & & \\
			& $\cdot$Summary learned behavior  & $\cdot$Understand and evaluate	&	 \textbf{Techniques:}	& $\cdot$Ensure understandability for 	\\
			&  $\cdot$Scalable to large models & $ $ learned policies on micro	&	 $\cdot$Structural models: graphical, 	&$ $ domain expert	\\
			& $\cdot$Comparable to untrained   & $ $ and macro-level	&	 $ $ decision trees, causal models	&	$\cdot$Enable thorough and \\
			& $ $ models   & $\cdot$Decide the next phase	&	 $\cdot$Structural representations:&	$ $ comparable explanations that  \\
			& $\cdot$Understandable by domain & & $ $ logic rules, programs& $ $ scale with model size and \\
			& $ $ experts & & $\cdot$Policy Summarization: rules, & $ $ complexity \\
			&&& $ $ code blocks& $\cdot$Further develop dashboards for \\
			&   & 	&	 \textbf{Directionality:}	& $ $ policy inspection from different 	\\
			&   & 	&	 $\cdot$Bidirectional	& $ $ viewpoints	\\
			&    & 	&	 \textbf{Users:}	& 	\\
			&   & 	&	 $\cdot$Domain experts	&	\\
			&  $ $   & 	&	 $\cdot$RL experts	& 	\\
			&  &  & & \\
			\cline{1-5}
			\multirow{12}{*}{\rotatebox{90}{\hspace{0em}\textsc{Deployment}}}
			&  &  & & \\
			&  $\cdot$Fast, clear, and concise to& $\cdot$Deploy agent	&\textbf{Techniques:}	&	$\cdot$Develop and apply new \\
			&  $ $  reduce cognitive load& $\cdot$Use agent’s decisions	&	 $\cdot$Visualization: saliency map,	&$ $ approaches beyond image and 	\\
			&  $\cdot$Understandable by & $\cdot$Interact with agents	&	 $ $ dendogram, bounding-box	&	$\ $ driving-based explanations\\
			& $ $ end-users	 & $\cdot$Define agents' real-world 	& $\cdot$Textual explanation	 & $\cdot$Use simple and fast explanations	\\
			& $\cdot$Hideable to prevent  & $ $ application goal and	&	 $\cdot$Intent Communication:	& $ $ uncertainty handling\\
			& $ $ detrimental effects on user & $ $ context& $ $ object saliency + textual& $ $ uncertainty handling\\
			& $ $ performance & 	&	\textbf{Directionality:}	&$\cdot$Communicate agent intent via 	\\
			&   & 	&	 $\cdot$Bidirectional	& $ $ different modalities	\\
			&   & 	&\textbf{Users:}	& $\cdot$Use explanations to build trust	\\
			&   & 	&	 $\cdot$End-users	&	\\
			&  &  & & \\
			\cline{1-5}
		\end{tabular}
	}
	\caption{Overview of the different explanations contexts in the four different phases. \emph{Explanation Requirements} enumerates desirable properties of explanations at this stage, and \emph{Human Involvement} describes how the human is involved in it. The \emph{Explainability} column lists (1) example techniques currently used, (2) directionality of explanations, i.e., agent to human, human to agent, or both, and (3) the types of users interacting with the agent at this stage. The \emph{Third Generation Challenges} describes blue-sky propositions we envision for a comprehensive HITL RL experience. See the section \ref{sec:Developing} for Agent Development, \ref{sec:AgentLearning} for Agent Learning, \ref{sec:Evaluation} for Agent Evaluation and \ref{sec:Deployment} for Agent Deployment.}
	\label{table:Explanations_table}
\end{table*}

%Remove? It has been mentioned in the abstract, but I'm not sure if that's enough - does it do anything for us?
We created  Table \ref{table:Explanations_table} to give the reader a comprehensive overview of the main insights of our paper. It provides a synopsis of the central aspects of this paper, and subsumes the requirements, challenges and human context for the four phases we discuss extensively in the following pages. The row corresponding to the agent deployment phase is shown in each section to 

In this paper we argue that RL greatly benefits from being thought of as a human-centered process and that explainability is required to enable this HITL RL approach. We highlight how current xAI methods can be used to facilitate such HITL approaches, and identify 
research gaps for further explainability research, ultimately enabling a more productive interaction of humans and RL agents.

% ==================
\section{Background}
\label{sec:background}

In the background, we establish the technical backdrop for our discussion of the four phases of the deployment of HITL RL agents. We therefore detail how to provide insights into the workings of ML models with the help of explainability approaches. We then explain how interactive learning allows to integrate the human-in-the-loop into RL. Finally, we summarize current challenges for reinforcement learning in general and more specific to the HITL context.

\subsection{Explainability}

Explainable artificial intelligence (xAI) is a framework for helping human users understand the process and outputs of machine learning models. As ML models are deployed in a growing number of applications that affect human life (e.g., agriculture, forestry, health,etc.), the need for such xAI frameworks is ever more apparent. The xAI approaches are essential for many human-AI collaboration scenarios, where understanding and trusting the outputs of the model is a prerequisite for their use.  

Explainability has grown from the 1990's with researchers aiming to extract rules from neural networks \citep{TickleEtAl:1998:HistoryNN}, towards a dedicated and expanding research community, as seen at the example of the DARPA initiative \citep{GunningAha:2019:DARPA}. The xAI efforts have since led to a number of very successful xAI methods \citep{HolzWoj:2022:XAIOverview, ZhouEtAl:2021:QualitySurvey}. Explainability in this context is a technical term used to refer to the collection of methods that aim to highlight decision-relevant parts of machine representations and machine models.

The following examples show how xAI frameworks and human users can interact:
\begin{enumerate}
\item Explaining the role of a data source in the final decision, for example to identify which data samples were used for a specific action or decision. This is also important for assigning credit to (and potentially compensating) the individuals who produced the data~\citep{zanzotto2019human}. 
\item Identifying and visualizing how components of the model contributed to given predictions during training via a heatmap \citep{SturmEtAl:2015:InteractiveHeatmap}.
\item Building trust in the human users specifically when safety is a major issue. In AI applications in medicine for example, the human user needs a reliable explanation for the decision made by the AI agent. Transparency and accountability are essential ~\citep{Schneeberger:2020:legalAI, Stoeger:2021:MedicalAI}. Other applications such as autonomous vehicles or robotics are additional examples of cases where the decisions have a significant impact on safety and therefore trust and accountability are pertinent~\citep{araiza2019safe, WellsBednarz:2021:xAIRLSurvey}.
\item Enabling humans to provide richer feedback through additional counterfactual examples. Feedback in the form of explanations provided by humans is used by the AI agents towards a two-way cooperation, which leads to more accurate, robust, and transparent models ~\citep{Karalus:2021:HITL-counterfactuals,PuiuttaVeith:2020:xAIRLSurvey}.
\end{enumerate}

These examples show the diverse applications for xAI frameworks. \citet{LiangEtAl:2017:HITLReinforcementLearn} argue that the cognitive ability of the human operator paired with the computational power of a machine have the potential to handle complex tasks. However, they state that it is essential for the machine as well as the human operator to be able to react to the environment to enable a productive interaction. Adjacently, humans need to understand and interpret the actions of the machine correctly to improve the algorithm. Moreover, the underlying algorithm and its decisions need to be understandable for a multitude of different audiences with various goals \citep{heuillet2021explainability}, which shows the importance of explainability in the context of human machine cooperation.

As categorization of explainability approaches, interpretability can be considered a subset of explainability, and is defined as the methods which passively make the model understandable, whereas explainability actively generates explanations for a model. We use the categorization of interpretable RL methods as presented by \citet{GlanoisEtAl:2021:SurveyInterpretableRL}, but extend their categorization of interpretability approaches to include explainability models as well. The three major categories are: 
\begin{enumerate}
    \item interpretable/explainable inputs of RL models
    \item interpretable/explainable transition and reward models for RL
    \item interpretable/explainable decision-making processes of RL
\end{enumerate}

The first category focuses on the inputs to the RL model used to make decisions. It not only includes the agent's state, but also other structural information, such as the problem descriptions from human experts \citep{hasanbeig2021deepsynth}, and the relational \citep{battaglia2018relational,martinez2017relational} or hierarchical structure \citep{andreas2017modular,lyu2019sdrl} of the problem. Such information helps humans better understand the decisions made by RL models. 
%Visualization
An important building block for explaining model inputs is to visualize them as perceived by the model. This is often combined with showing relevance and importance for a given decision. This helps to evaluate whether the model is looking at the right aspects of the input, but can also mislead the user if used incorrectly. See \citet{EvansEtAl:2021:ExplainabilityParadox} for further discussion of this problem complex.
Saliency maps are such an example and highlight important image regions. \citet{LiuEtAl:2018:LinearModelUTrees} show an example where continuous ``super-pixels'' with large feature influence are highlighted. \citet{Bach:2015:LayerWiseRelevancePropagation} develop the technique of ``Layer-Wise Relevance Propagation'' to iteratively change the model inputs to find the relative importance of individual (image) parts or features.

The second category of explainable transition- and reward models leverages understable models of the task or environment, e.g., a transition model \citep{martinez2016learning,zhu2020object} or a preference model \citep{icarte2018using,toro2019learning}. Such models help explain both the RL agent's reasoning about its decision-making and humans' understandings of the decision-making process. Many of these approaches either learn a probabilistic model to represent actions \citep{walsh2010efficient} or rules \citep{Walker} or work deterministically. Deterministic models can for example create object-oriented representations \citep{NEURIPS2018_df0aab05} or apply search algorithms to the state-space graphs to learn high-level policies \citep{pmlr-v80-zhang18k}, \citep{NEURIPS2019_5c48ff18}.

The third category is interpretable/explainable decision-making of RL agents, representing cases where policies can be learned in a intuitively understandable manner. Some approaches learn such interpretable policies in the form of decision trees \citep{likmeta2020combining,silva2020optimization,topin2021iterative}, formulas \citep{hein2018interpretable,hein2019generating}, fuzzy rules \citep{akrour2019towards,hein2017particle,zhang2021kogun}, logic rules \citep{jiang2019neural}, or programs \citep{sun2019program,verma2019imitation}. Other researchers first learn a non-interpretable policy, then transform this policy to an interpretable one through imitation learning or transfer learning \citep{bastani2018verifiable,VermaEtAl:2018:ProgrammaticallyInterpretableRL}.

Generally, it is challenging to reliably assess the quality and efficivity of xAI solutions since user cost is often hard to objectively measure \citep{bruneau2002eyes}. To better quantify this user cost, \cite{milani2022survey} name four key metrics for evaluating xAI solutions, which we will also use for our evaluation of different classes of xAI solutions: \emph{Fidelity}, the truthfulness of the explanation with regard to the model itself. \emph{Performance}, the default metric used to evaluate the success of the AI solution to be explained. \emph{Relevancy}, the relevance of the provided explanations to the task at hand, and \emph{Cognitive load}, the mental effort required to understand the provided explanations. 

A subset of xAI methods focuses on explaining decisions made by the models as parameterized by their learned policies. These models are therefore specific to RL. The following approaches of policy summarization and querying are examples for xAI techniques specifically related to explaining policies, and therefore belong to the third category of explainable decision making. 

%Policy Summarization
In policy summarization, the general aim is to make the underlying model and its policy tangible. This can be done via codifying its decision process as rules, as seen in the linear model u-trees by \citet{LiuEtAl:2018:LinearModelUTrees}. These networks aim to represent the Q-function and use it to make more transparent the feature influence and the rules learned by the network. 
Another approach is to represent the learned model with generated code blocks, as presented by \citet{VermaEtAl:2018:ProgrammaticallyInterpretableRL}. The `Programmatically-Interpretable RL'' first learns a neural policy network and then searches a programmatic policy that adequately codifies this network. This approach incurs a performance degradation during training, but results in human-readable policies and improves generalization. 

%Policy Querying

In policy querying, the decision process leading to a given result is explained. This can be either general (``when do you do X'') or or specific to a given action.
An example for a specific explanation is a natural language explanation for a classification in the ML space \citep{AlonsoEtAl:2018:xAINLBeerClassifier}, where they provide a textual explanation along with additional details that show how individual factors where quantified and evaluated. An example for policy querying is presented by \citet{HayesShah:2017:AutonomousPolicyExplanation}, which generate a summary of a ``when do you do X?'' type question in natural language to explain the actions of an agent. \\

% Causal Learning subsection

A fundamentally different approach to explainability is provided by causal models. Methods available within this framework, generally fall into the second and third categories referred to above (explainable transition and reward models and decision making), as they provide causal explanations for the task, environment or the policies themselves. 

% GNN
One causal approach are graph neural networks (GNN) \citep{Vu:2020:PGMExplainer}, which can generate explanations for a prediction via a probabilistic graphical model (PGM) that identifies crucial graph elements (e.g., nodes, edges) causally responsible for that prediction. Current work also incorporates the human-in-the-loop strategy \citep{HolzingerEtAl:2016:iMLExperiment, Holzinger:2019:HumanLoopAPIN} where expert domain knowledge is used to select actions. 

% Use opportunity chains as modality for good explanations
Along similar lines, \citet{MadumalEtAl:2020:CausalRLCFs} encode causal models using \emph{action influence graphs} to generate explanations using \emph{causal chains} resulting in better explanations as well as improved prediction performance. In their more recent work, \citet{Madumal:2020:DistalEF} propose that investigating interactions between RL agents and humans is the key to generating finer details in the explanations. An insight gained through their human studies was that to generate explanations, humans tend to refer to \emph{future} actions that were dependent on the current ones. This supports the idea that humans have a deep understanding of cause and effect chains of actions and events, often referred to as \emph{opportunity chains} in the cognitive psychology literature. Inspired by this, the authors create explanatory models that focus on these opportunity chains and the future actions.

% Counterfactuals
With the help of adequate personalized User Interfaces (UI) \citep{Sun:2021:TopologyPerturbationGNNs}, counterfactuals can also be created by following an action sequence driven by human expertise. This can also be injected in the form of the priors of the random variables and consists of a form of inductive bias.

% Not just use human explanations as prior, but use expertise to imitate
Observational data from expert actions go beyond injecting a prior to an explainable model \citep{Zhang:2020:CausalImitationLearning}. Causal imitation learning learns a structural causal model (SCM) \citep{Pearl:2000:ModelsReasoningInference} from policies performed by humans, even if the actual reward is not specified and the environment is not perceived the same to the learner and the human expert demonstrator. 

Dynamic SCMs are incorporated to formalize the partially-observable markov decision process (POMDPs) \citep{SuttonBarto:2018:RLIntroduction} as perceived by the agent, and take into account the human intervention and its implications. The so-called counterfactual agent does not blindly take the human's advice and execute it, but rather compares it with other possible actions and decides correspondingly. In cases where the reward and the transition functions are the same, the ``human-in-the-loop'' is beneficial, even if the human's instructions are suboptimal.

\subsection{Interactive Learning}
\label{sec:InteractiveLearning}
    
The most fundamental ways of learning in nature is parents teaching their off-springs in an interactive fashion. Similar learning dynamics exist between a teacher and a student, where the teacher tries to guide the student with their experience and knowledge. Following the same motivation, interactive learning~\citep{Arzate:2020:SurveyInteractiveRL} in RL aims to involve human-in-the-loop to guide the RL agent by leveraging domain knowledge and rich human experience. RL has been successfully applied to solve many real-world problems, ranging from drug discovery~\citep{popova2018deep}, navigating super pressure balloons in the stratosphere~\citep{bellemare2020autonomous} to robot manipulation tasks~\citep{nguyen2019review}. While this is an exciting direction for research, recent deep RL systems still face a lot of bottlenecks, including sample inefficiency, sim-to-real transfer issues, generalization, exploration, etc., to name a few~(see Subsection~\ref{sec:ChallengesRL} for further discussion)\citep{ibarz2021train}. Interactive RL aims to solve some of these challenges by involving a human prior to training~\citep{Guo:2022:RLSurveyHumanPriorKnowledge}, during training~\citep{Knox:2008:TAMER} or in the deployment phase of the RL system~\citep{guo2021edge}. Interactions could either be teacher-initiated~\citep{torrey2013teaching}, student-initiated~\citep{da2020uncertainty,MandelEtAl:2017ActionsInHITL} or jointly initiated~\citep{amir2016interactive} by both the parties.

In interactive learning, the human is characterized as a teacher and the teaching loop can contain different types of critique, advice modalities and guidance that can be fed back to the RL algorithm. There can be different modalities of human advice such as binary evaluative feedback [+1/-1] ~\citep{Knox:2008:TAMER}, action-advice~\citep{torrey2013teaching}, preference based feedback~\citep{Christiano:2017:DeepRLHumanPreferences,LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE}, and sub-goal specification~\citep{le2018hierarchical}. A comprehensive survey of various types of human guidance in Deep RL can be found in \citet{zhang2019leveraging}. \citet{Thomaz:2006:RLWithHumanTeachers} proposed one of the earliest work on Interactive RL which allowed human trainers to give binary feedback for the agent's behavior and specific objects associated with the task.

A common approach to modify the reward function is called reward shaping. In reward shaping, the teacher provides useful information to shape the reward function so that favourable parts of the state-space are encouraged and unfavourable parts are penalized \citep{ng:99}. This is useful in sparse reward environments and facilitates the reward specification in complex domains. A well-known reward shaping framework is TAMER~\citep{Knox:2008:TAMER, knox:13} where the human provides evaluative reinforcement (positive or negative feedback) by observing the agent in action, and using that signal to build a model of the human's reward function. 

Methods that consider modifying the agent's policy are called policy shaping \citep{cederborg2015policy,griffith2013policy,WuEtAl:2021:HITLDRLAutonomousDriving}. These methods augment an agent's policy directly using human knowledge. This technique does not require a well formulated reward function, but it assumes the trainer knows a near-optimal policy to guide the agent. 

Human advice can also be useful in guiding the agent in it’s exploration phase so that highly rewarding states or trajectories are identified by the agent in fewer environment interactions~\citep{amir2016interactive}. Action pruning is another way where human-in-the-loop RL can guide exploration and improve learning \citep{Abel:2017:AgentAgnosticHumanInTheLoopRL}.

Lastly, human-advice-based value functions can be combined with agent value functions~\citep{jiang:21,kartoun:10, taylor2011integrating, WuEtAl:2021:HITLDRLAutonomousDriving} to effectively guide the agent. Demonstrations \citep{hester2018deep,vecerik2017leveraging,nair2018overcoming} from humans can also augment the value function by biasing it according to the  actions taken by the expert. These approaches have been particularly successful in complex robotics tasks like pushing, sliding etc which can easily be demonstrated by humans.  Demonstrations, by default, may contain human biases which can, in turn, be removed by experts \citep{Wang:2022:SkillPreferences}.

The first indicator for the need for human intervention is model performance, since a bad model is more likely to produce bad policies. However, good models can also lead to bad policies as well at deployment scenarios because of the sim-to-real gap. In many cases, human intervention is simulated from pseudo-agents since both the potential benefits and the need to know where the imperfections of the model are can be detected before deployment. The designer of the interactive framework must also take into account that the human interventions might not always be perfect or beneficial; the user might need special training and an informative user interface to effectively improve the RL algorithm. 
%merged the UI paragraph from the Interactive learning section here

The type of UI, hardware-delivered or natural interaction,  determines the degree of expertise required and can affect the quality of the feedback \citep{lin:20}. Keyboard keys, mouse clicks with sliders, and game controllers are examples of UIs in hardware-delivered interactions, and experts or knowledgeable trainers generally use these UIs. On the other hand, sound interfaces that use the techniques of audification and sonification \citep{Hermann:2011:Sonification,kartoun:10, Saranti:2009,Scurto:2021:DesigningDeepRLHumanParameterExploration}, cameras to capture facial expressions \citep{arakawa:18}, etc. are examples of UIs for natural interaction that non-expert users prefer. 

\subsection{Challenges for Reinforcement Learning and HITL approaches} 
\label{sec:ChallengesRL}
%%%%

To conclude the background section, we want to discuss the underlying challenges for reinforcement learning in general and those more specific to HITL approaches in order to get a better understanding of fundamental and current challenges in the field.

%%%Exploration Exploitation Tradeoff
One fundamental challenge is the Exploitation/Exploration trade-off. The trade-off is defined by the decision when to continue exploiting a current option, and when to explore further for new options. This challenge is most often illustrated at the example of the one-armed bandit. An agent is tasked with finding the best (ie. most rewarding) one-armed bandit slot machine in a casino, and has a limited amount of coins. When the agent is now sitting at machine A, how long does he take to evaluate whether this machine provides a higher payoff, and when does he cut his losses to move on to another automaton? A more in-depth description of this problem can be found in \citet{AudibertMunosSzepesv:2009:ExplorationExploitation}.

%%%Reality Gap
Another general challenge is the ``sim-to-real gap'', which describes the difficult task of translating the experience from a simulation to an applications reality \citep{ZagalJavierVallejos:2004:RealityGap}. It first states that we are always under-modelling our system, which means that aspects of reality are missing which will present our agent with unforeseen challenges and sometime even prevents a policy learned in a simulation from being transferable to the real world. Secondly, real-world samples are very expensive (cost, complexity and time-wise), which makes modelling despite its challenges much more appealing  \citep{KoberBagnellPeters:2013:RLRoboticsSurvey}.

%%% Pixel based learning is hard and leads to brittle assumptions
In real-world application, vision is often central modality for agents. Therefore, learning from images and videos is essential. This is however a very hard problem, where the solutions often rely on brittle assumptions and in consequence do not generalize well \citep{TomarEtAl:2021:LearnPixelControlRepresentations}. 

Also refer to \citet{ibarz2021train} for their survey of RL applications and some of the main challenges they identified, namely sample efficiency, sim-to-real-gap, exploration challenges, generalization challenges, goal and reward shaping, and safety issues. 

More specific to reinforcement learning for embodied intelligence, \citet{RoyEtAl:2021:RLRoboticsChallenges} provide a comprehensive overview of different challenges. To highlight five particular constraints:
\begin{itemize}
    \item Interaction with real world entails safety risks for exploration and hard limits on resources like energy.
    \item Poor alignment of learned models and real world.
    \item Require stronger generalizations  and adaptation since specifications, goals and rewards might change.
    \item Observed data is plentiful but drawn from local distribution, requiring agent to learn a reasonable world model beyond  what is currently observed. 
    \item Agent morphology defines what can be learned from the environment and has to be considered when designing agents.
\end{itemize}

Finally, there are challenges specific to HITL approaches. One major question in this context is to what extent should RL imitate a good human player or expert, or when and how RL can be used to surpass the performance of the expert \citep{Abel:2017:AgentAgnosticHumanInTheLoopRL}. This requires the HITL RL agent to discover new action sequences, not yet thought of by humans. In addition with the requirements to enable generalization, a reward function can not be overly specific and complex (\citep{LiuAbbeel:2020:UnsupervisedActivePreTraining}). This challenge of finding the sweet spot between human intervention and the agents ability of exploration is another extension to the exploration-exploitation tradeoff described above. 

With regard to challenges in the field of explainability, \citet{GlanoisEtAl:2021:SurveyInterpretableRL} state that explanations are not reliable and do not make sense when the neural network is not yet fully trained, and therefore does not exhibit cohesive behavior which could be explained. This can be due overall bad performance, lack of generalization capability, misclassified examples and other underlying errors. Developers will therefore also need to consider at which point in development the application of which explanation method is appropriate and able to provide insights into the model. \citet{GlanoisEtAl:2021:SurveyInterpretableRL} further name as open challenges the problem complexes of scalability, performance as well as achieving full interpretability in general with RL xAI methods.

Another challenge in explainability is that most of the current xAI methods invented for deep neural networks are not created with RL principles in mind. They are typically developed with the intent of uncovering a simpler, interpretable model or to pinpoint the important elements of a potential input, driven by the mathematical principles of neural networks. In the example of the convolutional neural network (CNN) that was used to process the Atari images \citep{Mnih:2013:PlayingAtariDeepRL}, layer-wise relevance propagation (LRP) could be used \citep{Bach:2015:LayerWiseRelevancePropagation,Alber:2019:Innvestigate}. However, this would only provide to the user a heatmap about what is positive and what is negatively relevant for the prediction, meaning that it would only characterize (in RL terms) one input state. Those heatmaps are not juxtaposed or combined with the possible actions from that state, or their expected reward as a whole ---the human would not know why the RL algorithm decided for the selected next action. To reconstruct the complete strategy of a model, its rules and the underlying purposes of all (or at least the representative) state-action pairs out of those heatmaps, would be a very cumbersome task. 

We argue that the discussed challenges show that generalisable and performant RL is a fundamentally challenging problem, as exemplified by the different obstacles detailed in this Section. We claim that many of those challenges can be overcome with the application of HITL approaches, supported by \citet{MatthewsonPilarski:2022:DesigningAndEvaluatingHCIML} which argue that human-centered interactive approaches are essential for designing and deployment of all machine learning systems. While \citet{MatthewsonPilarski:2022:DesigningAndEvaluatingHCIML} take the bird-eye perspective on all machine learning systems as interactive machine learning and formulate an overall guideline for human-centered design of ML systems, we focus specifically on the design, evaluation and deployment of interactive HITL RL systems and associated short-term and long-term challenges.

We furthermore argue that xAI approaches are fundamental for the success of HITL approaches, which is shared by other researchers \citep{heuillet2021explainability,milani2022survey}. In each of the four deployment phases we outlined, xAI is essential. In the following Sections, we want to show where xAI can be applied in the deployment of HITL RL agents, which solutions exist and how they might be adapted to allow for better HITL interaction.

Now that we have summarized the background on Explainability, Interactive Learning, and the challenges in HITL RL, we will delve deeper into the four stages of deployment, starting in Section~\ref{sec:Developing}, discussing how xAI techniques could be applied to the initial phase of HITL RL development. In Section~\ref{sec:AgentLearning}, we focus on the Agent Learning phase, followed by the discussion of the subsequent Evaluation and Deployment of these systems, in Sections~\ref{sec:Evaluation} and \ref{sec:Deployment}, respectively.

% =====================

\section{Initial Agent Development}
\label{sec:Developing}

	\begin{table*}[!htbp]
		\tiny
		\resizebox{1\linewidth}{!}{
			\begin{tabular}{|c||l|l|l|l|}
				\cline{1-5}
				\multicolumn{1}{|c||}{ }  & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{ }\\
				\multicolumn{1}{|c||}{\textsc{Phase}}  & \multicolumn{1}{c|}{\textsc{Explanation Requirements}} & \multicolumn{1}{c|}{\textsc{Human Involvement }} & \multicolumn{1}{c|}{\textsc{Explanation} } & \multicolumn{1}{c|}{\textsc{Long-term Research Directions}}\\
				\multicolumn{1}{|c||}{ }  & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{ }\\
				\cline{1-5}
				\multirow{14}{*}{\rotatebox{90}{\hspace{0em}\textsc{Development}}} 
				&  &  & & \\
			& $\cdot$Comparable to other model&$\cdot$Define problem &\textbf{Techniques:} &	$\cdot$Create interactive, thorough, and\\
			& $\cdot$versions&$\cdot$Construct state space, &$\cdot$Structural models: &$ $ comparable model summaries\\
			& $\cdot$Fast and shallow for&$ $ action space, and reward &$ $ relational,graphical,decision tree, &$\cdot$Use compositional, \\
			& $ $ superficial inspection&	$ $ function  	&	 $ $ structural causal model	&	$\cdot$ Further integrate causal learning	\\
			& $\cdot$Slow but exhaustive for  &	$\cdot$Design model	&	 $\cdot$Structural representations: 	&	$ $ into HITL approaches	\\
			& $ $ deep inspection &	&	 $ $ symbolic, program, first-order logic	&	\\
			&&	 	&$\cdot$Post-hoc explanations:	  	& 	\\
			&&	 	&	 $ $ Counterfactuals	&	\\
			&&	 	&	 \textbf{Directionality:}	&	\\
			&&	 	&	 $\cdot$Unidirectional (Model to User)	&	\\
			&&	 	&	 \textbf{Users:}	&	\\		
			&&	 	&	 $\cdot$RL experts	&	\\
			&  &  & & \\
			\cline{1-5}
			\end{tabular}
		}
		\label{table:Development}
	\end{table*}
	
The first step of a HITL RL model deployment process is developing the underlying model, along with problem formulation and pre-learning considerations. This entails making the model understandable to the software developers and AI experts, who want detailed insight into their model. 
% Motivation for XAI consideration in the Development process
Integrating considerations for explainability into this development process of RL systems is strongly recommended, since it allows the AI experts to start off in the right direction, instead of having to make costly (post-hoc) changes later on in the RL life-cycle. Decisions on the groundwork made now have large implications for the overall RL life-cycle and are likely to be hard to change later on.

% Benefits - solid groundwork, avoid common errors, generate baseline
At this stage, working with an understandable model can help to ensure the model is based on solid assumptions and comes to consistent and sound conclusions. The numerous error sources pertaining to training data, model initialization, and the initial learning process can be monitored and limited in this phase. Ensuring proper model function and architecture at this step also ensures a proper baseline for comparing the trained model against.

For this phase, especially challenges like exploration-exploitation trade-off and sim-to-real gap are relevant (see \ref{sec:ChallengesRL}). These determine the selection of learning algorithm and parameters, which also defines which type of explanation is applicable to the model.

\subsection{Requirements}
% Considerations

We propose primary considerations for the development stage. In the beginning, the generated explanations should be comparable to those of other model versions in order to track the progress of development. 


Further, we identify two approaches for an explainable development process. The first is a broader and more superficial evaluation of the general model behavior, which allows for a high-level inspection of the model behavior. The second is an in-depth assessment that provides a more detailed and complex model view.

In the first case, explanations must be computed fast to enable a feedback loop during training. \citet{XinEtAl:2018:HITLMLFeedbackLoop} explore the implications of a quicker HITL feedback loop. The authors highlight aspects like introspection, the ability to rapidly analyze and compare the impact of changes to reuse intermediate results, and an easier end-to-end optimization by quicker feedback. With the metrics of \citet{milani2022survey}, we therefore  propose high demands for computational performance and a reduced cognitive load.


In the second case, explanations require more time to compute, understand, and interpret correctly. Therefore, people can use these explanations to analyze a small number of model snapshots.
Both approaches should complement each other since a thorough assessment of the model behavior requires both depth and breadth.

% removed requirements
Complex and detailed explanations is appropriate for the model developers, since they care more about detailed insights than easy understandability and can afford the required cognitive load. Additionally, at the development stage, computational resources are the least constrained, and the scalability of explanations is not essential either since the model is not yet fully trained and with that relies on less parameters. This results in lower demands for computational performance and less cognitive load but high fidelity as per \citet{milani2022survey}.

We identify different approaches that can help during this phase. First, pre-training can help with the groundwork of intelligent behavior and enable sensible debugging. Second, interpretability approaches should be considered at this step, since inherent understandability implemented at this step will also benefit all succeeding phases. Finally, we argue which types of explainability approaches are suitable for this phase.

\subsection{Pre-Training}

Pre-Training models are beneficial in the RL training workflow since they prepare the groundwork for the Human-Robot interaction and benefits HITL in several ways. It helps to 
develop useful priors, diverse behaviors, generalized policies/feedback, and efficient initial feedback by the human~\citep{daniel2016hierarchical,eysenbach2018diversity,florensa2017stochastic,hazan2019provably,LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE}.

% Preference-based Learning
Preference-based learning is one example for pre-training, in which a robot gives lets a human decide which is the preferred option of two possibilities (such as movement policies). This choice simplifies the difficult reward-selection process, avoiding to having to explicitly define the reward function. It is advantageous for this approach if the robot already exhibits two ''meaningful'' movement policies, rather than the normal frenetic behavior found in newly instantiated models.
Judging an already trained model is generally easier because it has progressed past the initial noise of random initialization and hopefully shows meaningful behavior. \citet{LeeSmithAbbeel:2021:FeedbackPreferenceHITLLearningPEBBLE} gives an example for the application of pre-training and HITL feedback by applying their PEBBLE framework, and show how pre-training can be used to increase the sample/request efficiency.

% lifelong learning
Generally, approaches like transfer learning and lifelong learning for RL agents are promising, since they also alleviate the problem of the noisy warm-up phase of RL \citep{taylor2009transfer,yang2021efficient}.

\subsection{Interpretability}
\label{subsec:interpretability}
% Combination with Pretraining allows to judge  ------------------------------------------------------

A central approach to imbuing agents with explainability (in the broader sense) is to make them interpretable ---that is, providing an inherently understandable AI solution. Interpretable models in combination with pre-trained or otherwise initialized systems facilitate the judgment of the model and its decision making.

%s Structural biases+ core knowledge+ innate reasoning = transparent models and decisions -------------
\citet{RoyEtAl:2021:RLRoboticsChallenges} enumerate several approaches that would translate to more interpretable models. For one, embedding core knowledge into models (like physical constraints) could provide agents with innate reasoning capabilities, which would then facilitate checking this reasoning \citep{HaSchmidhuber:2018:CoreKnowledgeWorldModels}. A second aspect is the use of compositional language, which could facilitate a high-level understanding of the concepts the model learned. \citet{Koditschek:2021:RoboticsCompositionalLanguage} suggests that the use of model composition and with it compositional language are key elements for embodied intelligence.

% Representative languages ----------------------------------------------------------------------------
Additionally, a representative language could allow abstractive reasoning with rigorous generalization. This can be achieved by graph neural networks, natural language, and attention mechanisms in combination with sys1/sys2 separation \citep{RoyEtAl:2021:RLRoboticsChallenges}, and further with the inherent understandability of the learned model.
% Benefit of this at the example of adversarial images -----------------------------------------------
The advantage of a combination of innate reasoning with understandable language could allow an intuitive understanding of the model. This advantage can be best seen in comparison with the large challenge of adversarial attacks, for example, on image recognition approaches. Here, a core problem lies that the learned (and often highly performing) models focus on very different aspects than we do, and ``understand'' images on a fundamentally different level \citep{ChakrabortyEtAl:2021:SurveyAdversarialAttacks}. This of course in turn prevents humans from understanding the model and its decision-making process without the help of other tools.

\subsection{Explainability}

Some explanations for developing RL models during the development phase are necessary. A lot of contextual information should be taken into consideration when defining what constitutes a ``good” explanation for an RL model, e.g., the background knowledge and levels of expertise of the addressee of this explanation, their needs, and expectations. There are various types of explanations, like visual \citep{DBLP:journals/corr/abs-1912-12191,DBLP:journals/corr/abs-1912-05743}, textual \citep{fukuchi2017autonomous,HayesShah:2017:AutonomousPolicyExplanation}, causal \citep{MadumalEtAl:2020:CausalRLCFs,Madumal:2020:DistalEF}, or decision tree explanations \citep{bastani2018verifiable}. The approach by \citet{LiuEtAl:2018:LinearModelUTrees} codifies the decision process as rules to make the feature influence and rules learned by the network more transparent. Another approach is to represent the learned DRL model with generated code blocks, as presented by \citet{VermaEtAl:2018:ProgrammaticallyInterpretableRL}. Here, a policy network is codified by learning a neural policy network and searching it for the optimal policy. This results in human-readable policies and improves generalization, but also incurs a performance hit during training. A third approach is to represent the network policies via natural language, such as \citet{AlonsoEtAl:2018:xAINLBeerClassifier}, who show an example of justifying classifications with a textual explanation of the choice made by a decision tree.

%Policy Querying
Furthermore, the subset of policy querying approaches that allow looking into questions like ``when do you do X'' can be used for this phase.
\citet{HayesShah:2017:AutonomousPolicyExplanation} provide an example of such questions, and generate a summary of a ``when do you do X?'' type question in natural language to explain the actions of an agent. An important addition to this approach is the use of counterfactuals (see \citet{EvansEtAl:2021:ExplainabilityParadox} for a more thorough assessment of the importance of counterfactuals). \citet{MadumalEtAl:2020:CausalRLCFs} learn a structural causal model for RL agents, which is in turn used to generate explanations of taken actions (see Subsection~\ref{sec:InteractiveLearning}). This approach also allows to respond to queries of counterfactuals, like: ``why did you not do Y?'', which is shown by the authors to produce satisfactory explanations and with those increases user trust.

%Causal Approaches

Finally, approaches in causal learning can help with understanding the underlying model and going beyond interpretability towards explainability.
An example of such xAI methods are probabilistic graphical models (PGM), which help construct causal models and are often applied to graph neural networks (GNN) \citep{Saranti:2019:LearningCompetencePGMs}. Graph neural networks are especially suitable for explainability methods in this stage since they allow easier and more direct visualization of critical components. \citet{Vu:2020:PGMExplainer} for example supports the informed creation of a causal model by identifying essential graph components and then generating PGMs approximating that prediction. This can help identify cause and effect in neural networks and determine cause and effect relations.


\subsection{Conclusion}

% Opportunities - think about new ways of interacting ------------------------------------------------

We conclude with a reflection on the opportunities embodied intelligence presents us with. \citet{RoyEtAl:2021:RLRoboticsChallenges} encourage us to think about other forms of sensors, sensor-fusion, and new components to enable new forms of interaction and application areas and ways if learning (refer to Subsection~\ref{sec:ChallengesRL} for a perspective on the respective challenges). Another direction involves human teaching \citep{kulick2013active} or programs \citep{PenkovR19,sun2019program} to guide the agent to learn symbolic structure or representations of the task, which greatly reduces the task complexity. In \citet{kulick2013active} for example, a human teaches a robot symbols to abstract the geometric information of objects, where the robot learns to maximize the information gain about the symbol to be learned in an active learning manner. \citet{PenkovR19} learn symbolic representations by mapping the perceived symbols to output actions following a handcrafted program. In this way, the agents learns transferable symbolic representations which improve its generalization abilities.

\noindent To \textbf{summarize} the development phase, we refer to Table \ref{table:Explanations_table}. The model explanations should be comparable against each other, allowing the user to understand the differences between tested architectures. Explanations should also be either fast to compute and cognitively more shallow, or with extensive coverage, but can then be slower both to compute and understand. Humans are involved at this step for defining the problem, overall model design as well as specification of state space, action space and reward function and evaluation metrics of the agent. Possible xAI approaches for HITL in this phase are focused on making the underlying model more understandable, for example by enabling interpretable model policies by representing them as code, trees or graphs. This can also be done by intrinsic structural representations as code or first-order logic, or structural causal model approaches. Finally, post-hoc explanations like counterfactuals can be useful to gain insights even at this early stage. Causal approaches should also be considered at this stage, in order to help identify cause and effect in this phase and, those further downstream.


In this section, we covered the Development phase, the first of the four stages in creating HITL RL systems. In the next section, we discuss the subsequent step of training the agent in an interactive fashion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Agent Learning}
\label{sec:AgentLearning}

	\begin{table*}[!htbp]
		\tiny
		\resizebox{1\linewidth}{!}{
			\begin{tabular}{|c||l|l|l|l|}
				\cline{1-5}
				\multicolumn{1}{|c||}{ }  & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{ }\\
				\multicolumn{1}{|c||}{\textsc{Phase}}  & \multicolumn{1}{c|}{\textsc{Explanation Requirements}} & \multicolumn{1}{c|}{\textsc{Human Involvement }} & \multicolumn{1}{c|}{\textsc{Explanation} } & \multicolumn{1}{c|}{\textsc{Long-term Research Directions}}\\
				\multicolumn{1}{|c||}{ }  & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{ }\\
				\cline{1-5}
				\multirow{13}{*}{\rotatebox{90}{\hspace{0em}\textsc{Agent Learning}}} 
				&  &  & & \\
				& $\cdot$Informative for human  &	 $\cdot$Give evaluative feedback	&	 \textbf{Techniques:} 	&	$\cdot$Make use of imitation learning	\\
				& $ $ trainers&	$\cdot$Deliver action-advice	&	 $\cdot$Structural representations: human	&	$ $ and preference-based learning as 	\\
				& $\cdot$Understandable by domain &	 $\cdot$Input preferences 	&	 $ $ preferences querying 	&	$ $ complementary approaches	\\
				& $ $ experts&	 $\cdot$Provide demonstrations 	&	$\cdot$Visualization: saliency map  	&	$\cdot$Adapt xAI approaches to HITL\\
				&&&$\cdot$Post-hoc explanations: & $ $ context\\
				&&&$ $ counterfactuals & $\cdot$Fully support and integrate \\
				&&&\textbf{Directionality:} & $ $ human-as-teacher approach \\
				&  & 	&	 $\cdot$Bidirectional 	&$\cdot$Find hybrid methods of different\\
				&   & 	&	 \textbf{Users:}	&	$ $ kinds of human interaction	\\
				&   & 	&	 $\cdot$Domain experts	&	$ $	\\
				&   & 	&	 $\cdot$RL experts	&	\\
				&  &  & & \\
				\cline{1-5}
			\end{tabular}
		}
		\label{table:AgentLearning}
	\end{table*}
	
% has been developed, it is trained by and with a human teacher.

Once developed, the agent is trained either autonomously or with the help of human input. Section~\ref{sec:Developing} discussed current explanation techniques used mainly during the development phase under close supervision of the developer. This section focuses on approaches where people act as a trainer or teacher, guiding the agent learning process.

Finding and determining the appropriate role of HITL should be the first step in developing an optimal mechanism for interaction. We propose that the focus should be on providing interpretable information and clear explanations to the human during this teaching process. Hence, human trainers can understand how agents perceive the world and provide better feedback.

Among the challenges identified in Subsection~\ref{sec:ChallengesRL}, the following are relevant to this phase: (1) determining to what extent agents should imitate or follow human advice, (2) optimizing reward shaping,  (3) choosing explanation methods and complexity according to the background knowledge and expertise of teachers, and (4) reducing the sim-to-real gap. 

\subsection{Requirements}
% Special Requirements
% Use xAI inputs since they are fast and easy
For one, this step is the first where a novice user may interact with the RL agent, which imposes certain requirements for a reduced complexity of the explanation. This also coincides with the requirement of rapid explanations for the sake of fluency, enabling a truly interactive training process. Interpretable inputs are well suited for these requirements, since they give rapid introspection into what the agent perceives. 

% Lifted Requirements
% Is shallow, but thats okay since we take care of this in other stages
The drawbacks of a more shallow introspection into the model is alleviated by the testing in other stages, since during development and the explicit evaluation phase extra care is taken for a thorough analysis. Fundamental errors of the model should be taken care of during the development phase, while hidden biases introduced during training are in focus during the evaluation phase.

We emphasize that even suboptimal explanations by human teachers are better than none. Current literature focuses on agents using human advice during the learning phase, leveraging humans' \emph{a priori} knowledge. Yet, even though the human decisions could be less accurate, \citet{Zhang:2020:human_out_loop} demonstrate that agents are more likely to learn sub-optimal policies if they ignore human advice. 

To refer to the metrics of \citet{milani2022survey}, we propose the requirements of low cognitive load and high computational performance at the agent learning stage.

\subsection{Explainability}
As stated in Subsection~\ref{sec:InteractiveLearning}, interactive RL uses human feedback to reduce problems in areas like sample efficiency, sim-to-real transfer problems and generalization. Like any interaction, interactive RL requires a level of agent-human understanding, and one effective way to improve communication involves explaining one's and others' behavior~\citep{de:17}. Therefore, more and more recent approaches augment interactive RL using explainability techniques. For instance, researchers found that most people training an AI agent assume that their behavior reveals their knowledge \citep{habibian:21}. Hence, some approaches account for this human belief by making the robot behavior explainable.

\citet{fukuchi2017autonomous} proposed a method to explain an agent's future behavior to its trainer while using the same expressions used by the trainer. The agent selects the phrases by assuming that a higher reward means the agent correctly followed the advice. In later work they apply the approach to agents that change policies dynamically~\citep{fukuchi2017application}. 

Furthermore, simple visualization techniques like saliency maps or layer-wise relevance propagation could be used to explain the agent's perception of the world at a glance. These techniques each have their own set of benefits and drawbacks more explicitly discussed in \citet{LiuEtAl:2018:LinearModelUTrees} and \citet{Bach:2015:LayerWiseRelevancePropagation}


\subsection{Interactive Learning}

%Provide feedback to agent
While explanations usually assume a human explainee, interactive RL may require giving explanations to the agent. One common way of providing feedback is to evaluate agent actions as positive or negative \citep{arakawa:18,Knox:2008:TAMER,knox:13,macglashan2017interactive}. However, this limited feedback could improve if the trainer explains why specific actions are wrong. \citet{guan2020explanation} augment the binary evaluative feedback with visual explanations using saliency maps from humans. In addition to improving the agent's sample efficiency, the approach also reduces the human input required.  Likewise, \citet{Karalus:2021:HITL-counterfactuals} enhance evaluative feedback but this time using counterfactual explanations, yielding significant improvements in convergence speed. In case of negative feedback, the humans can communicate to the agent that the feedback would have been positive \emph{if} action $a$ was performed in a different state $s'$. The authors limit the counterfactual feedback only to the negative reward cases, where it has the largest impact. 

%Causability - Counterfactuals
Another example for the application of counterfactuals is provided by \citet{Pearl:2009:Causality}, who use dynamical structural causal models (DSCM) to explicitly model the differences in capabilities of the agent and the human operator as the world states evolve. In this framework, the agent views the human feedback as the intended action and adjusts it (using counterfactual reasoning), if the action is sub-optimal. A trade-off between autonomy and optimality is demonstrated, meaning that fully autonomous agents are likely to be sub-optimal and could only achieve optimality if they receive critical feedback from their human operators. The counterfactual approach proposed by the authors improves on standard methods even when human advice is imperfect.

%Querying
Further, human trainers tend to give more positive feedback, and the learning agent should be able to inherently accommodate this feedback bias. Making the agent's assumptions transparent to the trainer can improve the overall process. Additionally, RL agents who learn from human demonstration, imitation, or by querying the trainer preferences can inherit biased human behavior. In that case, xAI can also shed light on the biases of the model before deployment. The robot then selects different behaviors and asks people about their preferences. \citet{habibian:21} study the influence of robots' questions on how their trainers perceive them. In their approach, the robot chooses informative questions that simultaneously reveal its learning. Compared to other approaches that do not account for human perception, \citet{habibian:21} found out that people prefer revealing and informative questions since they find these questions easier to answer, more revealing about the robot's learning, and better focused on uncertain aspects of the task. 

%Feedback
%While giving feedback speeds up agent learning, it can also cause fatigue or boredom to human trainers \citep{akalin:21}, which reduces the advice quality and frequency, resulting in a diminished cumulative reward. Additionally, \citet{macglashan2017interactive} showed that human feedback is dependent on the agent’s current policy.  Transparency might also arise during the training of an agent via human reward, causing the teacher to give incorrect feedback~\citep{knox:13}. These refer to situations where the agent took an unexpected action to satisfy some safety constraints, the human trainer being oblivious of such issues. Hence, explaining the agent’s policy to the trainer will help them to give better and useful feedback and they would be able to reason out about the agent’s unexpected choices. Explanation of the agent’s policy could also enable the human to provide targeted demonstration or advice in the regions where the agent policy is most uncertain or unstable.  Letting the trainer know how much of the advice provided was used compared to environment exploration could help the trainer decide when and where to provide feedback. Moreover, two-way communication where trainers explain the reason for their feedback and agents demonstrate where they applied the advice could undoubtedly provide a better experience or motivation to the trainer while improving the agent’s learning. 


\subsection{Types of Collaboration}

% Different roles for 
\citet{WuEtAl:2021:HITLMLSurvey} propose that a human can take different roles for interaction with RL agents, such as a Supervisor, Controller, Assistant, Collaborateur or Impactfactor. This encourages us to take into account how the collaboration is framed and what it entails for developing efficient teaching approaches. A learning agent will possibly interact with the designer, trainer, and final user. Therefore, it is important to consider expert to na\"ive collaborators. For instance, tools for visual explanations using typical data visualization techniques such as bars maybe useful for people with some scientific background; however, they add mental load to others~\citep{anderson:20}. 
 
% sweet spot for ideal interaction
\citet{WuEtAl:2021:HITLMLSurvey} state that the ideal interaction for HITL would be fluent, performant and reliable. For systems geared at performance, the interaction is usually framed as collaboration, while a focus on reliability favors the role of supervisor for the human. For fluency however, new roles of interaction are proposed and discussed, which would also require new kinds of interfaces. An agent could integrate implicit, empathic feedback from a human in the form of gestures, vocalizations and facial expressions as shown by \citet{CuiEtAl:2020:EMPATHICFrameworkHumanFeedback}, which enables a more intuitive interaction and richer feedback from human to learning agent. To leverage such feedback effectively, appropriate user interfaces should be developed and the underlying model should be able to process multi-modal data such as speech, image etc. Another form of implicitly improving feedback is the approach presented by \citet{PengEtAl:2016:AdaptingAgentSpeed}, which makes the agent move slower if it is uncertain, enabling the human to provide feedback where it is most useful with an intuitive cue. Identifying and leveraging useful implicit feedback from humans would facilitate agent learning by moving beyond what the human explicitly mentions as a teacher.

Ultimately, we propose that Agent Learning approaches should consider different approaches into their HITL framework rather than forcing one specific technique, which allows to better determine the mentioned sweetspot of interaction.

\subsection{Conclusion}

% The successive application of first imitation-learning to build fundamental behavior, followed by preference-based learning to finetune the actions seems most sensible at the given time. This could then be enhanced by enabling querying approaches to get most value out of teaching sessions in different environments. 

\noindent To \textbf{summarize} the agent learning phase as per Table \ref{table:Explanations_table}, explanations should provide the user with interpretable inputs and allow a rich interaction to better understand model behavior. This also requires explanations to be in the language of the domain expert, facilitating a productive ``Human as Teacher'' interaction. The human is involved at this stage to provide evaluative feedback, give advice and preferences to the agent, and provide demonstrations for complicated tasks. Main explainability techniques are those explaining the agent perceptions and evaluating behavior, for example with counterfactuals. Furthermore, techniques which enable the agent to query human preferences or give interactive feedback to instructions from the human teacher are recommendable, while visualisation approaches like saliency maps could be used for the sake of their low cognitive and computational load. Explainability is interactive at this stage, and RL experts as well as domain experts are involved. These requirements differ from the agent development phase in that a domain expert is involved in the process and explainability should be geared towards interactivity and efficient agent learning, whereas the development phase was more focused on generating insights into the model architecture.

In this section, we covered the second phase of development, discussing Training of HITL RL agents. In the next section, we discuss the subsequent step of thoroughly evaluating the learned policies and the emerging agent behavior. 

\section{Model Evaluation}
\label{sec:Evaluation}
\begin{table*}[!htbp]
	\tiny
	\resizebox{1\linewidth}{!}{
		\begin{tabular}{|c||l|l|l|l|}
			\cline{1-5}
			\multicolumn{1}{|c||}{ }  & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{ }\\
			\multicolumn{1}{|c||}{\textsc{Phase}}  & \multicolumn{1}{c|}{\textsc{Explanation Requirements}} & \multicolumn{1}{c|}{\textsc{Human Involvement }} & \multicolumn{1}{c|}{\textsc{Explanation} } & \multicolumn{1}{c|}{\textsc{Long-term Research Directions}}\\
			\multicolumn{1}{|c||}{ }  & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{ }\\
			\cline{1-5}
			\multirow{14}{*}{\rotatebox{90}{\hspace{0em}\textsc{Evaluation}}}
			&  &  & & \\
			& $\cdot$Summary learned behavior  & $\cdot$Understand and evaluate	&	 \textbf{Techniques:}	& $\cdot$Ensure understandability for 	\\
			&  $\cdot$Scalable to large models & $ $ learned policies on micro	&	 $\cdot$Structural models: graphical, 	&$ $ domain expert	\\
			& $\cdot$Comparable to untrained   & $ $ and macro-level	&	 $ $ decision trees, causal models	&	$\cdot$Enable thorough and \\
			& $ $ models   & $\cdot$Decide the next phase	&	 $\cdot$Structural representations:&	$ $ comparable explanations that  \\
			& $\cdot$Understandable by domain & & $ $ logic rules, programs& $ $ scale with model size and \\
			& $ $ experts & & $\cdot$Policy Summarization: rules, & $ $ complexity \\
			&&& $ $ code blocks& $\cdot$Further develop dashboards for \\
			&   & 	&	 \textbf{Directionality:}	& $ $ policy inspection from different 	\\
			&   & 	&	 $\cdot$Bidirectional	& $ $ viewpoints	\\
			&    & 	&	 \textbf{Users:}	& 	\\
			&   & 	&	 $\cdot$Domain experts	&	\\
			&  $ $   & 	&	 $\cdot$RL experts	& 	\\
			&  &  & & \\
			\cline{1-5}
		\end{tabular}
	}
	\label{table:Evaluation}
\end{table*}

In this section, we detail the requirements of the agent evaluation step, and how explainability approaches and safety considerations can help build trust in the learned model. For the success of human-robot-teamwork, safety is essential. The robot must meet innate expectations of human to be predictable and safe, and communicate its intentions \citep{EderHarperLeonards:2014:HITLRoboticsSafetyAssurance}. To ensure this, the trained system has to be tested extensively. It is important to make a distinction between errors in the underlying model, and errors learned during training. In the Model Deployment phase described in Section~\ref{sec:Deployment}, underlying errors to the model architecture should be discovered and fixed. Therefore, the phase of Model Evaluation can focus on discovering errors acquired during training, and ultimately ensure a safe decision-making process.

% Shortcut Learning
This type of acquired error becomes apparent in various forms. One is shortcut learning, where a model finds undesired shortcuts in the training data instead of learning the desired concept. This often prevents a generalization since the shortcut is, in most cases, not present in the application context. For example, an algorithm that learns the hospital token embedded in an image rather than the targeted signs of pneumonia on X-ray images represents a case of acquired error~\citep{GeirhosEtAl:2020:ShortcutLearningDNN}. 
% Adversarial Attacks
Another symptom of errors acquired during training are adversarial attacks, which show that the model did not learn the desired concept, but rather invisible patterns in the image \citep{GoodfellowShlensSzegedy:2014:AdversarialExamples}. There are several approaches to reduce the attack surface for adversarial attacks with optimizations in the training process, but we will focus on the underlying issue of models failing to learn concepts. This problem is also part of the challenge of alignment of learned model and real world, identified in Subsection~\ref{sec:ChallengesRL} and discussed by \citet{RoyEtAl:2021:RLRoboticsChallenges}.

% Focus on decision making process

Both issues show why it is important to examine the behavior of the trained model. We propose that at this stage of the development, the focus should be on the decision making process of the model which reflects the learned behavior. This decision making process can be made tangible with approaches like policy summarization, graph-based explanations and causal models. We furthermore consider different safety approaches which ensure that the agent takes only allowed actions and is transparent with regard to it's level of uncertainty.

% Overview
    % 1. Topics: How to ensure safe decisions and prevent a deterioration of properly pre-trained models
    % 2. Focus: Interpretable decision-making (policies and value-functions)
    % 3. xAI: Policy Summarization (as text, code,...), Graph-based explanations (trees, DAGs), Causal Models

\subsection{Requirements}
During the evaluation phase, xAI approaches have to work with large models and complex decision-making processes. This requirement, for example, makes the use of text or rule-based approaches \citep{TabrezHayes:2019:xRLTextualExplanations,HayesShah:2017:AutonomousPolicyExplanation} more challenging, since they might be helpful when producing one page of output, while parsing and understanding many pages of model policy explanations will become prohibitive. In a similar vein, visual approaches like trees or DAGs, in general, should not exceed a certain size to be still useful. 
%size and complexity
This requirement is supported by \citet{WellsBednarz:2021:xAIRLSurvey}, who find that the authors of several xAI approaches identify the scaling of their approaches as a major challenge, which also shows why many xAI approaches are only applied to toy examples. This is especially relevant to text-or graph-based explanations, which can rapidly become unwieldly.
%must be expert-readable

Furthermore, models should provide explanations which are understandable to the domain experts. It will often be the case that only the domain expert, instead of the developer, can judge whether a learned policy is consistent, which makes it a requirement that the domain expert can evaluate it.

%must be comparable
A final consideration is that the provided explanations should be able to highlight differences in the learned behavior with regard to a newly initialized or only pre-trained model, in order to gain an understanding of what the agent actually learned during the different steps of the training process.

%Lifted constraints
Other requirements, however, can be reduced. Experts may no longer require the rapid computation of the xAI method since the focus now lies mainly on a thorough evaluation, which can take longer to assess. We, therefore, propose lower demands on computational performance but higher demands on fidelity and relevancy as per \citet{milani2022survey}.

\subsection{Explainability}
% Policy Summarization

Policy summarization approaches focus on showing and explaining the model policies to the user. We referred to examples that codify the model decision process as rules \citep{LiuEtAl:2018:LinearModelUTrees}, as code blocks \citep{VermaEtAl:2018:ProgrammaticallyInterpretableRL} or via natural language. \citet{AlonsoEtAl:2018:xAINLBeerClassifier} shows an example of justifying classifications with a textual explanation of the choice made by a decision tree, which could in turn be transferred to RL applications.
Policy summarization is ideally suited for assessing a trained model and checking its policies for unexpected and undesired behavior. Depending on whether and which domain experts are included in the process, different summarization approaches are advisable. Summarizing model policies as code blocks can be intuitive for computer science and adjacent fields, but are likely inadvisable for domain experts with a non-technical background. Here, special care should be given to assessing how a model could best be summarized to be intuitively understandable for the explanation target since the additional cognitive load for understanding the explanation modality should be kept to a minimum.
A second aspect is that the scale of the model should be considered. Ten blocks of model policy code can easily be evaluated, but hundred blocks of code will be very difficult to understand and thoroughly inspect. Here, the approach of graph-based explanations can be a useful addition.

Furthermore, graphs-based explanations can be very helpful by providing a quick and intuitive overview of model behavior. \citet{Holzinger:2016:iML} recommends the use of Graph-based explanations for HITL systems, since they can be used to compare an experts domain knowledge intuitively with the learned model behavior. \citet{SongEtAl:2019:ExplainableGraphBasedRecommendations} show how graph-based explanations can be applied in recommender systems, a field where knowledge-graphs are often used. With their presented system, the user is shown a meaningful path within that graph on how a recommendation was formed, which helps provide effective recommendations and good explanations.
The use of graph-based explanations can however become overwhelming for the user if the model behavior or explained decision becomes too complex. Approaches like PGExplainer by \citep{Vu:2020:PGMExplainer} alleviate this by focusing the explanation graph on relevant parts of the decision graph.

\subsection{Safety Evaluation}
% Other methods
Testing the model is an essential part of the training loop, but should not be the only component for ensuring a safe operation. An example of how safety can be ensured is presented by \citet{XiongEtAl:2020:Robustness}, who propose using shield-based defenses, where agents learn to stay in predefined, safe boundaries during training and application and with that increase robustness.

Furthermore, approaches that estimate the model uncertainty in different scenarios can be useful. \citet{LuetjensEverettHow:2018:RLModelUncertainty} present a collision avoidance policy to provide computationally tractable and parallelizable uncertainty estimations in navigation tasks. This can be used to ensure that the model is sufficiently confident in the test scenarios employed by the developers, but also able to discover blind spots. These blind spots can then be used to test how the model behaves when encountering them.

\subsection{Conclusion}

We suggest that a combination of the explainability methods described can help significantly by ensuring that the model has only learned desired behavior. The use of graph-based explanations is recommended as complementary to the policy summarization approach, since the summarizing provides a broad overview of model policy, which can then be further inspected by querying specific explanations. The rapidity and intuitiveness of graph-based explanations are allow to inspect the learned model policies together with domain experts, and together with safety measures and uncertainty estimation build a trustworthy model which informs the user of it's limits.

\noindent To \textbf{summarize} the model evaluation phase as per Table \ref{table:Explanations_table}, explanations at this stage should scale to large trained models and be comparable to previous versions of the agent. They have to be understandable by the domain experts to allow a exhaustive comparison of the learned model behavior. Humans are involved to understand and evaluate these policies and the resulting behavior at micro-level (sensible individual decision) and on macro-level (cohesive overall behavior). Useful explainability techniques are policy-summarization, graph-based explanations and approaches for interpretable decision-making like extracting decision trees and logic rules,  involving domain experts and RL experts in bidirectional fashion. In this phase, the payoff of utilizing causal models can lead to intrinsic explainability for the model. The model should furthermore be evaluated with regard to safety aspects to ensure lasting user-trust. In comparison to the other stages, this phase has many similarities in the used xAI techniques with the agent development phase, but requires in addition the understandability for the domain expert and the scaling of xAI methods to more complex policies.

In this section, considerations regarding the evaluation of HITL RL systems were discussed. In the next section, we proceed to discuss the final step of deploying the agent to a real-world context.

\section{Agent Deployment}
\label{sec:Deployment}

\begin{table*}[!htbp]
	\tiny
	\resizebox{1\linewidth}{!}{
		\begin{tabular}{|c||l|l|l|l|}
			\cline{1-5}
			\multicolumn{1}{|c||}{ }  & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{ }\\
			\multicolumn{1}{|c||}{\textsc{Phase}}  & \multicolumn{1}{c|}{\textsc{Explanation Requirements}} & \multicolumn{1}{c|}{\textsc{Human Involvement }} & \multicolumn{1}{c|}{\textsc{Explanation} } & \multicolumn{1}{c|}{\textsc{Long-term Research Directions}}\\
			\multicolumn{1}{|c||}{ }  & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{ } & \multicolumn{1}{c|}{ }\\
			\cline{1-5}
			\multirow{12}{*}{\rotatebox{90}{\hspace{0em}\textsc{Deployment}}}
			&  &  & & \\
			&  $\cdot$Fast, clear, and concise to& $\cdot$Deploy agent	&\textbf{Techniques:}	&	$\cdot$Develop and apply new \\
			&  $ $  reduce cognitive load& $\cdot$Use agent’s decisions	&	 $\cdot$Visualization: saliency map,	&$ $ approaches beyond image and 	\\
			&  $\cdot$Understandable by & $\cdot$Interact with agents	&	 $ $ dendogram, bounding-box	&	$\ $ driving-based explanations\\
			& $ $ end-users	 & $\cdot$Define agents' real-world 	& $\cdot$Textual explanation	 & $\cdot$Use simple and fast explanations	\\
			& $\cdot$Hideable to prevent  & $ $ application goal and	&	 $\cdot$Intent Communication:	& $ $ uncertainty handling\\
			& $ $ detrimental effects on user & $ $ context& $ $ object saliency + textual& $ $ uncertainty handling\\
			& $ $ performance & 	&	\textbf{Directionality:}	&$\cdot$Communicate agent intent via 	\\
			&   & 	&	 $\cdot$Bidirectional	& $ $ different modalities	\\
			&   & 	&\textbf{Users:}	& $\cdot$Use explanations to build trust	\\
			&   & 	&	 $\cdot$End-users	&	\\
			&  &  & & \\
			\cline{1-5}
		\end{tabular}
	}
	\label{table:Deployment}
\end{table*}
% Tradeoff for explainability
In this Section, we focus on approaches that facilitate an efficient interaction of the trained agent and its human user. This frequent and repeated interaction requires to find a balance between appropriately showing explanations and not hindering the task at hand --- a tradeoff also highlighted by \citet{AndersonBischof:2013:PerformanceGestureGuides}, who state that while initially guides can be helpful, they can also be detrimental for long-time performance and learning. Additionally, challenges concerning safety and generalization to overcome unforeseen situations come into play (see \citet{RoyEtAl:2021:RLRoboticsChallenges} and Subsection~\ref{sec:ChallengesRL}). 

% ensure performance gain
We propose that the usage of HITL agents can lead to significant performance gains, along the four stages of development, learning, evaluation and deployment. In this Section, we focus on approaches ensuring that those benefits actually reach the end user, factoring in issues like mental overload and distrust.

\subsection{Requirements}

The xAI systems used by end-users can draw on the vast pool of research on HCI usability. Therefore, we adapt considerations and requirements from the well-known ``golden rules of interface design'' \citep{ShneidermanEtAl:2016:GoldenRulesHCI}.

% requirements: have to be fast, both computable and understandable
Corresponding to the goal of reducing memory load, explanations have to be easily and rapidly understandable. We aim to facilitate difficult tasks, and should avoid complicating the human-robot interaction with overly complex explanations. Since operators are likely to work with rapidly changing perspectives and environments, explanations should be computed in real-time to ensure they correspond to the current situation. An example for this is an autonomous car. If explanations are provided with a lag of several seconds, actions which could require intervention will already have happened.

% second branch - on demand, which can use standard xAI for decision - has to be hideable
Concerning the guideline to allow experienced users to take shortcuts, agents should provide explanations on-demand and should be deactivated if desired, as further discussed by \cite{AndersonBischof:2013:PerformanceGestureGuides}. This option is essential for preventing information fatigue and enabling a natural and efficient human-robot collaboration. 

% third - predictability, errors and uncertainty
Our third consideration is to simplify error handling and giving the user the feeling of being in control. We propose that a HITL model should provide some means to show whether it is uncertain about a given situation or decision, for example in the form of a warning light as seen in cars. Such a mechanism can give the user a notification that something is wrong or uncertain, and allow to then investigate what causes this.  Along similar lines, we propose some kind of startup check sequence, again based on the warning light startup sequence of a car, where users can ensure that the system is in order and correctly understands the situational context.

Contrary to the other three steps of HITL RL deployment, we do not propose that major requirements can be lifted at this stage. We rather suggest that this step is the most demanding of the four enumerated, since it combines constraints on computational and cognitive capacities, requiring high fidelity, relevancy, performance, and low cognitive load~\citep{milani2022survey}.

\subsection{Explainability}

% fast approaches
Several researchers provide examples of how real-time explanations for different use-cases could look like. \citet{RodriguezEtAl:2021:DeepCovidxAI} provide feature-based explanations for COVID-19 case predictions, while \citet{Kulkarni:2021:EducationAIDashboard} developed a classroom dashboard that gives an overview of students performance with dendograms and text-based explanations. The majority of those real-time explainability systems are data/software-based, while for the area of explanations for robotic systems, there are much fewer examples. Most autonomous driving-systems provide explanations in form of bounding-boxes and labels for recognized objects, which is an appropriate option for explaining model perception. 

The next step is decision explanations. Here, \citet{Ben-YounesEtAl:2022:DrivingBehaviorEx} present a method where object saliency is combined with a textual explanation for an action. For example, the observed traffic light is highlighted, in combination with the textual explanation of a ``stop'' action. Such an approach is already helpful and quick to evaluate by the end-user. Still, it could, for example, be even further refined when using known symbols and signs instead of text along with regional highlighting.

% Show intent
A major component for trusting an agent is the predictability of the agents actions. Therefore, we suggest that xAI approaches used in the real-world focus on making the agents decision and planning transparent for the user by showing intended actions. Strictly, since actions do not have to be explained, just announced, this approach belongs to the area of interpretability, which constitutes a subset of explainability \citep{Dragan:2015:LegibleRobotMotion}. Requiring only interpretability and not explainability simplifies the requirements for such an indication, though of course HCI principles still have to be taken into account to avoid incurring too much mental load. \citet{Caltagarione:2017:DrivingPathGeneration}, for example, show a predicted trajectory for autonomous driving applications, which could easily be translated to other movement-based domains. 
An open challenge is how those predictions can be communicated in other contexts than autonomous cars and with other modalities. Items like smartwatches, headphones or just visual indicators could provide familiar and flexible interfaces.

\subsection{Error Handling}
% Use warning lights
We furthermore suggest that the use of ``warning light'' alerts could be beneficial, which recognize when the agent is unsure about a decision and notify the user. This could on the one hand increase the general robustness of the agents decision, and also foster human trust in the agents decision, since the user can now estimate better if the agent is certain about a decision.

% Estimate uncertainty
Such a warning light could be based on uncertainty estimation and be activated when the uncertainty rises over a given threshold. \citet{JainEtAl:2021:EpistemicUncertaintyPrediction} give an example of how epistemic uncertainty can be estimated to a certain degree. The introduction of such an approach could help the user with focusing on the given task and interaction with the robot, while still being in control and able to intervene when required.

% Operator corrects errors
Such an intervention approach is demonstrated by \citet{WuEtAl:2021:HITLDRLAutonomousDriving}. They allow the HITL operator to intervene when the agent makes erroneous decisions, and furthermore allow the model to learn from those interventions. 
% Startup
The startup sequence approach could complement this error handling concept. \citet{LiuGuoMahmud:2021:HITLErrorDetectionFramework} for example propose an error detection framework, where the HITL operator is presented with a list of most relevant, explainable features, to detect unusual or nonsensical behavior. This list is evaluated during startup with a quick glance, and provides considerable trust benefits.\\


\subsection{Conclusion}

\noindent To \textbf{summarize}, the agent deployment phase as per Table \ref{table:Explanations_table}, the main requirement is for explanations to be understandable by the end-user. Additionally, they will be used in the field and therefore should not incur a significant overhead, either with  too much cognitive load or long and costly computation times. The human is involved as end-user deploying the agent, defining the usage context and specific agent task. Explainability can be provided in the form of a combined explainability dashboard, communicating the agent intents and actions and ways of handling uncertainty and errors in bi-directional fashion. Explanations can visualize important aspects of the agents perception, or provide textual or other explanations for the agent's decision and intent. For visualization, currently used techniques are saliency maps, dendograms and bounding boxes, which could in combination with textual explanations be used to communicate the agent's intent. This phase is the first entirely focused on the end user and with that on generating user trust in the system, allowing to accurately assess it's strengths and weaknesses and enabling an efficient real-world cooperation. This does not require to provide insights into the model architecture as in the development and deployment phase, but rather to highlight relevant aspects of it's perception and decision making as in the agent learning phase.

We have now covered the four stages leading to the deployment of HITL RL systems. In the next section, we first open up the discussion on propositions for general research directions which emerged in this paper. Then, we focus on how research could more specifically be applied to complement the requirements identified in the four steps for HITL RL deployment.

\section{Discussion}
\label{sec:discussion}
We first discuss general challenges and open research directions in HITL RL approaches, and then evaluate the presented opportunities and requirements  with regards to future work. In the background section we discussed what is possible with current methods, while the four steps for HITL RL deployment focused on how current methods can be used and adapted to better suit the HITL requirements. Section /XY now focuses on further research to develop new methods and technologies to support HITL RL.

\subsection{Open Research Directions}

We refer back to Section \ref{sec:ChallengesRL} to emphasize that we consider RL a challenging problem setting that could greatly benefit from HITL approaches. We further emphasize that there is no one-size-fits-all solution for explainabiliy, and that the requirements for suitable HITL xAI differ between each phase. We do not propose a strict separation of explainability techniques in different phases, but rather recognize the suitability of certain types of xAI for each phase depending on the nature of the human involvement, the aspect of the agent in focus (e.g., model architecture, its decision making process, or its perception of the world), and the task to be tackled. The types of xAI approaches we recommend are informed by the explanation requirements listed in Table ~\ref{table:Explanations_table} as well as the nature of human involvement, the types of users, and the directionality of the interaction between the human and the agent.

%We envision that the depicted HITL RL approaches could, in the future, enable a human-robot collaboration, greatly enhancing the productivity of a human-robot team.
We envision that the depicted HITL RL approaches could, in the future, enhance the productivity of a human-robot team. \citet{KhatibEtAl:1999:RihEnvironment} stated that HITL contributes experience, domain knowledge and is able to ensure the correct execution of tasks. The robot, on the other hand, can increase the human's capabilities in terms of force, speed and precision. Moreover, the robot should reduce human exposure to harmful and hazardous conditions. 

As for explainability, most work in xAI is heavily biased by what researchers assumed to be good explanations for a given task or domain \citep{Miller:2019:xAISocialSciencesInsights}, not taking into account the preferences and expertise of the human end-users. Based on operators ' feedback, the models and approaches must adapt their language and modalities to be effective, which requires a more human-centered development \citep{PuiuttaVeith:2020:xAIRLSurvey}. To ensure that explainability methods align with users' expectations, we call for a comprehensive set of guidelines and requirements for developing xAI systems. 

% Teamup requirements
But the human-robot collaboration comes with challenges beyond explainability. When the agent is deployed, the trust requirement is essential, or else the agent will not be used. According to \citet{DeSaintsEtAl:2008:phri}, only trustworthy robots are able to work in such a team. Humans display a tendency to anthropomorphize robots~\citep{damiano2018anthropomorphism}, thereby overestimating their cognitive capabilities. \citet{DeSaintsEtAl:2008:phri} argue that a user's mental model might result in a fake robot \emph{dependability}, which exacerbates the problem of safety in human-robot collaboration. This collaboration relies on the predictability of the robot's actions, testability, explainability of the policies, as well as performance increases.


\subsection{Challenges}
\label{sec:ThirdGeneration}

In this section, we first sum up the specific challenges encountered in each phase. Then, we discuss the blue-sky approaches we envision for improving HITL RL with regard to those challenges.

As for current challenges in the development phase, we find many xAI approaches intended for this setting. We however encounter a lack of interactivity and comparability of the explanations, which hinders thorough introspection. We furthermore find that current explainability approaches often lack the causality and intuitive understandability required for a thorough introspection of a newly developed model.

For the agent learning phase, we find that many approaches support the \emph{Human-as-Teacher} paradigm itself, but identify a lack of explainability approaches which facilitate this interaction. More generally, xAI approaches in this phase need to evolve to support the interactivity in the HITL setting, which is currently underdeveloped.

In the model evaluation phase, we identify a major challenge in the scalability of xAI approaches, that are often suited to smaller models, but fail to provide explainability for large trained models and their learned policies. We also find that few approaches allow to identify variations between different versions of a trained model, which we consider a central feature for this phase in order to conduct an exhaustive evaluation of the learned policy.

For the final step of agent deployment, there are very few suitable xAI approaches. The application of current approaches is most often hindered by the failure to speak the user`s language, limited available computation, or too much complexity to be usable in a real-time context. Additionally, many HITL RL approaches fail to gain (and deserve) the user`s trust, in addition to failing to communicate uncertainty when warranted.

We therefore picture the following blue-sky propositions that are currently out of reach but could lead to large improvements in HITL RL systems. Refer to Table \ref{table:Explanations_table} for an overview of the propositions discussed hereafter.

% Developing - Interactive, thorough and comparable model summaries

We suggest to use compositional and representational language for explainability to enhance the intuitive understandability of models as mentioned by \citet{RoyEtAl:2021:RLRoboticsChallenges}.
Furthermore, representing the model as hierarchical, graphical, or topological structure \citep{lyu2019sdrl,battaglia2018relational} is more understandable to humans rather than a traditional neural network model. Such structural models are not powerful as neural network models since their expression and computation ability are limited, so an optimal approach would be to integrate them with the model itself without losing explainability.
We furthermore think that causal learning approaches should be adapted and integrated much more deeply into HITL approaches, since in addition to generating better explanations, they could also yield improved prediction performance as exemplified by \citet{MadumalEtAl:2020:CausalRLCFs}. Thirdly, policy querying approaches like those presented by \citet{HayesShah:2017:AutonomousPolicyExplanation} could be adapted to allow specific inquiries into model structure, and be expanded with counterfactual structures.
We ultimately envision interactive, thorough, and comparable model summaries. Individual components of such a solution can already be found, but the simplicity of a comprehensive solution could greatly benefit the development process.


% Teaching - Efficient training in the field, with replanning and corrections

For the agent learning step, the various approaches enumerated in the first generation should be adapted further for HITL and RL contexts. An important adaptation is the development of a suite of tools to facilitate the systematic deployment and comparison of the different approaches to discover sweet-spot mixes. An example of an effective combination could be to build fundamental behavior via imitation learning, followed by fine-turning actions by preference-based learning and finally identifying and solving weak spots using querying approaches. We propose to strive for a solution that allows efficient HITL training in the field along with subject matter experts, enabling users to rapidly bootstrap agent behavior and support this process further with replanning and corrections. A combination of such approaches could be very efficient and fast in bringing up robust agents suited for real-world applications.

% Testing - Thorough model decision explanations, with detailed diffs

We highlighted that many explainability approaches that suffice in the development and learning phase need adaptation to the evaluation stage due to model size and complexity. Approaches like code block summarization as provided by \citet{VermaEtAl:2018:ProgrammaticallyInterpretableRL} could be extended by focusing only on relevant parts of the explanations as illustrated by \citet{Vu:2020:PGMExplainer}. Furthermore, expert-readability needs to be ensured to allow subject matter experts to help with the testing and evaluating whether the learned policies are sensible.
Furthermore, post-hoc explanations like visual \citep{DBLP:journals/corr/abs-1912-12191,DBLP:journals/corr/abs-1912-05743} or textual \citep{fukuchi2017autonomous,HayesShah:2017:AutonomousPolicyExplanation} explanations would increase explainability.

We recommend integrating various tools to help evaluate and scrutinize a model from many different viewpoints. We propose that thorough model decision explanations are essential for this process. Furthermore, the focus should be on providing explanations in such a way that they are understandable to the subject matter experts, allowing to not only debug superficial model behavior, but also check the learned routines for semantically sensible behavior, which is a task that also calls on the knowledge of the subject matter expert.

% Using - simple and fast highlight-symbol explanations, explaining abstract actions via different modalities, motor warning light and startup sequence

With regard to explainability in the deployment of HITL approaches, we propose that the currently available approaches look beyond the use-cases of autonomous driving focused on visual aspects and consider other modalities. One example is the context of credit or policy computations, which requires explaining textual facts. Also, modalities beyond graphic dashboards should be considered to ensure, such as auditory and tactic perspectives. Furthermore, visual perspectives should be explored in different form-factors such as smartwatches, LED indicators and image projections.
We emphasize the need for simple and fast explanations, where there are only few such current approaches. We furthermore recommend to consider explaining agent actions via different modalities, such as visual indicators, but also haptic or auditory signals, aspects which are largely unexplored as of now. Finally, we envision a suite of tools equipped with warning indicators that show when the agent encounters difficult situations, allowing the user to trust the agent when it is within its generalization capabilities, and inform the user if that is not the case. Ultimately, a startup-sequence with different checks would allow the user to ensure that the agent is properly initialized and trustworthy.

In this section, we have discussed both current research directions and challenges as well as more specific propositions for xAI solutions in the four phases. In the next section, we summarize the contents and central insights of this paper, and conclude with the visions we have for the future of HITL RL.

\section{Conclusion and Future Outlook}
\label{sec:conclusion}
In summary, we emphasise that RL is a fundamentally difficult problem setting and could benefit greatly from human-in-the-loop interactions. A human expert can contribute conceptual understanding gained through many years of experience to the task at hand, thereby significantly improving robustness as well as explainability \citep{Holzinger:2021:TrustAI}. Numerous approaches have demonstrated that RL benefits from human-centred approaches \citep{Li:2019:HumanCenteredRLSurvey,MatthewsonPilarski:2022:DesigningAndEvaluatingHCIML}.

We further argue that HITL RL in particular Benefits greatly from xAI approaches. These are, after all, fundamentally human approaches, which in turn ensure successful interactions, acceptance, and trust as well as conceptual knowledge about the agents' limitations \citep{heuillet2021explainability,milani2022survey}.

We identify the following phases for deploying HITL RL solutions: 1) RL model development, 2) agent learning, 3) model evaluation, and 4) agent deployment. In our work, we discuss how xAI can support each of these phases and what are some considerations for successful deployment. With that, the HITL combination enables a better human-robot collaboration and ultimately increases on-task productivity and efficiency.

In the deployment phase, interactive, thorough and coherent model summaries can enable an agile and transparent workflow. During the agent's interactive learning, xAI approaches can enable more efficient training in the field through interactive replanning and corrections. In the evaluation phase, comprehensive explanations of model decisions can provide detailed insights into the trained model and lead to informed decisions about whether to proceed with the deployment or start another development cycle. In the deployment phase, simple and quick explanations of actions and different explanations for error handling could significantly increase user confidence in the agent and lead to more efficient collaboration.

Last but not least, we propose a vision of an interactive human-robot collaboration that enables new use cases for RL applications and allows both humans and robots to realize their full potential and respective strengths. Such a collaboration requires strong interaction and trust between both parties, that can only be achieved through comprehensive explainability and deep and intuitive understanding of the mental model generated by the agent.

\begin{comment}

\section{Author Contributions}

\begin{enumerate}
\item Carl Orge Retzlaff --- Main Author. Conceptualization, Methodology, Writing, Administration.
\item Srijita
\item Christabel --- Research, Writing, Review, Editing.  
\item Payam
\item Anna
\item Alessa
\item Mohammad
\item Tianpei
\item Matthew E.\ Taylor --- Conceptualization, Framing, Resources, Funding Acquisition.
\item Andreas
\end{enumerate}

\section*{Abbreviations}

\begin{itemize}

\item AI = Artificial Intelligence
\item CAM = Class Activation Mapping
\item BP = Bongard Problem
\item c-EB = contrastive Excitation Backpropagation
\item CG = Counterfactual Graph
\item CNN = Convolutional Neural Network
\item CRF = Conditional Random Fields
\item CT = Computational Tomography
\item DF = Decision Forest
\item DGNN = Dynamic  Graph  Neural  Network
\item EB = Excitation Backpropagation
\item GAN = Generative Adversarial Network
\item GB = Guided Backpropagation
\item GCNN = Graph Convolutional (Neural) Network
\item GloVe = Global Vectors for Word Representation
\item GNN = Graph Neural Network
\item Grad-CAM = gradient-weighted Class Activation Mapping
\item GraphSAGE = Graph Sampling \& Aggregation
\item GRL = Graph Representation Learning
\item HER = Hindsight Experience Replay
\item ICG = Interaction \& Correspondence Graph
\item LIME = Local Interpretable Model-Agnostic Explanations
\item LSTM = Long Short-Term Memory
\item LRP = Layer Wise Relevance Propagation
\item MM = Multi-Modal
\item MRI = Magnetic Resonance Imaging
\item NAM = Node Attribution Method
\item NIV = Node Importance Visualization
\item OCT = Optical Coherence Tomography
\item OGB = Open Graph Benchmark
\item PGN = Pointer Graph Network
\item PGM = Probabilistic Graphical Models
\item PET = Positron Emission Tomography
\item RL = Reinforcement Learning
\item RW = Random Walks
\item ReLU = Rectified Linear Unit
\item SA = Sensitivity Analysis
\item UI = User Interface
\item xAI = explainable Artificial Intelligence
\item XGNN = Explanations of Graph Neural Networks

\end{itemize}

\end{comment}

\acks{Parts of this work have been funded by the Austrian Science Fund (FWF), Project: P-32554 ``explainable Artificial Intelligence''.}
\newpage

%\appendix
%\section*{Appendix A.}
%\label{}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:


{
%\bibliographystyle{IEEEtran}
\bibliography{references}
}


\end{document}